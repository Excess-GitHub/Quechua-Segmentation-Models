# SegmentaciÃ³n MorfolÃ³gica del Quechua

**ğŸŒ Idioma / Language:** [English](README.md) | EspaÃ±ol

---

> **SegmentaciÃ³n MorfolÃ³gica Supervisada para el Quechua SureÃ±o: Priors, Filtros y Aumento con LLM**

Un conjunto de herramientas integral para la segmentaciÃ³n morfolÃ³gica del quechua sureÃ±o, con arquitecturas neuronales aumentadas con priors lingÃ¼Ã­sticamente informados.

## à°… Resumen de Resultados

### Resultados en Conjunto de Prueba (913 palabras)

| Modelo | EM | +Filtro | B-F1 |
|--------|:--:|:-------:|:----:|
| Transformer Seq2Seq | 43.2% | â€” | â€” |
| BiLSTM (CarÃ¡cter) | 52.7% | â€” | 0.817 |
| BiLSTM (Grafema) | 56.1% | â€” | 0.840 |
| BiLSTM + Morfessor | 55.1% | â€” | 0.838 |
| BiLSTM + Prior DT | 54.1% | 64.8% | 0.815 |
| BiLSTM + Prior HMM | 57.4% | 66.6% | 0.822 |
| **BiLSTM + HMM + GPT-4o (200)** | **63.9%** | **74.2%** | **0.898** |

**EM** = Coincidencia Exacta, **+Filtro** = con filtro de rechazo, **B-F1** = F1 de Fronteras (sin filtro)

### Resultados de ValidaciÃ³n Cruzada (5 pliegues)

| Modelo | VC EM | VC B-F1 |
|--------|:-----:|:-------:|
| Transformer Seq2Seq | 60.5 Â± 1.5% | â€” |
| BiLSTM-CRF | 84.9 Â± 1.5% | â€” |
| BiLSTM + Prior DT | 84.2 Â± 1.6% | 0.960 Â± 0.004 |
| BiLSTM + Prior HMM | 85.8 Â± 1.2% | 0.952 Â± 0.005 |

### Hallazgos Principales

1. **BiLSTM > Transformer**: Los modelos basados en BiLSTM superan sustancialmente a los Transformers (+13.9% EM), confirmando que los sesgos inductivos importan en escenarios de bajos recursos.

2. **La tokenizaciÃ³n por grafemas ayuda**: Respetar los dÃ­grafos del quechua (ch, ll, ph, etc.) proporciona mejoras modestas pero consistentes (+3.4% EM).

3. **Los priors lingÃ¼Ã­sticos mejoran**: El prior HMM de sufijos logra 57.4% vs 56.1% de la lÃ­nea base de grafemas (+1.3%).

4. **Los filtros de rechazo son cruciales**: Las restricciones morfotÃ¡cticas duras en la inferencia proporcionan +9.2% de mejora en EM.

5. **El aumento con LLM es efectivo**: GPT-4o con 200 ejemplos sintÃ©ticos produce nuestro mejor resultado (74.2% EM).

6. **GPT-4o > GPT-5 para esta tarea**: La fidelidad en el seguimiento de instrucciones importa mÃ¡s que la capacidad bruta para tareas morfolÃ³gicas especÃ­ficas.

## à°† Arquitectura

```
Palabra â†’ Tokenizador de Grafemas â†’ Codificador BiLSTM â†’ Logits de Frontera â†’ Filtro de Rechazo â†’ Palabra Segmentada
                                          â†‘
                                  Prior HMM/DT (suave)
```

El sistema combina:
- **Priors suaves** (HMM o Ãrbol de DecisiÃ³n) durante el entrenamiento mediante fusiÃ³n a nivel de logits
- **Restricciones duras** (filtro de rechazo de sufijos) en la inferencia

## à°‡ Estructura del Repositorio

```
quechua-segmentation/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py          # Exportaciones del paquete
â”‚   â”œâ”€â”€ preprocessing.py     # TokenizaciÃ³n, normalizaciÃ³n
â”‚   â”œâ”€â”€ models.py            # Arquitecturas neuronales y priors
â”‚   â”œâ”€â”€ evaluation.py        # MÃ©tricas y utilidades de evaluaciÃ³n
â”‚   â””â”€â”€ training.py          # Bucles de entrenamiento y checkpoints
â”œâ”€â”€ notebooks/               # Notebooks Jupyter originales
â”œâ”€â”€ images/                  # Figuras y visualizaciones
â”œâ”€â”€ data/                    # Archivos de datos (ver secciÃ³n Datos)
â”œâ”€â”€ models/                  # Checkpoints de modelos entrenados
â””â”€â”€ README.md
```

## à°ˆ Inicio RÃ¡pido

### InstalaciÃ³n

```bash
pip install torch numpy pandas scikit-learn morfessor regex
```

### Uso BÃ¡sico

```python
from src import (
    to_graphemes, 
    BiLSTMBoundary, 
    HMMSuffixPrior,
    SuffixRejectionFilter,
    apply_boundaries
)

# Tokenizar una palabra
tokens = to_graphemes("rikuchkani")
# ['r', 'i', 'k', 'u', 'ch', 'k', 'a', 'n', 'i']

# Cargar modelo entrenado y predecir
model = BiLSTMBoundary(vocab_size=42, emb_dim=64, hidden_size=128)
# ... cargar pesos ...

# Aplicar prior HMM
hmm_prior = HMMSuffixPrior()
hmm_prior.fit(training_morph_splits)
prior_probs = hmm_prior.predict_probs(tokens)

# Obtener predicciones y aplicar filtro
boundary_labels = [0, 0, 0, 1, 0, 0, 1, 0]  # del modelo
segments = apply_boundaries(tokens, boundary_labels)
# ['riku', 'chka', 'ni']

# Filtro de rechazo
filter = SuffixRejectionFilter(suffix_vocabulary)
filtered = filter.filter("rikuchkani", segments)
```

## à°‰ Datos

### Datos de Entrenamiento (Privados)
- **6,896 palabras Ãºnicas** de transcripciones de entrevistas con ~70 hablantes adultos de quechua
- Recopilados bajo aprobaciÃ³n IRB con consentimiento informado
- Anotados por dos consultores ancianos de la comunidad
- Disponibles bajo solicitud en forma desidentificada bajo condiciones de acceso controlado

### Datos de Prueba (PÃºblicos)
- **913 palabras Ãºnicas** separadas antes del entrenamiento
- Publicados con este repositorio

### EstadÃ­sticas del Corpus

Nuestros datos de entrenamiento exhiben una fuerte correlaciÃ³n entre la longitud de palabra y el conteo de morfemas (Pearson r = 0.79, p < 0.001):

![Longitud de Palabra vs Conteo de Morfemas](images/heatmap.png)

*Mapa de calor mostrando la relaciÃ³n entre longitud de palabra (caracteres) y nÃºmero de morfemas. El patrÃ³n diagonal refleja la restricciÃ³n de raÃ­ces bisilÃ¡bicas del quechua y la sufijaciÃ³n regular.*

![RegresiÃ³n Lineal](images/regression.png)

*RelaciÃ³n lineal: morfemas â‰ˆ 0.28 Ã— longitud + 0.32 (RÂ² = 0.63)*

Para el corpus pÃºblico (~2.1M tokens, 206K tipos):

![Ley de Heaps](images/heaps.png)

*El crecimiento del vocabulario sigue la ley de Heaps con Î² = 0.90 (RÂ² = 0.98), indicando productividad continua.*

![Zipf-Mandelbrot](images/zipf.png)

*DistribuciÃ³n de frecuencia de palabras con ajuste Zipf-Mandelbrot (s = 1.06, q = 6.0).*

## à°Š Modelos

### Etiquetador de Fronteras BiLSTM
BiLSTM a nivel de carÃ¡cter/grafema con predicciÃ³n de fronteras por posiciÃ³n.

```python
from src.models import BiLSTMBoundary

model = BiLSTMBoundary(
    vocab_size=42,
    emb_dim=64,
    hidden_size=128,
    num_layers=2,
    dropout=0.1
)
```

### BiLSTM con Priors LingÃ¼Ã­sticos
Integra priors HMM o Ãrbol de DecisiÃ³n mediante fusiÃ³n a nivel de logits.

```python
from src.models import BiLSTMWithPrior, HMMSuffixPrior

prior = HMMSuffixPrior(max_suffix_len=8)
prior.fit(morph_splits)

model = BiLSTMWithPrior(vocab_size=42, prior_alpha=1.0)
```

### Filtro de Rechazo
ValidaciÃ³n post-procesamiento contra vocabulario de sufijos conocidos.

```python
from src.models import SuffixRejectionFilter

filter = SuffixRejectionFilter(suffix_set)
valid = filter.validate(["riku", "chka", "ni"])  # True
valid = filter.validate(["ri", "ku", "xyz"])     # False
```

## à°‹ Aumento con LLM

Usamos GPT-4o para generar ejemplos de entrenamiento sintÃ©ticos:

1. **SelecciÃ³n de candidatos**: Palabras del corpus pÃºblico de quechua que coinciden con patrones de raÃ­ces en los datos de entrenamiento
2. **Prompting few-shot**: 37 pares de demostraciÃ³n, temperatura=0
3. **ValidaciÃ³n**: Rechazar salidas que contengan palabras en inglÃ©s o formato incorrecto
4. **Cantidad Ã³ptima**: 200 ejemplos (mÃ¡s puede perjudicar el rendimiento)

El pipeline respeta la gobernanza de datos al no exponer datos de entrenamiento privados a APIs externas.

## à±  CitaciÃ³n

```bibtex
@inproceedings{anonymous2026quechua,
  title={Supervised Morphological Segmentation for Southern Quechua: 
         Priors, Filters, and LLM Augmentation},
  author={Anonymous},
  booktitle={Proceedings of ACL 2026},
  year={2026}
}
```

## à° Ã‰tica y Gobernanza de Datos

- Datos de entrenamiento recopilados bajo aprobaciÃ³n IRB con consentimiento informado
- Los ancianos de la comunidad sirvieron como anotadores con compensaciÃ³n justa a tarifas de EE.UU.
- Honramos los principios de soberanÃ­a de datos indÃ­genas (CARE)
- Los datos privados no se exponen a APIs externas
- La publicaciÃ³n pÃºblica prioriza aplicaciones de mantenimiento del idioma y educaciÃ³n

## à° Licencia

El cÃ³digo se publica bajo Licencia MIT. El conjunto de prueba de 913 palabras se publica para fines de investigaciÃ³n.

## à° Agradecimientos

Agradecemos a las comunidades quechuahablantes que contribuyeron datos y experiencia lingÃ¼Ã­stica, y a los consultores ancianos de la comunidad que proporcionaron las anotaciones.
