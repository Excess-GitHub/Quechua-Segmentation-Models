{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbb1617a",
   "metadata": {},
   "source": [
    "# Stats: Outlier Removal Experiment\n",
    "\n",
    "Experimental analysis to test if removing statistical outliers from training data improves BiLSTM+CRF morphology parser performance. Compares two models: one trained on full data, one trained on filtered data (outliers removed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01beb0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import os\n",
    "import ast\n",
    "import json\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "\n",
    "# Data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "# Statistics\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "# ML\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchcrf import CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91fb675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATA_FOLDER = \"data\"\n",
    "MODEL_NAME = \"stats\"\n",
    "MODELS_FOLDER = f\"models_{MODEL_NAME}\"\n",
    "os.makedirs(DATA_FOLDER, exist_ok=True)\n",
    "os.makedirs(MODELS_FOLDER, exist_ok=True)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdd403a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load gold standard data\n",
    "print(\"loading gold data...\")\n",
    "gold_df = pd.read_parquet(os.path.join(DATA_FOLDER, \"Sue_kalt.parquet\"))\n",
    "gold_df['Word'] = gold_df['word']\n",
    "gold_df['morph'] = gold_df['morph'].str.replace('-', ' ')\n",
    "gold_df['Morph_split_str'] = gold_df['morph']\n",
    "gold_df['Morph_split'] = gold_df['morph'].str.split(' ')\n",
    "gold_df = gold_df[['Word', 'Morph_split', 'Morph_split_str']]\n",
    "gold_df = gold_df.drop_duplicates(subset=['Word']).reset_index(drop=True)\n",
    "gold_df = gold_df.dropna(subset=['Word'])\n",
    "print(f\"got {len(gold_df):,} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b5cd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract basic features\n",
    "gold_df['num_morphemes'] = gold_df['Morph_split'].apply(len)\n",
    "gold_df['word_len'] = gold_df['Word'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbcc0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of word length vs morpheme count\n",
    "heatmap_data = gold_df.groupby(['word_len', 'num_morphemes']).size().unstack(fill_value=0)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(heatmap_data, annot=True, fmt='d', cmap='YlGnBu')\n",
    "plt.title('Heatmap of Word Length vs. Morpheme Count')\n",
    "plt.xlabel('Number of Morphemes')\n",
    "plt.ylabel('Word Length (Characters)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a63cf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "x = gold_df['word_len']\n",
    "y = gold_df['num_morphemes']\n",
    "\n",
    "pearson_corr, pearson_p = pearsonr(x, y)\n",
    "spearman_corr, spearman_p = spearmanr(x, y)\n",
    "\n",
    "print(f\"pearson correlation: {pearson_corr:.3f} (p={pearson_p:.3e})\")\n",
    "print(f\"spearman correlation: {spearman_corr:.3f} (p={spearman_p:.3e})\")\n",
    "print(\"\\nstrong positive correlation means longer words tend to have more morphemes\")\n",
    "print(\"we'll use this relationship to find outliers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01e99ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression to identify outliers\n",
    "gold_df1 = gold_df.copy()\n",
    "print(f\"original size: {gold_df1.shape}\")\n",
    "\n",
    "X = gold_df1[['word_len']]\n",
    "y = gold_df1['num_morphemes']\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "gold_df1['predicted'] = model.predict(X)\n",
    "gold_df1['residual'] = gold_df1['num_morphemes'] - gold_df1['predicted']\n",
    "\n",
    "std_residual = gold_df1['residual'].std()\n",
    "filtered_df = gold_df1[np.abs(gold_df1['residual']) <= std_residual]\n",
    "print(f\"cleaned size (outliers removed): {filtered_df.shape}\")\n",
    "print(f\"outliers removed: {len(gold_df1) - len(filtered_df):,} examples\")\n",
    "\n",
    "X_filtered = filtered_df[['word_len']]\n",
    "y_filtered = filtered_df['num_morphemes']\n",
    "\n",
    "model_filtered = LinearRegression()\n",
    "model_filtered.fit(X_filtered, y_filtered)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=filtered_df, x='word_len', y='num_morphemes', alpha=0.5, label='Filtered Data')\n",
    "plt.plot(X_filtered, model_filtered.predict(X_filtered), color='red', linewidth=2, label='Regression Line')\n",
    "plt.title('Optimized Linear Regression: Word Length vs Morpheme Count')\n",
    "plt.xlabel('Word Length (Characters)')\n",
    "plt.ylabel('Number of Morphemes')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "slope = model.coef_[0]\n",
    "intercept = model.intercept_\n",
    "print(f\"pre-refined regression: num_morphemes ≈ {slope:.2f} × word_len + {intercept:.2f}\")\n",
    "\n",
    "slope = model_filtered.coef_[0]\n",
    "intercept = model_filtered.intercept_\n",
    "print(f\"refined regression: num_morphemes ≈ {slope:.2f} × word_len + {intercept:.2f}\")\n",
    "\n",
    "r2_full = r2_score(y, gold_df1['predicted'])\n",
    "r2_filtered = r2_score(y_filtered, model_filtered.predict(X_filtered))\n",
    "print(f\"R2 (original): {r2_full:.3f}\")\n",
    "print(f\"R2 (filtered): {r2_filtered:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef3f96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest regression\n",
    "gold_df2 = gold_df.copy()\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(gold_df2[['word_len']], gold_df2['num_morphemes'])\n",
    "\n",
    "gold_df2['predicted_rf'] = rf.predict(gold_df2[['word_len']])\n",
    "gold_df2['residual_rf'] = gold_df2['num_morphemes'] - gold_df2['predicted_rf']\n",
    "\n",
    "mse_full = mean_squared_error(gold_df2['num_morphemes'], gold_df2['predicted_rf'])\n",
    "mae_full = mean_absolute_error(gold_df2['num_morphemes'], gold_df2['predicted_rf'])\n",
    "r2_full = r2_score(gold_df2['num_morphemes'], gold_df2['predicted_rf'])\n",
    "\n",
    "std_residual = gold_df2['residual_rf'].std()\n",
    "filtered_df_rf = gold_df2[np.abs(gold_df2['residual_rf']) <= std_residual].copy()\n",
    "\n",
    "rf_filtered = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "X_filtered = filtered_df_rf[['word_len']]\n",
    "y_filtered = filtered_df_rf['num_morphemes']\n",
    "rf_filtered.fit(X_filtered, y_filtered)\n",
    "\n",
    "filtered_df_rf['predicted_rf'] = rf_filtered.predict(X_filtered)\n",
    "r2_filtered = r2_score(y_filtered, filtered_df_rf['predicted_rf'])\n",
    "mse_filtered = mean_squared_error(y_filtered, filtered_df_rf['predicted_rf'])\n",
    "mae_filtered = mean_absolute_error(y_filtered, filtered_df_rf['predicted_rf'])\n",
    "\n",
    "print(\"random forest (before outlier removal):\")\n",
    "print(f\"MSE: {mse_full:.3f}\")\n",
    "print(f\"MAE: {mae_full:.3f}\")\n",
    "print(f\"R²:  {r2_full:.3f}\")\n",
    "\n",
    "print(\"random forest (after outlier removal):\")\n",
    "print(f\"MSE: {mse_filtered:.3f}\")\n",
    "print(f\"MAE: {mae_filtered:.3f}\")\n",
    "print(f\"R²:  {r2_filtered:.3f}\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=filtered_df_rf['word_len'], y=filtered_df_rf['num_morphemes'], alpha=0.5, label='Filtered Data')\n",
    "sns.lineplot(x=filtered_df_rf['word_len'], y=filtered_df_rf['predicted_rf'], color='red', label='RF Prediction (Filtered)')\n",
    "plt.xlabel('Word Length (Characters)')\n",
    "plt.ylabel('Number of Morphemes')\n",
    "plt.title('Random Forest Regression (Filtered)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93da3bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial regression\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_poly = poly.fit_transform(gold_df2[['word_len']])\n",
    "model_poly = LinearRegression().fit(X_poly, gold_df2['num_morphemes'])\n",
    "preds_poly = model_poly.predict(X_poly)\n",
    "r2_before = r2_score(gold_df2['num_morphemes'], preds_poly)\n",
    "print(f\"polynomial regression R2 (before filtering): {r2_before:.3f}\")\n",
    "\n",
    "residuals_poly = gold_df2['num_morphemes'] - preds_poly\n",
    "std_resid_poly = residuals_poly.std()\n",
    "mask = np.abs(residuals_poly) <= std_resid_poly\n",
    "filtered_df_poly = gold_df2[mask].copy()\n",
    "\n",
    "X_filtered_poly = poly.fit_transform(filtered_df_poly[['word_len']])\n",
    "model_poly_filtered = LinearRegression().fit(X_filtered_poly, filtered_df_poly['num_morphemes'])\n",
    "preds_filtered = model_poly_filtered.predict(X_filtered_poly)\n",
    "r2_after = r2_score(filtered_df_poly['num_morphemes'], preds_filtered)\n",
    "print(f\"polynomial regression R2 (after filtering):  {r2_after:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a60c467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify outliers from all models\n",
    "linear_outliers = gold_df1[np.abs(gold_df1['residual']) > std_residual]\n",
    "rf_outliers = gold_df2[np.abs(gold_df2['residual_rf']) > std_residual]\n",
    "poly_outliers = gold_df2[np.abs(residuals_poly) > std_resid_poly]\n",
    "\n",
    "all_outliers = pd.concat([linear_outliers, rf_outliers, poly_outliers])\n",
    "all_outliers = all_outliers[['word_len', 'num_morphemes']].drop_duplicates()\n",
    "\n",
    "# Visualize outliers on heatmap\n",
    "heatmap_data = gold_df.groupby(['word_len', 'num_morphemes']).size().unstack(fill_value=0)\n",
    "outlier_coords = all_outliers[['word_len', 'num_morphemes']]\n",
    "outlier_coords = outlier_coords[\n",
    "    (outlier_coords['word_len'].isin(heatmap_data.index)) & \n",
    "    (outlier_coords['num_morphemes'].isin(heatmap_data.columns))\n",
    "]\n",
    "outlier_coords = outlier_coords.copy()\n",
    "outlier_coords['freq'] = outlier_coords.apply(\n",
    "    lambda row: heatmap_data.at[row['word_len'], row['num_morphemes']], axis=1\n",
    ")\n",
    "\n",
    "norm = plt.Normalize(vmin=heatmap_data.values.min(), vmax=heatmap_data.values.max())\n",
    "colors = cm.Purples_r(norm(outlier_coords['freq'].values))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.heatmap(heatmap_data, annot=True, fmt='d', cmap='Reds', cbar_kws={'label': 'Frequency'})\n",
    "plt.title('Heatmap of Word Length vs. Morpheme Count with Outliers')\n",
    "plt.xlabel('Number of Morphemes')\n",
    "plt.ylabel('Word Length (Characters)')\n",
    "\n",
    "for j, (_, row) in enumerate(outlier_coords.iterrows()):\n",
    "    plt.scatter(\n",
    "        x=row['num_morphemes'] + 0.5,\n",
    "        y=row['word_len'] + 0.5,\n",
    "        color=colors[j],\n",
    "        s=100,\n",
    "        marker='X',\n",
    "        linewidths=2,\n",
    "        label='Outlier' if j == 0 else \"\"\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0906375b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_boundary_labels(word, split):\n",
    "    \"\"\"Generate boundary labels for a word given its morpheme split.\"\"\"\n",
    "    labels = [0] * len(word)\n",
    "    idx = 0\n",
    "    for morpheme in split[:-1]:\n",
    "        idx += len(morpheme)\n",
    "        if idx < len(word):\n",
    "            labels[idx - 1] = 1\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788223c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MorphemeDataset(Dataset):\n",
    "    \"\"\"Dataset for morphological segmentation training.\"\"\"\n",
    "    def __init__(self, path, char2idx=None):\n",
    "        self.data = []\n",
    "        with open(path, encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                item = json.loads(line)\n",
    "                if len(item[\"chars\"]) == 0:\n",
    "                    continue\n",
    "                self.data.append(item)\n",
    "\n",
    "        if char2idx is None:\n",
    "            chars = set(c for item in self.data for c in item[\"chars\"])\n",
    "            self.char2idx = {c: i + 1 for i, c in enumerate(sorted(chars))}\n",
    "            self.char2idx[\"<PAD>\"] = 0\n",
    "        else:\n",
    "            self.char2idx = char2idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        x = torch.tensor([self.char2idx[c] for c in item[\"chars\"]], dtype=torch.long)\n",
    "        y = torch.tensor(item[\"labels\"], dtype=torch.float)\n",
    "        return x, y\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Collate function: pads sequences to the same length.\"\"\"\n",
    "    xs, ys = zip(*batch)\n",
    "    lengths = [len(x) for x in xs]\n",
    "    max_len = max(lengths)\n",
    "    padded_x = torch.zeros(len(xs), max_len, dtype=torch.long)\n",
    "    padded_y = torch.zeros(len(ys), max_len, dtype=torch.long)\n",
    "    for i, (x, y) in enumerate(zip(xs, ys)):\n",
    "        padded_x[i, :len(x)] = x\n",
    "        padded_y[i, :len(y)] = y.long()\n",
    "    mask = (padded_x != 0)\n",
    "    return padded_x, padded_y, torch.tensor(lengths), mask\n",
    "\n",
    "class Segmenter(nn.Module):\n",
    "    \"\"\"BiLSTM+CRF model for morphological segmentation.\"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim=64, hidden_dim=128, num_labels=2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_labels)\n",
    "        self.crf = CRF(num_labels, batch_first=True)\n",
    "\n",
    "    def forward(self, x, lengths, labels=None, mask=None):\n",
    "        emb = self.embedding(x)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(emb, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_out, _ = self.lstm(packed)\n",
    "        lstm_out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)\n",
    "        h = self.relu(self.fc1(self.dropout(lstm_out)))\n",
    "        emissions = self.fc2(h)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss = -self.crf(emissions, labels.long(), mask=mask.bool(), reduction='mean')\n",
    "            return loss\n",
    "        else:\n",
    "            best_paths = self.crf.decode(emissions, mask=mask.bool())\n",
    "            return best_paths\n",
    "\n",
    "def train(model, dataloader, epochs=10, lr=1e-3, device=None):\n",
    "    \"\"\"Training function for the Segmenter model.\"\"\"\n",
    "    if device is None:\n",
    "        device = next(model.parameters()).device\n",
    "    \n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for x, y, lengths, mask in dataloader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            lengths = lengths.to(device)\n",
    "            mask = mask.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss = model(x, lengths, labels=y, mask=mask)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"epoch {epoch+1}: loss = {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140670ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_id(emb_dim, hidden_dim, epochs, batch_size, lr, vocab_size, model_type=\"full\"):\n",
    "    \"\"\"Hash training params to get unique model ID.\"\"\"\n",
    "    params_dict = {\n",
    "        'emb_dim': emb_dim,\n",
    "        'hidden_dim': hidden_dim,\n",
    "        'epochs': epochs,\n",
    "        'batch_size': batch_size,\n",
    "        'lr': lr,\n",
    "        'vocab_size': vocab_size,\n",
    "        'model_type': model_type\n",
    "    }\n",
    "    params_str = json.dumps(params_dict, sort_keys=True)\n",
    "    return hashlib.md5(params_str.encode()).hexdigest()[:16]\n",
    "\n",
    "def save_model_checkpoint(model, char2idx, model_id, models_folder=MODELS_FOLDER):\n",
    "    \"\"\"Save model checkpoint.\"\"\"\n",
    "    model_dir = os.path.join(models_folder, model_id)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    checkpoint_path = os.path.join(model_dir, \"segmenter_model.pt\")\n",
    "    torch.save({\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"char2idx\": char2idx\n",
    "    }, checkpoint_path)\n",
    "    \n",
    "    metadata_path = os.path.join(model_dir, \"metadata.json\")\n",
    "    with open(metadata_path, \"w\") as f:\n",
    "        json.dump({\n",
    "            'model_id': model_id,\n",
    "            'vocab_size': len(char2idx),\n",
    "            'model_name': MODEL_NAME\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    print(f\"saved checkpoint to {model_dir}\")\n",
    "    return model_dir\n",
    "\n",
    "def load_model_checkpoint(model_id, models_folder=MODELS_FOLDER):\n",
    "    \"\"\"Load model checkpoint.\"\"\"\n",
    "    model_dir = os.path.join(models_folder, model_id)\n",
    "    checkpoint_path = os.path.join(model_dir, \"segmenter_model.pt\")\n",
    "    \n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        return None\n",
    "    \n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    print(f\"loaded checkpoint from {model_dir}\")\n",
    "    return {\n",
    "        'model_state': checkpoint['model_state'],\n",
    "        'char2idx': checkpoint['char2idx'],\n",
    "        'checkpoint_path': checkpoint_path,\n",
    "        'model_dir': model_dir\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ee3c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare full dataset for training\n",
    "gold_df['char_seq'] = gold_df['Word'].apply(list)\n",
    "gold_df['boundary_labels'] = gold_df.apply(\n",
    "    lambda row: get_boundary_labels(row['Word'], row['Morph_split']), axis=1\n",
    ")\n",
    "\n",
    "# Save full dataset to JSONL\n",
    "output_path = os.path.join(DATA_FOLDER, \"stats_segmentation_data_full.jsonl\")\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for _, row in gold_df.iterrows():\n",
    "        record = {\n",
    "            \"chars\": row['char_seq'],\n",
    "            \"labels\": row['boundary_labels']\n",
    "        }\n",
    "        f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"full dataset saved to {output_path}\")\n",
    "print(f\"  total examples: {len(gold_df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbeaab54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "EMB_DIM = 64\n",
    "HIDDEN_DIM = 128\n",
    "EPOCHS = 15\n",
    "BATCH_SIZE = 16\n",
    "LR = 3e-3\n",
    "\n",
    "# Model 1: Trained on FULL dataset (with outliers)\n",
    "data_path = os.path.join(DATA_FOLDER, \"stats_segmentation_data_full.jsonl\")\n",
    "dataset = MorphemeDataset(data_path)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "vocab_size = len(dataset.char2idx)\n",
    "\n",
    "model_id_full = generate_model_id(EMB_DIM, HIDDEN_DIM, EPOCHS, BATCH_SIZE, LR, vocab_size, model_type=\"full\")\n",
    "\n",
    "print(f\"\\n=== MODEL 1: FULL DATASET (with outliers) ===\")\n",
    "print(f\"looking for model {model_id_full}...\")\n",
    "loaded_full = load_model_checkpoint(model_id_full, models_folder=MODELS_FOLDER)\n",
    "\n",
    "if loaded_full is not None:\n",
    "    print(f\"found it! loading from {loaded_full['model_dir']}\")\n",
    "    char2idx_full = loaded_full['char2idx']\n",
    "    model_full = Segmenter(vocab_size=len(char2idx_full), emb_dim=EMB_DIM, hidden_dim=HIDDEN_DIM).to(device)\n",
    "    model_full.load_state_dict(loaded_full['model_state'])\n",
    "    model_full.eval()\n",
    "    print(\"skipping training, model ready\")\n",
    "else:\n",
    "    print(f\"not found, training from scratch...\")\n",
    "    model_full = Segmenter(vocab_size, emb_dim=EMB_DIM, hidden_dim=HIDDEN_DIM).to(device)\n",
    "    train(model_full, dataloader, epochs=EPOCHS, lr=LR)\n",
    "    save_model_checkpoint(model_full, dataset.char2idx, model_id_full, models_folder=MODELS_FOLDER)\n",
    "    char2idx_full = dataset.char2idx\n",
    "    print(f\"\\nmodel 1 training done! model saved with ID: {model_id_full}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa769e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_segments(word, model, char2idx):\n",
    "    \"\"\"Predict morphological segmentation for a word.\"\"\"\n",
    "    model.eval()\n",
    "    x = torch.tensor([[char2idx.get(c, 0) for c in word]], dtype=torch.long).to(device)\n",
    "    lengths = torch.tensor([len(word)]).to(device)\n",
    "    mask = (x != 0).to(device)\n",
    "    with torch.no_grad():\n",
    "        label_seq = model(x, lengths, labels=None, mask=mask)[0]\n",
    "    \n",
    "    segments = []\n",
    "    start = 0\n",
    "    for i, label in enumerate(label_seq):\n",
    "        if label == 1:\n",
    "            segments.append(word[start:i+1])\n",
    "            start = i + 1\n",
    "    if start < len(word):\n",
    "        segments.append(word[start:])\n",
    "    return segments\n",
    "\n",
    "# Test Model 1\n",
    "print(\"model 1 (full dataset) example:\")\n",
    "print(f\"  pikunas -> {predict_segments('pikunas', model_full, char2idx_full)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5758a4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier detection and removal for Model 2\n",
    "gold_df3 = gold_df.copy()\n",
    "print(f\"original size: {gold_df3.shape}\")\n",
    "\n",
    "X = gold_df3[['word_len']]\n",
    "y = gold_df3['num_morphemes']\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "gold_df3['predicted'] = model.predict(X)\n",
    "gold_df3['residual'] = gold_df3['num_morphemes'] - gold_df3['predicted']\n",
    "\n",
    "std_residual = gold_df3['residual'].std()\n",
    "filtered_df = gold_df3[np.abs(gold_df3['residual']) <= std_residual].copy()\n",
    "print(f\"cleaned size (outliers removed): {filtered_df.shape}\")\n",
    "print(f\"outliers removed: {len(gold_df3) - len(filtered_df):,} examples ({100*(len(gold_df3) - len(filtered_df))/len(gold_df3):.1f}% of data)\")\n",
    "\n",
    "# Prepare filtered dataset\n",
    "filtered_df = filtered_df.copy()\n",
    "filtered_df['char_seq'] = filtered_df['Word'].apply(list)\n",
    "filtered_df['boundary_labels'] = filtered_df.apply(\n",
    "    lambda row: get_boundary_labels(row['Word'], row['Morph_split']), axis=1\n",
    ")\n",
    "\n",
    "# Save filtered dataset\n",
    "output_path_filtered = os.path.join(DATA_FOLDER, \"stats_segmentation_data_filtered.jsonl\")\n",
    "with open(output_path_filtered, \"w\", encoding=\"utf-8\") as f:\n",
    "    for _, row in filtered_df.iterrows():\n",
    "        json.dump({\n",
    "            \"chars\": row[\"char_seq\"],\n",
    "            \"labels\": row[\"boundary_labels\"]\n",
    "        }, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"filtered dataset saved to {output_path_filtered}\")\n",
    "print(f\"  total examples: {len(filtered_df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cd057e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2: Trained on FILTERED dataset (outliers removed)\n",
    "data_path_filtered = os.path.join(DATA_FOLDER, \"stats_segmentation_data_filtered.jsonl\")\n",
    "dataset2 = MorphemeDataset(data_path_filtered)\n",
    "dataloader2 = DataLoader(dataset2, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "vocab_size_filtered = len(dataset2.char2idx)\n",
    "\n",
    "model_id_filtered = generate_model_id(EMB_DIM, HIDDEN_DIM, EPOCHS, BATCH_SIZE, LR, vocab_size_filtered, model_type=\"filtered\")\n",
    "\n",
    "print(f\"\\n=== MODEL 2: FILTERED DATASET (outliers removed) ===\")\n",
    "print(f\"looking for model {model_id_filtered}...\")\n",
    "loaded_filtered = load_model_checkpoint(model_id_filtered, models_folder=MODELS_FOLDER)\n",
    "\n",
    "if loaded_filtered is not None:\n",
    "    print(f\"found it! loading from {loaded_filtered['model_dir']}\")\n",
    "    char2idx_filtered = loaded_filtered['char2idx']\n",
    "    model_filtered = Segmenter(vocab_size=len(char2idx_filtered), emb_dim=EMB_DIM, hidden_dim=HIDDEN_DIM).to(device)\n",
    "    model_filtered.load_state_dict(loaded_filtered['model_state'])\n",
    "    model_filtered.eval()\n",
    "    print(\"skipping training, model ready\")\n",
    "else:\n",
    "    print(f\"not found, training from scratch...\")\n",
    "    model_filtered = Segmenter(vocab_size_filtered, emb_dim=EMB_DIM, hidden_dim=HIDDEN_DIM).to(device)\n",
    "    train(model_filtered, dataloader2, epochs=EPOCHS, lr=LR)\n",
    "    save_model_checkpoint(model_filtered, dataset2.char2idx, model_id_filtered, models_folder=MODELS_FOLDER)\n",
    "    char2idx_filtered = dataset2.char2idx\n",
    "    print(f\"\\nmodel 2 training done! model saved with ID: {model_id_filtered}\")\n",
    "\n",
    "# Test Model 2\n",
    "print(\"model 2 (filtered dataset) example:\")\n",
    "print(f\"  pikunas -> {predict_segments('pikunas', model_filtered, char2idx_filtered)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46455a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_kfold_cross_validation(\n",
    "    df,\n",
    "    n_folds=5,\n",
    "    emb_dim=64,\n",
    "    hidden_dim=128,\n",
    "    epochs=15,\n",
    "    batch_size=16,\n",
    "    lr=3e-3,\n",
    "    random_state=42,\n",
    "    device=device\n",
    "):\n",
    "    \"\"\"K-fold cross-validation for more robust evaluation.\"\"\"\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"K-FOLD CV (k={n_folds}) WITH BILSTM+CRF\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    \n",
    "    kfold = KFold(n_splits=n_folds, shuffle=True, random_state=random_state)\n",
    "    indices = np.arange(len(df))\n",
    "    \n",
    "    fold_results = []\n",
    "    all_metrics = {\n",
    "        'val_loss': [],\n",
    "        'exact_match': []\n",
    "    }\n",
    "    \n",
    "    for fold_idx, (train_indices, val_indices) in enumerate(kfold.split(indices), 1):\n",
    "        print(f\"\\n--- fold {fold_idx}/{n_folds} ---\")\n",
    "        print(f\"train: {len(train_indices)}, val: {len(val_indices)}\")\n",
    "        \n",
    "        train_df_fold = df.iloc[train_indices].reset_index(drop=True)\n",
    "        val_df_fold = df.iloc[val_indices].reset_index(drop=True)\n",
    "        \n",
    "        import tempfile\n",
    "        import os as os_module\n",
    "        \n",
    "        temp_dir = tempfile.mkdtemp()\n",
    "        train_path_fold = os_module.path.join(temp_dir, f\"train_fold_{fold_idx}.jsonl\")\n",
    "        val_path_fold = os_module.path.join(temp_dir, f\"val_fold_{fold_idx}.jsonl\")\n",
    "        \n",
    "        with open(train_path_fold, \"w\", encoding=\"utf-8\") as f:\n",
    "            for _, row in train_df_fold.iterrows():\n",
    "                record = {\n",
    "                    \"chars\": row['char_seq'],\n",
    "                    \"labels\": row['boundary_labels']\n",
    "                }\n",
    "                f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "        \n",
    "        with open(val_path_fold, \"w\", encoding=\"utf-8\") as f:\n",
    "            for _, row in val_df_fold.iterrows():\n",
    "                record = {\n",
    "                    \"chars\": row['char_seq'],\n",
    "                    \"labels\": row['boundary_labels']\n",
    "                }\n",
    "                f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "        \n",
    "        train_dataset_fold = MorphemeDataset(train_path_fold)\n",
    "        val_dataset_fold = MorphemeDataset(val_path_fold, char2idx=train_dataset_fold.char2idx)\n",
    "        train_loader_fold = DataLoader(train_dataset_fold, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "        val_loader_fold = DataLoader(val_dataset_fold, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "        \n",
    "        vocab_size_fold = len(train_dataset_fold.char2idx)\n",
    "        \n",
    "        model_fold = Segmenter(vocab_size=vocab_size_fold, emb_dim=emb_dim, hidden_dim=hidden_dim).to(device)\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        best_epoch = 0\n",
    "        best_exact_match = 0.0\n",
    "        \n",
    "        optimizer_fold = torch.optim.Adam(model_fold.parameters(), lr=lr)\n",
    "        \n",
    "        for epoch in range(1, epochs + 1):\n",
    "            model_fold.train()\n",
    "            total_loss = 0.0\n",
    "            for x, y, lengths, mask in train_loader_fold:\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                lengths = lengths.to(device)\n",
    "                mask = mask.to(device)\n",
    "                \n",
    "                optimizer_fold.zero_grad()\n",
    "                loss = model_fold(x, lengths, labels=y, mask=mask)\n",
    "                loss.backward()\n",
    "                optimizer_fold.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            train_loss = total_loss / len(train_loader_fold)\n",
    "            \n",
    "            model_fold.eval()\n",
    "            val_loss = 0.0\n",
    "            exact_matches = 0\n",
    "            total_val = 0\n",
    "            \n",
    "            def predict_segments_fold(word, model, char2idx):\n",
    "                \"\"\"Fold-specific version of predict_segments.\"\"\"\n",
    "                model.eval()\n",
    "                x = torch.tensor([[char2idx.get(c, 0) for c in word]], dtype=torch.long).to(device)\n",
    "                lengths = torch.tensor([len(word)]).to(device)\n",
    "                mask = (x != 0).to(device)\n",
    "                with torch.no_grad():\n",
    "                    label_seq = model(x, lengths, labels=None, mask=mask)[0]\n",
    "                segments = []\n",
    "                start = 0\n",
    "                for i, label in enumerate(label_seq):\n",
    "                    if label == 1:\n",
    "                        segments.append(word[start:i+1])\n",
    "                        start = i + 1\n",
    "                if start < len(word):\n",
    "                    segments.append(word[start:])\n",
    "                return segments\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for x, y, lengths, mask in val_loader_fold:\n",
    "                    x = x.to(device)\n",
    "                    y = y.to(device)\n",
    "                    lengths = lengths.to(device)\n",
    "                    mask = mask.to(device)\n",
    "                    \n",
    "                    loss = model_fold(x, lengths, labels=y, mask=mask)\n",
    "                    val_loss += loss.item()\n",
    "                \n",
    "                for i in range(len(val_dataset_fold)):\n",
    "                    word = val_df_fold.iloc[i]['Word']\n",
    "                    gold_split = val_df_fold.iloc[i]['Morph_split']\n",
    "                    \n",
    "                    predicted_segments = predict_segments_fold(word, model_fold, train_dataset_fold.char2idx)\n",
    "                    \n",
    "                    if predicted_segments == gold_split:\n",
    "                        exact_matches += 1\n",
    "                    total_val += 1\n",
    "            \n",
    "            val_loss = val_loss / len(val_loader_fold)\n",
    "            exact_match_rate = exact_matches / total_val if total_val > 0 else 0.0\n",
    "            \n",
    "            print(f\"  ep {epoch:02d} | train_loss={train_loss:.4f}  val_loss={val_loss:.4f}  exact_match={exact_match_rate:.3f}\")\n",
    "            \n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_exact_match = exact_match_rate\n",
    "                best_epoch = epoch\n",
    "        \n",
    "        print(f\"\\n  best epoch: {best_epoch}\")\n",
    "        print(f\"  best validation: loss={best_val_loss:.4f}  exact_match={best_exact_match:.3f}\")\n",
    "        \n",
    "        try:\n",
    "            os_module.remove(train_path_fold)\n",
    "            os_module.remove(val_path_fold)\n",
    "            os_module.rmdir(temp_dir)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        fold_results.append({\n",
    "            'fold': fold_idx,\n",
    "            'val_loss': best_val_loss,\n",
    "            'exact_match': best_exact_match,\n",
    "            'best_epoch': best_epoch\n",
    "        })\n",
    "        \n",
    "        all_metrics['val_loss'].append(best_val_loss)\n",
    "        all_metrics['exact_match'].append(best_exact_match)\n",
    "    \n",
    "    mean_metrics = {k: np.mean(v) for k, v in all_metrics.items()}\n",
    "    std_metrics = {k: np.std(v) for k, v in all_metrics.items()}\n",
    "    best_fold_idx = min(range(len(fold_results)), key=lambda i: fold_results[i]['val_loss'])\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(\"CV SUMMARY\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    for r in fold_results:\n",
    "        print(f\"  fold {r['fold']}: loss={r['val_loss']:.4f}, exact_match={r['exact_match']:.3f}\")\n",
    "    \n",
    "    print(f\"\\nmean +/- std over {n_folds} folds:\")\n",
    "    print(f\"  validation loss:   {mean_metrics['val_loss']:.4f} +/- {std_metrics['val_loss']:.4f}\")\n",
    "    print(f\"  exact match rate:  {mean_metrics['exact_match']:.3f} +/- {std_metrics['exact_match']:.3f}\")\n",
    "    print(f\"\\nbest fold: {fold_results[best_fold_idx]['fold']} \"\n",
    "          f\"(loss: {fold_results[best_fold_idx]['val_loss']:.4f}, \"\n",
    "          f\"exact_match: {fold_results[best_fold_idx]['exact_match']:.3f})\")\n",
    "    print(f\"{'=' * 80}\\n\")\n",
    "    \n",
    "    return {\n",
    "        'fold_results': fold_results,\n",
    "        'mean_metrics': mean_metrics,\n",
    "        'std_metrics': std_metrics,\n",
    "        'best_fold_idx': best_fold_idx,\n",
    "        'all_metrics': all_metrics\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3410ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run k-fold cross-validation on full dataset\n",
    "kfold_results_full = run_kfold_cross_validation(\n",
    "    df=gold_df,\n",
    "    n_folds=5,\n",
    "    emb_dim=EMB_DIM if 'EMB_DIM' in globals() else 64,\n",
    "    hidden_dim=HIDDEN_DIM if 'HIDDEN_DIM' in globals() else 128,\n",
    "    epochs=EPOCHS if 'EPOCHS' in globals() else 15,\n",
    "    batch_size=BATCH_SIZE if 'BATCH_SIZE' in globals() else 16,\n",
    "    lr=LR if 'LR' in globals() else 3e-3,\n",
    "    random_state=42,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"\\navg exact match rate: {kfold_results_full['mean_metrics']['exact_match']:.3f} +/- {kfold_results_full['std_metrics']['exact_match']:.3f}\")\n",
    "print(f\"avg validation loss: {kfold_results_full['mean_metrics']['val_loss']:.4f} +/- {kfold_results_full['std_metrics']['val_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081b58c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_correct_prediction(predicted, gold_variants):\n",
    "    \"\"\"Check if predicted segmentation matches any gold variant.\"\"\"\n",
    "    if gold_variants is None:\n",
    "        return False\n",
    "    \n",
    "    if isinstance(gold_variants, np.ndarray):\n",
    "        gold_variants = gold_variants.tolist()\n",
    "    \n",
    "    if isinstance(gold_variants, list):\n",
    "        normalized = []\n",
    "        for variant in gold_variants:\n",
    "            if isinstance(variant, np.ndarray):\n",
    "                normalized.append(variant.tolist())\n",
    "            elif isinstance(variant, list):\n",
    "                normalized.append([item.tolist() if isinstance(item, np.ndarray) else item for item in variant])\n",
    "            else:\n",
    "                normalized.append(variant)\n",
    "        gold_variants = normalized\n",
    "    \n",
    "    return any(predicted == variant for variant in gold_variants)\n",
    "\n",
    "def split_count_metrics(predicted_segments, gold_variants):\n",
    "    \"\"\"Compute split-count accuracy variants.\"\"\"\n",
    "    pred_count = len(predicted_segments)\n",
    "    \n",
    "    if gold_variants is None:\n",
    "        return {\"Exact\": False, \"+1\": False, \"-1\": False, \"±1\": False}\n",
    "    \n",
    "    if isinstance(gold_variants, np.ndarray):\n",
    "        gold_variants = gold_variants.tolist()\n",
    "    \n",
    "    if isinstance(gold_variants, list):\n",
    "        normalized = []\n",
    "        for variant in gold_variants:\n",
    "            if isinstance(variant, np.ndarray):\n",
    "                normalized.append(variant.tolist())\n",
    "            elif isinstance(variant, list):\n",
    "                normalized.append([item.tolist() if isinstance(item, np.ndarray) else item for item in variant])\n",
    "            else:\n",
    "                normalized.append(variant)\n",
    "        gold_variants = normalized\n",
    "    \n",
    "    gold_counts = [len(gold) for gold in gold_variants]\n",
    "\n",
    "    exact = any(pred_count == g for g in gold_counts)\n",
    "    plus1 = any(pred_count == g + 1 for g in gold_counts)\n",
    "    minus1 = any(pred_count == g - 1 for g in gold_counts)\n",
    "    pm1 = any(abs(pred_count - g) <= 1 for g in gold_counts)\n",
    "\n",
    "    return {\"Exact\": exact, \"+1\": plus1, \"-1\": minus1, \"±1\": pm1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11b0d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "print(\"loading test data...\")\n",
    "test_df = pd.read_parquet(os.path.join(DATA_FOLDER, \"cleaned_data_df.parquet\"))\n",
    "print(f\"loaded {len(test_df):,} test examples\")\n",
    "\n",
    "# Evaluate both models\n",
    "records_full = []\n",
    "records_filtered = []\n",
    "all_words = test_df[\"Word\"].tolist()\n",
    "\n",
    "print(\"\\nevaluating model 1 (full dataset with outliers)...\")\n",
    "for word in all_words:\n",
    "    predicted_full = predict_segments(word, model_full, char2idx_full)\n",
    "    gold_variants = test_df[test_df[\"Word\"] == word][\"Gold\"].iloc[0] if len(test_df[test_df[\"Word\"] == word]) > 0 else []\n",
    "    \n",
    "    correct_exact_full = is_correct_prediction(predicted_full, gold_variants)\n",
    "    split_metrics_full = split_count_metrics(predicted_full, gold_variants)\n",
    "    \n",
    "    records_full.append({\n",
    "        \"Word\": word,\n",
    "        \"Prediction\": predicted_full,\n",
    "        \"Gold\": gold_variants,\n",
    "        \"CorrectExactSeg\": correct_exact_full,\n",
    "        \"CorrectSplitCount\": split_metrics_full[\"Exact\"],\n",
    "        \"SplitCount+1\": split_metrics_full[\"+1\"],\n",
    "        \"SplitCount-1\": split_metrics_full[\"-1\"],\n",
    "        \"SplitCount±1\": split_metrics_full[\"±1\"],\n",
    "        \"OverlapExactAndSplit\": correct_exact_full and split_metrics_full[\"Exact\"]\n",
    "    })\n",
    "\n",
    "print(\"evaluating model 2 (filtered dataset without outliers)...\")\n",
    "for word in all_words:\n",
    "    predicted_filtered = predict_segments(word, model_filtered, char2idx_filtered)\n",
    "    gold_variants = test_df[test_df[\"Word\"] == word][\"Gold\"].iloc[0] if len(test_df[test_df[\"Word\"] == word]) > 0 else []\n",
    "    \n",
    "    correct_exact_filtered = is_correct_prediction(predicted_filtered, gold_variants)\n",
    "    split_metrics_filtered = split_count_metrics(predicted_filtered, gold_variants)\n",
    "    \n",
    "    records_filtered.append({\n",
    "        \"Word\": word,\n",
    "        \"Prediction\": predicted_filtered,\n",
    "        \"Gold\": gold_variants,\n",
    "        \"CorrectExactSeg\": correct_exact_filtered,\n",
    "        \"CorrectSplitCount\": split_metrics_filtered[\"Exact\"],\n",
    "        \"SplitCount+1\": split_metrics_filtered[\"+1\"],\n",
    "        \"SplitCount-1\": split_metrics_filtered[\"-1\"],\n",
    "        \"SplitCount±1\": split_metrics_filtered[\"±1\"],\n",
    "        \"OverlapExactAndSplit\": correct_exact_filtered and split_metrics_filtered[\"Exact\"]\n",
    "    })\n",
    "\n",
    "results_full_df = pd.DataFrame(records_full)\n",
    "results_filtered_df = pd.DataFrame(records_filtered)\n",
    "\n",
    "# Compute aggregate metrics\n",
    "accuracy_full = results_full_df[\"CorrectExactSeg\"].mean()\n",
    "split_exact_full = results_full_df[\"CorrectSplitCount\"].mean()\n",
    "split_plus1_full = results_full_df[\"SplitCount+1\"].mean()\n",
    "split_minus1_full = results_full_df[\"SplitCount-1\"].mean()\n",
    "split_pm1_full = results_full_df[\"SplitCount±1\"].mean()\n",
    "overlap_full = results_full_df[\"OverlapExactAndSplit\"].mean()\n",
    "\n",
    "accuracy_filtered = results_filtered_df[\"CorrectExactSeg\"].mean()\n",
    "split_exact_filtered = results_filtered_df[\"CorrectSplitCount\"].mean()\n",
    "split_plus1_filtered = results_filtered_df[\"SplitCount+1\"].mean()\n",
    "split_minus1_filtered = results_filtered_df[\"SplitCount-1\"].mean()\n",
    "split_pm1_filtered = results_filtered_df[\"SplitCount±1\"].mean()\n",
    "overlap_filtered = results_filtered_df[\"OverlapExactAndSplit\"].mean()\n",
    "\n",
    "# Comparison results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"comparing models: full dataset vs filtered dataset (outliers removed)\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nchecking if removing outliers helps or hurts\")\n",
    "print(f\"outliers removed: {len(gold_df) - len(filtered_df):,} examples ({100*(len(gold_df) - len(filtered_df))/len(gold_df):.1f}% of data)\")\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(f\"{'Metric':<30} {'Model 1 (Full)':<20} {'Model 2 (Filtered)':<20} {'Difference':<15}\")\n",
    "print(\"-\"*80)\n",
    "print(f\"{'Exact Segmentation Accuracy':<30} {accuracy_full:<20.4f} {accuracy_filtered:<20.4f} {accuracy_filtered-accuracy_full:+.4f}\")\n",
    "print(f\"{'Split-count (Exact)':<30} {split_exact_full:<20.4f} {split_exact_filtered:<20.4f} {split_exact_filtered-split_exact_full:+.4f}\")\n",
    "print(f\"{'Split-count (+1)':<30} {split_plus1_full:<20.4f} {split_plus1_filtered:<20.4f} {split_plus1_filtered-split_plus1_full:+.4f}\")\n",
    "print(f\"{'Split-count (−1)':<30} {split_minus1_full:<20.4f} {split_minus1_filtered:<20.4f} {split_minus1_filtered-split_minus1_full:+.4f}\")\n",
    "print(f\"{'Split-count (±1)':<30} {split_pm1_full:<20.4f} {split_pm1_filtered:<20.4f} {split_pm1_filtered-split_pm1_full:+.4f}\")\n",
    "print(f\"{'Overlap (Exact ∩ Split)':<30} {overlap_full:<20.4f} {overlap_filtered:<20.4f} {overlap_filtered-overlap_full:+.4f}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "if accuracy_filtered > accuracy_full:\n",
    "    print(f\"\\nmodel 2 (filtered) did better than model 1 (full)\")\n",
    "    print(f\"  went up by {100*(accuracy_filtered-accuracy_full):.2f} percentage points\")\n",
    "    print(\"  so removing outliers seems to help\")\n",
    "elif accuracy_filtered < accuracy_full:\n",
    "    print(f\"\\nmodel 2 (filtered) did worse than model 1 (full)\")\n",
    "    print(f\"  went down by {100*(accuracy_full-accuracy_filtered):.2f} percentage points\")\n",
    "    print(\"  so removing outliers seems to hurt\")\n",
    "else:\n",
    "    print(f\"\\nmodel 2 (filtered) did the same as model 1 (full)\")\n",
    "    print(\"  so removing outliers doesn't seem to matter\")\n",
    "\n",
    "# Save results\n",
    "results_full_path = os.path.join(DATA_FOLDER, \"stats_model_full_eval_results.csv\")\n",
    "results_filtered_path = os.path.join(DATA_FOLDER, \"stats_model_filtered_eval_results.csv\")\n",
    "\n",
    "results_full_df.to_csv(results_full_path, index=False)\n",
    "results_filtered_df.to_csv(results_filtered_path, index=False)\n",
    "\n",
    "print(f\"\\nevaluation results saved:\")\n",
    "print(f\"  model 1 (full): {results_full_path}\")\n",
    "print(f\"  model 2 (filtered): {results_filtered_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
