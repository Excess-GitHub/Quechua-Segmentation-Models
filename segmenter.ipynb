{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bac9a685",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SEGMENTER: GRAPHEME-LEVEL BILSTM MORPHOLOGY PARSER\n",
    "==================================================\n",
    "\n",
    "This notebook implements a character-level BiLSTM model for morphological segmentation\n",
    "of Quechua words using grapheme-level tokenization. Unlike segmenter-old.ipynb which\n",
    "uses character-level tokenization, this notebook uses linguistically-informed grapheme\n",
    "tokenization that recognizes Quechua multigraphs (e.g., \"ch\", \"ll\", \"rr\", \"sh\", etc.).\n",
    "\n",
    "Key Features:\n",
    "- Grapheme-level tokenization (recognizes Quechua multigraphs like \"ch\", \"ll\", \"rr\")\n",
    "- BiLSTM architecture for sequence labeling\n",
    "- Binary classification: predicts boundary (1) or no boundary (0) at each grapheme position\n",
    "- Comprehensive evaluation metrics (precision, recall, F1, exact match, split-count accuracy)\n",
    "- Model checkpointing to avoid redundant training\n",
    "\n",
    "Key Differences from segmenter-old.ipynb:\n",
    "- Uses grapheme tokenization (multigraph-aware) instead of simple character tokenization\n",
    "- More linguistically informed for Quechua morphology\n",
    "- Handles ejectives and special Quechua graphemes properly\n",
    "\n",
    "All data is read from the 'data' folder and models are saved to the 'models_segmenter' folder.\n",
    "\"\"\"\n",
    "\n",
    "import ast\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4be2176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading gold standard data...\n",
      "Loaded 6,896 gold standard examples\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# DATA FOLDER CONFIGURATION\n",
    "# =========================\n",
    "# All data files should be read from and saved to the data folder\n",
    "DATA_FOLDER = \"data\"\n",
    "\n",
    "# Model folder named after this notebook\n",
    "MODEL_NAME = \"segmenter\"\n",
    "MODELS_FOLDER = f\"models_{MODEL_NAME}\"\n",
    "\n",
    "# Create folders if they don't exist\n",
    "os.makedirs(DATA_FOLDER, exist_ok=True)\n",
    "os.makedirs(MODELS_FOLDER, exist_ok=True)\n",
    "\n",
    "# =========================\n",
    "# LOAD GOLD STANDARD DATA\n",
    "# =========================\n",
    "# The gold standard dataset contains high-quality morphological segmentations\n",
    "# This is the base training data for the grapheme-level BiLSTM model\n",
    "print(\"Loading gold standard data...\")\n",
    "gold_df = pd.read_parquet(os.path.join(DATA_FOLDER, \"Sue_kalt.parquet\"))\n",
    "gold_df['Word'] = gold_df['word']\n",
    "gold_df['morph'] = gold_df['morph'].str.replace('-', ' ')  # Normalize separators\n",
    "gold_df['Morph_split_str'] = gold_df['morph']  # String version\n",
    "gold_df['Morph_split'] = gold_df['morph'].str.split(' ')  # List version\n",
    "gold_df = gold_df[['Word', 'Morph_split', 'Morph_split_str']]\n",
    "gold_df.drop_duplicates(subset='Word', keep='first', inplace=True)\n",
    "gold_df.dropna(subset=['Word'], inplace=True)\n",
    "print(f\"Loaded {len(gold_df):,} gold standard examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32002a1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Morph_split</th>\n",
       "      <th>Morph_split_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cementerioman</td>\n",
       "      <td>[cementerio, man]</td>\n",
       "      <td>cementerio man</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kawsachkananta</td>\n",
       "      <td>[kawsa, chka, na, n, ta]</td>\n",
       "      <td>kawsa chka na n ta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mañakunpis</td>\n",
       "      <td>[maña, ku, n, pis]</td>\n",
       "      <td>maña ku n pis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>imaynapichus</td>\n",
       "      <td>[imayna, pi, chus]</td>\n",
       "      <td>imayna pi chus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>qipiyuq</td>\n",
       "      <td>[qipi, yuq]</td>\n",
       "      <td>qipi yuq</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Word               Morph_split     Morph_split_str\n",
       "0   cementerioman         [cementerio, man]      cementerio man\n",
       "1  kawsachkananta  [kawsa, chka, na, n, ta]  kawsa chka na n ta\n",
       "2      mañakunpis        [maña, ku, n, pis]       maña ku n pis\n",
       "3    imaynapichus        [imayna, pi, chus]      imayna pi chus\n",
       "4         qipiyuq               [qipi, yuq]            qipi yuq"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79117e28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6896, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de2e2953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%  (put this near your imports)\n",
    "import unicodedata, regex as re\n",
    "import string\n",
    "\n",
    "# >>> CHANGED: unify apostrophes to a single codepoint for ejectives, etc.\n",
    "APOSTROPHE_CHARS = {\"'\", \"’\", \"ʼ\", \"‛\", \"`\"}\n",
    "STD_APOS = \"\\u02BC\"  # ʼ\n",
    "\n",
    "# build a translation table that deletes punctuation\n",
    "_EXTRA_PUNCT = \"±，“”‘’\"   # add any more special symbols you want stripped\n",
    "_DELETE = str.maketrans(\"\", \"\", string.punctuation + _EXTRA_PUNCT)\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    # NFC compose; lowercase; unify apostrophes\n",
    "    s = unicodedata.normalize(\"NFC\", str(s)).lower()\n",
    "    s = \"\".join(STD_APOS if ch in APOSTROPHE_CHARS else ch for ch in s)\n",
    "    # remove punctuation (ASCII + extras) and strip whitespace\n",
    "    s = s.translate(_DELETE).strip()\n",
    "    return s\n",
    "\n",
    "# >>> CHANGED: Quechua multigraph inventory (extend if your corpus has more)\n",
    "QUECHUA_MULTIGRAPHS = [\n",
    "    \"ch\"+STD_APOS, \"k\"+STD_APOS, \"p\"+STD_APOS, \"q\"+STD_APOS, \"t\"+STD_APOS,  # ejectives (optional)\n",
    "    \"ch\", \"ph\", \"qh\", \"kh\", \"ll\", \"rr\", \"sh\",\n",
    "]\n",
    "MG_SET = set(QUECHUA_MULTIGRAPHS)\n",
    "MAX_MG = max((len(mg) for mg in QUECHUA_MULTIGRAPHS), default=1)\n",
    "\n",
    "def to_graphemes_quechua(s: str) -> list[str]:\n",
    "    \"\"\"Greedy longest-match multigraph fusion; fallback to Unicode grapheme clusters (\\X).\"\"\"\n",
    "    s = normalize_text(s)\n",
    "    tokens, i, n = [], 0, len(s)\n",
    "    while i < n:\n",
    "        match = None\n",
    "        for L in range(MAX_MG, 1, -1):\n",
    "            if i + L <= n:\n",
    "                cand = s[i:i+L]\n",
    "                if cand in MG_SET:\n",
    "                    match = cand\n",
    "                    break\n",
    "        if match:\n",
    "            tokens.append(match)\n",
    "            i += len(match)\n",
    "        else:\n",
    "            m = re.match(r\"\\X\", s[i:])  # single Unicode grapheme cluster\n",
    "            g = m.group(0)\n",
    "            tokens.append(g)\n",
    "            i += len(g)\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96dcfa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# TOKENIZE WORDS AND MORPHEMES\n",
    "# =========================\n",
    "# Convert words and morphemes to grapheme token sequences\n",
    "# This prepares the data for the grapheme-level BiLSTM model\n",
    "\n",
    "gold_df['token_seq'] = gold_df['Word'].apply(lambda w: to_graphemes_quechua(w))\n",
    "gold_df['morph_token_splits'] = gold_df['Morph_split'].apply(\n",
    "    lambda var: [to_graphemes_quechua(m) for m in var]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99d57829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Morph_split</th>\n",
       "      <th>Morph_split_str</th>\n",
       "      <th>token_seq</th>\n",
       "      <th>morph_token_splits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cementerioman</td>\n",
       "      <td>[cementerio, man]</td>\n",
       "      <td>cementerio man</td>\n",
       "      <td>[c, e, m, e, n, t, e, r, i, o, m, a, n]</td>\n",
       "      <td>[[c, e, m, e, n, t, e, r, i, o], [m, a, n]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kawsachkananta</td>\n",
       "      <td>[kawsa, chka, na, n, ta]</td>\n",
       "      <td>kawsa chka na n ta</td>\n",
       "      <td>[k, a, w, s, a, ch, k, a, n, a, n, t, a]</td>\n",
       "      <td>[[k, a, w, s, a], [ch, k, a], [n, a], [n], [t,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mañakunpis</td>\n",
       "      <td>[maña, ku, n, pis]</td>\n",
       "      <td>maña ku n pis</td>\n",
       "      <td>[m, a, ñ, a, k, u, n, p, i, s]</td>\n",
       "      <td>[[m, a, ñ, a], [k, u], [n], [p, i, s]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>imaynapichus</td>\n",
       "      <td>[imayna, pi, chus]</td>\n",
       "      <td>imayna pi chus</td>\n",
       "      <td>[i, m, a, y, n, a, p, i, ch, u, s]</td>\n",
       "      <td>[[i, m, a, y, n, a], [p, i], [ch, u, s]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>qipiyuq</td>\n",
       "      <td>[qipi, yuq]</td>\n",
       "      <td>qipi yuq</td>\n",
       "      <td>[q, i, p, i, y, u, q]</td>\n",
       "      <td>[[q, i, p, i], [y, u, q]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Word               Morph_split     Morph_split_str  \\\n",
       "0   cementerioman         [cementerio, man]      cementerio man   \n",
       "1  kawsachkananta  [kawsa, chka, na, n, ta]  kawsa chka na n ta   \n",
       "2      mañakunpis        [maña, ku, n, pis]       maña ku n pis   \n",
       "3    imaynapichus        [imayna, pi, chus]      imayna pi chus   \n",
       "4         qipiyuq               [qipi, yuq]            qipi yuq   \n",
       "\n",
       "                                  token_seq  \\\n",
       "0   [c, e, m, e, n, t, e, r, i, o, m, a, n]   \n",
       "1  [k, a, w, s, a, ch, k, a, n, a, n, t, a]   \n",
       "2            [m, a, ñ, a, k, u, n, p, i, s]   \n",
       "3        [i, m, a, y, n, a, p, i, ch, u, s]   \n",
       "4                     [q, i, p, i, y, u, q]   \n",
       "\n",
       "                                  morph_token_splits  \n",
       "0        [[c, e, m, e, n, t, e, r, i, o], [m, a, n]]  \n",
       "1  [[k, a, w, s, a], [ch, k, a], [n, a], [n], [t,...  \n",
       "2             [[m, a, ñ, a], [k, u], [n], [p, i, s]]  \n",
       "3           [[i, m, a, y, n, a], [p, i], [ch, u, s]]  \n",
       "4                          [[q, i, p, i], [y, u, q]]  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd38ee28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# BOUNDARY LABEL GENERATION\n",
    "# =========================\n",
    "# Convert morpheme splits into grapheme-level boundary labels\n",
    "# Labels mark the end position of each morpheme (except the last one)\n",
    "\n",
    "def get_boundary_labels_tokens(tokens: list[str], morph_tokens: list[list[str]]) -> list[int]:\n",
    "    \"\"\"\n",
    "    Generate binary boundary labels for a word given its morpheme token splits.\n",
    "    \n",
    "    Args:\n",
    "        tokens: List of grapheme tokens for the full word\n",
    "        morph_tokens: List of morpheme token lists (each morpheme is a list of graphemes)\n",
    "    \n",
    "    Returns:\n",
    "        List of binary labels (0=no boundary, 1=boundary) for each grapheme position\n",
    "        The label at position i indicates if there's a boundary after grapheme i\n",
    "    \"\"\"\n",
    "    labels = [0] * len(tokens)\n",
    "    idx = 0\n",
    "    # All but last morpheme end in a boundary\n",
    "    for mt in morph_tokens[:-1]:\n",
    "        idx += len(mt)\n",
    "        if 0 < idx <= len(tokens):\n",
    "            labels[idx-1] = 1  # Boundary at the end of this morpheme\n",
    "    return labels\n",
    "\n",
    "gold_df['boundary_labels'] = gold_df.apply(\n",
    "    lambda row: get_boundary_labels_tokens(row['token_seq'], row['morph_token_splits']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Optional diagnostics: track morpheme counts and token lengths\n",
    "gold_df['num_morphemes'] = gold_df['Morph_split'].apply(len)\n",
    "gold_df['word_len_tokens'] = gold_df['token_seq'].apply(len)\n",
    "\n",
    "# For compatibility with model code, use 'char_seq' to refer to token sequences\n",
    "gold_df['char_seq'] = gold_df['token_seq']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7030288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Vocabulary size: 42 graphemes\n",
      "Training samples: 6,206\n",
      "Validation samples: 690\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# PYTORCH IMPORTS AND SETUP\n",
    "# =========================\n",
    "# Import libraries for neural network training and data handling\n",
    "\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Set device (GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# =========================\n",
    "# VOCABULARY CONSTRUCTION\n",
    "# =========================\n",
    "# Build grapheme-level vocabulary for embedding layer\n",
    "# Each unique grapheme (including multigraphs) gets an integer ID\n",
    "\n",
    "PAD, UNK = \"<PAD>\", \"<UNK>\"  # Special tokens for padding and unknown graphemes\n",
    "\n",
    "def build_vocab(seqs: List[List[str]]):\n",
    "    \"\"\"\n",
    "    Build vocabulary from grapheme token sequences.\n",
    "    \n",
    "    Args:\n",
    "        seqs: List of grapheme token sequences (each sequence is a list of graphemes)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (stoi, itos):\n",
    "        - stoi: Dictionary mapping grapheme to integer ID\n",
    "        - itos: List mapping integer ID to grapheme\n",
    "    \"\"\"\n",
    "    toks = {t for seq in seqs for t in seq}  # Collect all unique graphemes\n",
    "    itos = [PAD, UNK] + sorted(toks)  # Index-to-string: [PAD, UNK, 'a', 'b', 'ch', ...]\n",
    "    stoi = {t:i for i,t in enumerate(itos)}  # String-to-index dictionary\n",
    "    return stoi, itos\n",
    "\n",
    "# Build vocabulary from all grapheme sequences in the gold data\n",
    "stoi, itos = build_vocab(gold_df[\"char_seq\"].tolist())\n",
    "print(f\"Vocabulary size: {len(itos)} graphemes\")\n",
    "\n",
    "def encode(seq: List[str]) -> List[int]:\n",
    "    \"\"\"Convert grapheme sequence to integer IDs.\"\"\"\n",
    "    return [stoi.get(t, stoi[UNK]) for t in seq]\n",
    "\n",
    "def encode_labels(labels: List[int]) -> List[int]:\n",
    "    \"\"\"Labels are already 0/1, so just return them as-is.\"\"\"\n",
    "    return labels\n",
    "\n",
    "# =========================\n",
    "# DATASET AND DATALOADER\n",
    "# =========================\n",
    "# PyTorch Dataset and DataLoader for batching and padding sequences\n",
    "\n",
    "class CharBoundaryDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for grapheme-level boundary prediction.\n",
    "    Each sample contains a grapheme sequence and its boundary labels.\n",
    "    \"\"\"\n",
    "    def __init__(self, df):\n",
    "        self.x = df[\"char_seq\"].tolist()  # Grapheme sequences\n",
    "        self.y = df[\"boundary_labels\"].tolist()  # Boundary labels\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "def pad_batch(batch, pad_id=0):\n",
    "    \"\"\"\n",
    "    Collate function for DataLoader: pads sequences to the same length.\n",
    "    \n",
    "    Args:\n",
    "        batch: List of (grapheme_sequence, boundary_labels) tuples\n",
    "        pad_id: ID to use for padding (default: 0, which is PAD token)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of tensors:\n",
    "        - x_pad: Padded grapheme sequences (B, T)\n",
    "        - y_pad: Padded boundary labels (B, T)\n",
    "        - mask: Boolean mask indicating valid positions (B, T)\n",
    "        - lengths: Actual length of each sequence (B,)\n",
    "    \"\"\"\n",
    "    # batch: List[ (List[str], List[int]) ]\n",
    "    seqs, labels = zip(*batch)\n",
    "    x_ids = [encode(s) for s in seqs]  # Convert graphemes to IDs\n",
    "    y_ids = [encode_labels(y) for y in labels]  # Labels are already 0/1\n",
    "    lengths = [len(x) for x in x_ids]\n",
    "    maxlen = max(lengths)\n",
    "    \n",
    "    # Pad sequences and labels to maxlen\n",
    "    x_pad = [xi + [pad_id]*(maxlen - len(xi)) for xi in x_ids]\n",
    "    y_pad = [yi + [0]*(maxlen - len(yi)) for yi in y_ids]  # Pad labels as 0 (will be masked)\n",
    "    mask  = [[1]*len(xi) + [0]*(maxlen - len(xi)) for xi in x_ids]  # 1 for valid, 0 for padding\n",
    "    \n",
    "    return (\n",
    "        torch.LongTensor(x_pad),\n",
    "        torch.FloatTensor(y_pad),   # BCE expects float targets\n",
    "        torch.BoolTensor(mask),\n",
    "        torch.LongTensor(lengths),\n",
    "    )\n",
    "\n",
    "# =========================\n",
    "# TRAIN/VALIDATION SPLIT\n",
    "# =========================\n",
    "# Split data into 90% training and 10% validation\n",
    "rng = np.random.default_rng(42)  # Fixed seed for reproducibility\n",
    "indices = np.arange(len(gold_df))\n",
    "rng.shuffle(indices)\n",
    "split = int(0.9*len(indices))\n",
    "train_idx, val_idx = indices[:split], indices[split:]\n",
    "\n",
    "train_df = gold_df.iloc[train_idx].reset_index(drop=True)\n",
    "val_df   = gold_df.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "print(f\"Training samples: {len(train_df):,}\")\n",
    "print(f\"Validation samples: {len(val_df):,}\")\n",
    "\n",
    "train_ds = CharBoundaryDataset(train_df)\n",
    "val_ds   = CharBoundaryDataset(val_df)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=pad_batch)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, collate_fn=pad_batch)\n",
    "\n",
    "# =========================\n",
    "# BILSTM MODEL ARCHITECTURE\n",
    "# =========================\n",
    "# Grapheme-level BiLSTM for boundary prediction\n",
    "\n",
    "class BiLSTMBoundary(nn.Module):\n",
    "    \"\"\"\n",
    "    Bidirectional LSTM model for grapheme-level boundary prediction.\n",
    "    \n",
    "    Architecture:\n",
    "    1. Grapheme embeddings (emb_dim dimensions)\n",
    "    2. Bidirectional LSTM (hidden_size per direction)\n",
    "    3. Dropout for regularization\n",
    "    4. Linear output layer (predicts boundary probability at each position)\n",
    "    \n",
    "    The model processes sequences grapheme-by-grapheme and outputs a logit\n",
    "    for each position indicating the probability of a boundary after that grapheme.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size: int, emb_dim: int = 16, hidden_size: int = 16, num_layers: int = 1, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        # Grapheme embedding layer\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        \n",
    "        # Bidirectional LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=emb_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=True,  # Process sequence in both directions\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Output layer: 2*hidden_size because bidirectional LSTM concatenates forward/backward\n",
    "        self.out = nn.Linear(hidden_size * 2, 1)  # Binary classification per time-step\n",
    "    \n",
    "    def forward(self, x, lengths):\n",
    "        \"\"\"\n",
    "        Forward pass through the model.\n",
    "        \n",
    "        Args:\n",
    "            x: Input grapheme IDs (B, T) - Long tensor\n",
    "            lengths: Actual length of each sequence (B,) - Long tensor\n",
    "        \n",
    "        Returns:\n",
    "            logits: Boundary prediction logits (B, T) - Float tensor\n",
    "        \"\"\"\n",
    "        # x: (B, T) Long; lengths: (B,)\n",
    "        emb = self.emb(x)  # (B, T, E) - Embed graphemes\n",
    "        \n",
    "        # Pack sequences to ignore padding during LSTM processing\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(emb, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_out, _ = self.lstm(packed)\n",
    "        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)  # (B, T, 2H)\n",
    "        \n",
    "        out = self.dropout(out)\n",
    "        logits = self.out(out).squeeze(-1)  # (B, T) - One logit per grapheme position\n",
    "        return logits\n",
    "\n",
    "# =========================\n",
    "# LOSS FUNCTION\n",
    "# =========================\n",
    "# Masked binary cross-entropy loss (ignores padding positions)\n",
    "\n",
    "def count_pos_neg(df_):\n",
    "    \"\"\"\n",
    "    Count positive (boundary) and negative (non-boundary) examples for class weighting.\n",
    "    \"\"\"\n",
    "    pos = sum(sum(lbls) for lbls in df_['boundary_labels'])\n",
    "    total = sum(len(seq) for seq in df_['char_seq'])\n",
    "    neg = total - pos\n",
    "    return pos, neg\n",
    "\n",
    "pos, neg = count_pos_neg(gold_df)\n",
    "pos_weight_value = float(neg) / max(float(pos), 1.0)  # Weight to balance class imbalance\n",
    "\n",
    "def masked_bce_loss(logits, targets, mask):\n",
    "    \"\"\"\n",
    "    Compute masked binary cross-entropy loss.\n",
    "    Only computes loss on valid (non-padded) positions.\n",
    "    \n",
    "    Args:\n",
    "        logits: Model predictions (B, T)\n",
    "        targets: Ground truth labels (B, T)\n",
    "        mask: Boolean mask indicating valid positions (B, T)\n",
    "    \n",
    "    Returns:\n",
    "        Scalar loss value\n",
    "    \"\"\"\n",
    "    loss_fn = nn.BCEWithLogitsLoss(reduction=\"none\",\n",
    "                                   pos_weight=torch.tensor(pos_weight_value, device=logits.device))\n",
    "    loss_per_token = loss_fn(logits, targets) * mask.float()  # Zero out padding positions\n",
    "    denom = mask.float().sum().clamp_min(1.0)  # Total number of valid tokens\n",
    "    return loss_per_token.sum() / denom\n",
    "\n",
    "# =========================\n",
    "# EVALUATION METRICS\n",
    "# =========================\n",
    "# Functions to compute precision, recall, and F1 score for boundary prediction\n",
    "\n",
    "def boundary_f1(logits, targets, mask, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Compute precision, recall, and F1 score for boundary prediction.\n",
    "    \n",
    "    Args:\n",
    "        logits: Model predictions (B, T)\n",
    "        targets: Ground truth labels (B, T)\n",
    "        mask: Boolean mask indicating valid positions (B, T)\n",
    "        threshold: Probability threshold for binary classification (default: 0.5)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (precision, recall, f1_score)\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        probs = torch.sigmoid(logits)  # Convert logits to probabilities\n",
    "        preds = (probs >= threshold).long()  # Binary predictions\n",
    "        t = targets.long()\n",
    "        m = mask.long()\n",
    "\n",
    "        # Compute true positives, false positives, false negatives\n",
    "        tp = ((preds == 1) & (t == 1) & (m == 1)).sum().item()\n",
    "        fp = ((preds == 1) & (t == 0) & (m == 1)).sum().item()\n",
    "        fn = ((preds == 0) & (t == 1) & (m == 1)).sum().item()\n",
    "\n",
    "        # Compute metrics with safe division\n",
    "        prec = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        rec  = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1   = 2*prec*rec / (prec + rec) if (prec + rec) > 0 else 0.0\n",
    "        return prec, rec, f1\n",
    "\n",
    "# =========================\n",
    "# INFERENCE FUNCTIONS\n",
    "# =========================\n",
    "# Functions to predict boundaries and reconstruct morphemes from predictions\n",
    "\n",
    "def predict_boundaries(words: List[str], model, stoi, threshold=0.5, device=device) -> List[List[int]]:\n",
    "    \"\"\"\n",
    "    Predict boundary labels for a list of words.\n",
    "    \n",
    "    Args:\n",
    "        words: List of word strings\n",
    "        model: Trained BiLSTMBoundary model\n",
    "        stoi: String-to-index vocabulary dictionary\n",
    "        threshold: Probability threshold for binary classification\n",
    "        device: Device to run inference on\n",
    "    \n",
    "    Returns:\n",
    "        List of boundary label lists (one per word)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    token_lists = [to_graphemes_quechua(w) for w in words]  # Tokenize words to graphemes\n",
    "    x_ids = [[stoi.get(t, stoi[UNK]) for t in toks] for toks in token_lists]\n",
    "    lengths = [len(x) for x in x_ids]\n",
    "    maxlen = max(lengths) if lengths else 0\n",
    "    pad_id = stoi[PAD]\n",
    "\n",
    "    x_pad = [xi + [pad_id]*(maxlen - len(xi)) for xi in x_ids]\n",
    "    mask  = [[1]*len(xi) + [0]*(maxlen - len(xi)) for xi in x_ids]\n",
    "\n",
    "    x = torch.LongTensor(x_pad).to(device)\n",
    "    lengths_t = torch.LongTensor(lengths).to(device)\n",
    "    mask_t = torch.BoolTensor(mask).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(x, lengths_t)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        preds = (probs >= threshold) & mask_t\n",
    "\n",
    "    # Trim padding and convert to list\n",
    "    out = []\n",
    "    for i, L in enumerate(lengths):\n",
    "        out.append(preds[i, :L].int().tolist())\n",
    "    return out\n",
    "\n",
    "def apply_boundaries_tokens(tokens: list[str], boundary_labels: List[int]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Reconstruct morphemes from grapheme token list and boundary labels.\n",
    "    \n",
    "    Args:\n",
    "        tokens: List of grapheme tokens\n",
    "        boundary_labels: List of binary boundary labels\n",
    "    \n",
    "    Returns:\n",
    "        List of morpheme strings\n",
    "    \"\"\"\n",
    "    segs, start = [], 0\n",
    "    for i, b in enumerate(boundary_labels):\n",
    "        if b == 1:\n",
    "            segs.append(\"\".join(tokens[start:i+1]))\n",
    "            start = i+1\n",
    "    if start < len(tokens):\n",
    "        segs.append(\"\".join(tokens[start:]))\n",
    "    return segs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00db18c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for existing model with ID: df7336ba5b7b6893\n",
      "No existing model found. Training new model...\n",
      "Epoch 01 | train_loss=0.4930  val_loss=0.2674  P=0.739 R=0.971 F1=0.839\n",
      "Model checkpoint saved to models_segmenter\\df7336ba5b7b6893\n",
      "  ↳ saved checkpoint (best F1 so far)\n",
      "Epoch 02 | train_loss=0.2011  val_loss=0.1868  P=0.852 R=0.971 F1=0.907\n",
      "Model checkpoint saved to models_segmenter\\df7336ba5b7b6893\n",
      "  ↳ saved checkpoint (best F1 so far)\n",
      "Epoch 03 | train_loss=0.1497  val_loss=0.1580  P=0.883 R=0.969 F1=0.924\n",
      "Model checkpoint saved to models_segmenter\\df7336ba5b7b6893\n",
      "  ↳ saved checkpoint (best F1 so far)\n",
      "Epoch 04 | train_loss=0.1206  val_loss=0.1486  P=0.895 R=0.963 F1=0.928\n",
      "Model checkpoint saved to models_segmenter\\df7336ba5b7b6893\n",
      "  ↳ saved checkpoint (best F1 so far)\n",
      "Epoch 05 | train_loss=0.1018  val_loss=0.1336  P=0.875 R=0.983 F1=0.925\n",
      "Epoch 06 | train_loss=0.0891  val_loss=0.1213  P=0.905 R=0.973 F1=0.938\n",
      "Model checkpoint saved to models_segmenter\\df7336ba5b7b6893\n",
      "  ↳ saved checkpoint (best F1 so far)\n",
      "Epoch 07 | train_loss=0.0743  val_loss=0.1203  P=0.917 R=0.971 F1=0.943\n",
      "Model checkpoint saved to models_segmenter\\df7336ba5b7b6893\n",
      "  ↳ saved checkpoint (best F1 so far)\n",
      "Epoch 08 | train_loss=0.0687  val_loss=0.1309  P=0.911 R=0.969 F1=0.939\n",
      "Epoch 09 | train_loss=0.0575  val_loss=0.1126  P=0.918 R=0.978 F1=0.947\n",
      "Model checkpoint saved to models_segmenter\\df7336ba5b7b6893\n",
      "  ↳ saved checkpoint (best F1 so far)\n",
      "Epoch 10 | train_loss=0.0528  val_loss=0.1134  P=0.935 R=0.971 F1=0.953\n",
      "Model checkpoint saved to models_segmenter\\df7336ba5b7b6893\n",
      "  ↳ saved checkpoint (best F1 so far)\n",
      "Epoch 11 | train_loss=0.0458  val_loss=0.1216  P=0.933 R=0.977 F1=0.954\n",
      "Model checkpoint saved to models_segmenter\\df7336ba5b7b6893\n",
      "  ↳ saved checkpoint (best F1 so far)\n",
      "Epoch 12 | train_loss=0.0419  val_loss=0.1139  P=0.923 R=0.981 F1=0.951\n",
      "Epoch 13 | train_loss=0.0395  val_loss=0.1251  P=0.927 R=0.976 F1=0.951\n",
      "Epoch 14 | train_loss=0.0354  val_loss=0.1200  P=0.931 R=0.972 F1=0.951\n",
      "Epoch 15 | train_loss=0.0349  val_loss=0.1416  P=0.942 R=0.971 F1=0.956\n",
      "Model checkpoint saved to models_segmenter\\df7336ba5b7b6893\n",
      "  ↳ saved checkpoint (best F1 so far)\n",
      "\n",
      "Training complete! Best validation F1: 0.9562\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# MODEL CHECKPOINTING FUNCTIONS\n",
    "# =========================\n",
    "# Functions to save and load trained models to avoid retraining\n",
    "\n",
    "def generate_model_id(emb_dim, hidden_size, num_layers, dropout, epochs, batch_size, lr, weight_decay):\n",
    "    \"\"\"\n",
    "    Generate a unique identifier for a model based on its training parameters.\n",
    "    \n",
    "    Args:\n",
    "        All training hyperparameters\n",
    "    \n",
    "    Returns:\n",
    "        A string identifier (hash) for the model\n",
    "    \"\"\"\n",
    "    params_dict = {\n",
    "        'emb_dim': emb_dim,\n",
    "        'hidden_size': hidden_size,\n",
    "        'num_layers': num_layers,\n",
    "        'dropout': dropout,\n",
    "        'epochs': epochs,\n",
    "        'batch_size': batch_size,\n",
    "        'lr': lr,\n",
    "        'weight_decay': weight_decay,\n",
    "        'vocab_size': len(itos)\n",
    "    }\n",
    "    params_str = json.dumps(params_dict, sort_keys=True)\n",
    "    model_id = hashlib.md5(params_str.encode()).hexdigest()[:16]\n",
    "    return model_id\n",
    "\n",
    "def save_model_checkpoint(model, stoi, itos, model_id, models_folder=MODELS_FOLDER):\n",
    "    \"\"\"\n",
    "    Save model checkpoint to the models folder.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained BiLSTMBoundary model\n",
    "        stoi: String-to-index vocabulary dictionary\n",
    "        itos: Index-to-string vocabulary list\n",
    "        model_id: Unique identifier for this model\n",
    "        models_folder: Folder to save models in\n",
    "    \"\"\"\n",
    "    model_dir = os.path.join(models_folder, model_id)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    checkpoint_path = os.path.join(model_dir, \"bilstm_grapheme_boundary.pt\")\n",
    "    torch.save({\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"stoi\": stoi,\n",
    "        \"itos\": itos\n",
    "    }, checkpoint_path)\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata_path = os.path.join(model_dir, \"metadata.json\")\n",
    "    with open(metadata_path, \"w\") as f:\n",
    "        json.dump({\n",
    "            'model_id': model_id,\n",
    "            'vocab_size': len(itos),\n",
    "            'model_name': MODEL_NAME\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    print(f\"Model checkpoint saved to {model_dir}\")\n",
    "    return model_dir\n",
    "\n",
    "def load_model_checkpoint(model_id, models_folder=MODELS_FOLDER):\n",
    "    \"\"\"\n",
    "    Load model checkpoint from the models folder.\n",
    "    \n",
    "    Args:\n",
    "        model_id: Unique identifier for the model\n",
    "        models_folder: Folder where models are saved\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with 'model_state', 'stoi', 'itos', 'checkpoint_path', 'model_dir' or None if not found\n",
    "    \"\"\"\n",
    "    model_dir = os.path.join(models_folder, model_id)\n",
    "    checkpoint_path = os.path.join(model_dir, \"bilstm_grapheme_boundary.pt\")\n",
    "    \n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        return None\n",
    "    \n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    print(f\"Model checkpoint loaded from {model_dir}\")\n",
    "    return {\n",
    "        'model_state': checkpoint['model_state'],\n",
    "        'stoi': checkpoint['stoi'],\n",
    "        'itos': checkpoint['itos'],\n",
    "        'checkpoint_path': checkpoint_path,\n",
    "        'model_dir': model_dir\n",
    "    }\n",
    "\n",
    "# =========================\n",
    "# MODEL INITIALIZATION AND TRAINING\n",
    "# =========================\n",
    "# Initialize model and optimizer, then train (or load if already trained)\n",
    "\n",
    "# Model hyperparameters\n",
    "EMB_DIM = 64\n",
    "HIDDEN_SIZE = 128\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.1\n",
    "EPOCHS = 15\n",
    "BATCH_SIZE = 64\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "# Generate model identifier\n",
    "model_id = generate_model_id(EMB_DIM, HIDDEN_SIZE, NUM_LAYERS, DROPOUT, EPOCHS, BATCH_SIZE, LR, WEIGHT_DECAY)\n",
    "\n",
    "# Try to load existing model\n",
    "print(f\"Checking for existing model with ID: {model_id}\")\n",
    "loaded = load_model_checkpoint(model_id, models_folder=MODELS_FOLDER)\n",
    "\n",
    "if loaded is not None:\n",
    "    print(f\"Found existing model! Loading from {loaded['model_dir']}\")\n",
    "    stoi = loaded['stoi']\n",
    "    itos = loaded['itos']\n",
    "    model = BiLSTMBoundary(vocab_size=len(itos), emb_dim=EMB_DIM, hidden_size=HIDDEN_SIZE, \n",
    "                           num_layers=NUM_LAYERS, dropout=DROPOUT).to(device)\n",
    "    model.load_state_dict(loaded['model_state'])\n",
    "    model.eval()\n",
    "    print(\"Model loaded successfully. Skipping training.\")\n",
    "else:\n",
    "    print(f\"No existing model found. Training new model...\")\n",
    "    \n",
    "    # Initialize model and optimizer\n",
    "    model = BiLSTMBoundary(vocab_size=len(itos), emb_dim=EMB_DIM, hidden_size=HIDDEN_SIZE, \n",
    "                          num_layers=NUM_LAYERS, dropout=DROPOUT).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    \n",
    "    # =========================\n",
    "    # TRAINING LOOP\n",
    "    # =========================\n",
    "    # Train the model for specified number of epochs\n",
    "    # Save checkpoint whenever validation F1 improves\n",
    "    \n",
    "    best_val_f1 = 0.0\n",
    "    \n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        total_tokens = 0\n",
    "        for x, y, mask, lengths in train_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            mask = mask.to(device)\n",
    "            lengths = lengths.to(device)\n",
    "\n",
    "            logits = model(x, lengths)\n",
    "            loss = masked_bce_loss(logits, y, mask)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * mask.sum().item()\n",
    "            total_tokens += mask.sum().item()\n",
    "\n",
    "        train_loss = total_loss / max(total_tokens, 1)\n",
    "\n",
    "        # ---- Validation phase\n",
    "        model.eval()\n",
    "        val_loss, val_tokens = 0.0, 0\n",
    "        all_prec, all_rec, all_f1 = [], [], []\n",
    "        with torch.no_grad():\n",
    "            for x, y, mask, lengths in val_loader:\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                mask = mask.to(device)\n",
    "                lengths = lengths.to(device)\n",
    "\n",
    "                logits = model(x, lengths)\n",
    "                loss = masked_bce_loss(logits, y, mask)\n",
    "                val_loss += loss.item() * mask.sum().item()\n",
    "                val_tokens += mask.sum().item()\n",
    "\n",
    "                p, r, f = boundary_f1(logits, y, mask, threshold=0.5)\n",
    "                all_prec.append(p); all_rec.append(r); all_f1.append(f)\n",
    "\n",
    "        val_loss = val_loss / max(val_tokens, 1)\n",
    "        prec = np.mean(all_prec) if all_prec else 0.0\n",
    "        rec  = np.mean(all_rec)  if all_rec  else 0.0\n",
    "        f1   = np.mean(all_f1)   if all_f1   else 0.0\n",
    "\n",
    "        print(f\"Epoch {epoch:02d} | train_loss={train_loss:.4f}  val_loss={val_loss:.4f}  P={prec:.3f} R={rec:.3f} F1={f1:.3f}\")\n",
    "\n",
    "        # Keep best model based on validation F1\n",
    "        if f1 > best_val_f1:\n",
    "            best_val_f1 = f1\n",
    "            save_model_checkpoint(model, stoi, itos, model_id, models_folder=MODELS_FOLDER)\n",
    "            print(\"  ↳ saved checkpoint (best F1 so far)\")\n",
    "    \n",
    "    print(f\"\\nTraining complete! Best validation F1: {best_val_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28306193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rikuchkani [0, 0, 0, 1, 0, 0, 1, 0, 0] -> ['riku', 'chka', 'ni']\n",
      "pikunas [0, 1, 0, 0, 0, 1, 0] -> ['pi', 'kuna', 's']\n",
      "ñichkanchus [0, 1, 0, 0, 1, 1, 0, 0, 0] -> ['ñi', 'chka', 'n', 'chus']\n"
     ]
    }
   ],
   "source": [
    "test_words = [\"rikuchkani\", \"pikunas\", \"ñichkanchus\"]\n",
    "pred_b = predict_boundaries(test_words, model, stoi, threshold=0.5)\n",
    "for w, b in zip(test_words, pred_b):\n",
    "    toks = to_graphemes_quechua(w)\n",
    "    print(w, b, \"->\", apply_boundaries_tokens(toks, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca78ab0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data...\n",
      "Loaded 913 test examples\n",
      "Model checkpoint loaded from models_segmenter\\df7336ba5b7b6893\n",
      "Model loaded successfully for evaluation.\n",
      "Exact segmentation accuracy: 0.5608\n",
      "Boundary metrics (token space):\n",
      "  Micro  - P: 0.8194  R: 0.8619  F1: 0.8401\n",
      "  Macro  - P: 0.8257  R: 0.8471  F1: 0.8196\n",
      "\n",
      "=== Split-count metrics ===\n",
      "Split-count (Exact):          0.6605\n",
      "Split-count (+1):             0.1873\n",
      "Split-count (−1):             0.1281\n",
      "Split-count (±1):             0.9748\n",
      "Overlap (Exact ∩ Split):      0.5608\n",
      "\n",
      "Evaluation results saved to data\\bilstm_grapheme_eval_results.csv\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# LOAD TEST DATA\n",
    "# =========================\n",
    "# Load the test/accuracy evaluation dataset\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "print(\"Loading test data...\")\n",
    "df = pd.read_parquet(os.path.join(DATA_FOLDER, \"cleaned_data_df.parquet\"))\n",
    "print(f\"Loaded {len(df):,} test examples\")\n",
    "\n",
    "# =========================\n",
    "# LOAD TRAINED MODEL\n",
    "# =========================\n",
    "# Load the best model checkpoint from the models folder\n",
    "# Use the same model ID that was generated during training\n",
    "\n",
    "# Model hyperparameters (must match training)\n",
    "EMB_DIM = 64\n",
    "HIDDEN_SIZE = 128\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.1\n",
    "EPOCHS = 15\n",
    "BATCH_SIZE = 64\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "# Generate the same model ID\n",
    "model_id = generate_model_id(EMB_DIM, HIDDEN_SIZE, NUM_LAYERS, DROPOUT, EPOCHS, BATCH_SIZE, LR, WEIGHT_DECAY)\n",
    "\n",
    "# Load checkpoint\n",
    "loaded = load_model_checkpoint(model_id, models_folder=MODELS_FOLDER)\n",
    "if loaded is None:\n",
    "    raise FileNotFoundError(f\"Model checkpoint not found. Please train the model first (model_id: {model_id})\")\n",
    "\n",
    "stoi, itos = loaded[\"stoi\"], loaded[\"itos\"]\n",
    "model = BiLSTMBoundary(vocab_size=len(itos), emb_dim=EMB_DIM, hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS, dropout=DROPOUT)\n",
    "model.load_state_dict(loaded[\"model_state\"])\n",
    "model.eval()\n",
    "print(\"Model loaded successfully for evaluation.\")\n",
    "\n",
    "# =========================\n",
    "# EVALUATION HELPER FUNCTIONS\n",
    "# =========================\n",
    "# Functions for predicting boundaries and evaluating segmentation accuracy\n",
    "\n",
    "def is_correct_prediction(predicted, gold_variants):\n",
    "    \"\"\"\n",
    "    Check if predicted segmentation exactly matches any gold variant.\n",
    "    \n",
    "    Args:\n",
    "        predicted: List of predicted morphemes\n",
    "        gold_variants: List of gold segmentation variants\n",
    "    \n",
    "    Returns:\n",
    "        True if prediction matches any gold variant, False otherwise\n",
    "    \"\"\"\n",
    "    return any(predicted == variant for variant in gold_variants)\n",
    "\n",
    "# =========================\n",
    "# EVALUATION METRICS FUNCTIONS\n",
    "# =========================\n",
    "# Functions to compute various evaluation metrics for morphological segmentation\n",
    "# These work in grapheme-token space to ensure consistent comparison\n",
    "\n",
    "def boundary_positions_from_labels(labels, L=None):\n",
    "    \"\"\"\n",
    "    Convert per-token boundary labels into boundary positions at indices 0..L-2.\n",
    "    L is the tokenized length (len(toks)). We ignore the last position by design.\n",
    "    \"\"\"\n",
    "    if not labels:\n",
    "        return set()\n",
    "    if L is None:\n",
    "        L = len(labels)\n",
    "    upto = min(L - 1, len(labels))\n",
    "    return {i for i in range(upto) if labels[i] == 1}\n",
    "\n",
    "def boundary_positions_from_morpheme_tokens(morpheme_token_lists):\n",
    "    \"\"\"\n",
    "    Given a list of morphemes, each represented as a list of grapheme tokens,\n",
    "    return boundary positions (end-of-morpheme token indices) excluding the final morpheme.\n",
    "    \"\"\"\n",
    "    pos = set()\n",
    "    acc = 0\n",
    "    for k, toks in enumerate(morpheme_token_lists):\n",
    "        acc += len(toks)\n",
    "        if k < len(morpheme_token_lists) - 1:\n",
    "            pos.add(acc - 1)\n",
    "    return pos\n",
    "\n",
    "def prf_from_sets(pred_set, gold_set):\n",
    "    tp = len(pred_set & gold_set)\n",
    "    fp = len(pred_set - gold_set)\n",
    "    fn = len(gold_set - pred_set)\n",
    "\n",
    "    if tp + fp == 0:\n",
    "        precision = 1.0 if (tp + fn == 0) else 0.0\n",
    "    else:\n",
    "        precision = tp / (tp + fp)\n",
    "\n",
    "    if tp + fn == 0:\n",
    "        recall = 1.0 if (tp + fp == 0) else 0.0\n",
    "    else:\n",
    "        recall = tp / (tp + fn)\n",
    "\n",
    "    if precision + recall == 0:\n",
    "        f1 = 1.0 if (tp + fp + fn) == 0 else 0.0\n",
    "    else:\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    return tp, fp, fn, precision, recall, f1\n",
    "\n",
    "def best_variant_metrics_token_space(word_tokens, pred_boundary_labels, gold_variants):\n",
    "    \"\"\"\n",
    "    Compare predicted boundary labels (in token space) against each gold variant\n",
    "    converted into token-space boundaries using the same grapheme tokenizer.\n",
    "    Choose the gold variant that maximizes F1.\n",
    "    Returns: (chosen_gold_positions, tp, fp, fn, P, R, F1)\n",
    "    \"\"\"\n",
    "    # Predicted boundary positions from labels\n",
    "    pred_b = boundary_positions_from_labels(pred_boundary_labels, L=len(word_tokens))\n",
    "\n",
    "    best = None\n",
    "    for variant in gold_variants:\n",
    "        # Tokenize each gold morpheme to count token lengths consistently\n",
    "        variant_token_lists = [to_graphemes_quechua(m) for m in variant]\n",
    "        gold_b = boundary_positions_from_morpheme_tokens(variant_token_lists)\n",
    "        tp, fp, fn, P, R, F1 = prf_from_sets(pred_b, gold_b)\n",
    "        key = (F1, tp, -fn, -fp)  # stable tie-break\n",
    "        if (best is None) or (key > best[0]):\n",
    "            best = (key, gold_b, tp, fp, fn, P, R, F1)\n",
    "\n",
    "    if best is None:\n",
    "        # Fallback (shouldn't happen with your cleaned data)\n",
    "        gold_b = set()\n",
    "        tp, fp, fn, P, R, F1 = prf_from_sets(pred_b, gold_b)\n",
    "        return pred_b, gold_b, tp, fp, fn, P, R, F1\n",
    "\n",
    "    _, gold_b, tp, fp, fn, P, R, F1 = best\n",
    "    return pred_b, gold_b, tp, fp, fn, P, R, F1\n",
    "\n",
    "def is_correct_prediction(predicted, gold_variants):\n",
    "    return any(predicted == variant for variant in gold_variants)\n",
    "\n",
    "def normalize_gold_variants(gold_variants):\n",
    "    \"\"\"\n",
    "    Convert gold_variants to a list format, handling numpy arrays and nested structures.\n",
    "    \"\"\"\n",
    "    if gold_variants is None:\n",
    "        return []\n",
    "    \n",
    "    # If it's a numpy array, convert to list\n",
    "    if isinstance(gold_variants, np.ndarray):\n",
    "        gold_variants = gold_variants.tolist()\n",
    "    \n",
    "    # If it's already a list, ensure nested elements are also lists (not numpy arrays)\n",
    "    if isinstance(gold_variants, list):\n",
    "        normalized = []\n",
    "        for variant in gold_variants:\n",
    "            if isinstance(variant, np.ndarray):\n",
    "                normalized.append(variant.tolist())\n",
    "            elif isinstance(variant, list):\n",
    "                # Recursively normalize nested lists\n",
    "                normalized.append([item.tolist() if isinstance(item, np.ndarray) else item for item in variant])\n",
    "            else:\n",
    "                normalized.append(variant)\n",
    "        return normalized\n",
    "    \n",
    "    return []\n",
    "\n",
    "# =========================\n",
    "# EVALUATION ON TEST SET\n",
    "# =========================\n",
    "# Predict boundaries for all test words and compute evaluation metrics\n",
    "\n",
    "# Batch predict boundaries for all test words\n",
    "all_words = df[\"Word\"].tolist()\n",
    "all_boundaries = predict_boundaries(all_words, model, stoi, threshold=0.5, device='cpu')\n",
    "\n",
    "records = []\n",
    "micro_tp = micro_fp = micro_fn = 0\n",
    "macro_Ps, macro_Rs, macro_F1s = [], [], []\n",
    "\n",
    "for word, gold_variants, boundary_labels in zip(all_words, df[\"Gold\"], all_boundaries):\n",
    "    # Normalize gold_variants (convert numpy arrays to lists)\n",
    "    gold_variants = normalize_gold_variants(gold_variants)\n",
    "\n",
    "    # Tokenize word into graphemes (your tokenizer)\n",
    "    toks = to_graphemes_quechua(word)\n",
    "\n",
    "    # Build predicted segmentation from token-level labels (your helper)\n",
    "    predicted_segments = apply_boundaries_tokens(toks, boundary_labels)\n",
    "\n",
    "    # Exact-match accuracy (string-level; you already had this)\n",
    "    correct_exact = is_correct_prediction(predicted_segments, gold_variants)\n",
    "\n",
    "    # Boundary metrics in token space (pick best gold variant per word)\n",
    "    pred_b, gold_b_chosen, tp, fp, fn, P, R, F1 = best_variant_metrics_token_space(\n",
    "        toks, boundary_labels, gold_variants\n",
    "    )\n",
    "\n",
    "    records.append({\n",
    "        \"Word\": word,\n",
    "        \"Prediction\": predicted_segments,\n",
    "        \"Gold\": gold_variants,\n",
    "        \"PredBoundaries(tok_idx)\": sorted(pred_b),\n",
    "        \"GoldBoundaries(Chosen tok_idx)\": sorted(gold_b_chosen),\n",
    "        \"TP\": tp, \"FP\": fp, \"FN\": fn,\n",
    "        \"P_word\": P, \"R_word\": R, \"F1_word\": F1,\n",
    "        \"CorrectExactSeg\": correct_exact\n",
    "    })\n",
    "\n",
    "    micro_tp += tp\n",
    "    micro_fp += fp\n",
    "    micro_fn += fn\n",
    "    macro_Ps.append(P)\n",
    "    macro_Rs.append(R)\n",
    "    macro_F1s.append(F1)\n",
    "\n",
    "results_df = pd.DataFrame(records)\n",
    "\n",
    "# Exact segmentation accuracy\n",
    "accuracy = results_df[\"CorrectExactSeg\"].mean()\n",
    "\n",
    "# Micro metrics (global)\n",
    "if micro_tp + micro_fp == 0:\n",
    "    P_micro = 1.0 if micro_tp + micro_fn == 0 else 0.0\n",
    "else:\n",
    "    P_micro = micro_tp / (micro_tp + micro_fp)\n",
    "\n",
    "if micro_tp + micro_fn == 0:\n",
    "    R_micro = 1.0 if micro_tp + micro_fp == 0 else 0.0\n",
    "else:\n",
    "    R_micro = micro_tp / (micro_tp + micro_fn)\n",
    "\n",
    "if P_micro + R_micro == 0:\n",
    "    F1_micro = 1.0 if (micro_tp + micro_fp + micro_fn) == 0 else 0.0\n",
    "else:\n",
    "    F1_micro = 2 * P_micro * R_micro / (P_micro + R_micro)\n",
    "\n",
    "# Macro metrics (average of per-word scores)\n",
    "P_macro = float(pd.Series(macro_Ps).mean()) if macro_Ps else 0.0\n",
    "R_macro = float(pd.Series(macro_Rs).mean()) if macro_Rs else 0.0\n",
    "F1_macro = float(pd.Series(macro_F1s).mean()) if macro_F1s else 0.0\n",
    "\n",
    "print(f\"Exact segmentation accuracy: {accuracy:.4f}\")\n",
    "print(\"Boundary metrics (token space):\")\n",
    "print(f\"  Micro  - P: {P_micro:.4f}  R: {R_micro:.4f}  F1: {F1_micro:.4f}\")\n",
    "print(f\"  Macro  - P: {P_macro:.4f}  R: {R_macro:.4f}  F1: {F1_macro:.4f}\")\n",
    "\n",
    "# ==== Split-count accuracy metrics ====\n",
    "\n",
    "def split_count_metrics(predicted_segments, gold_variants):\n",
    "    \"\"\"\n",
    "    Compute split-count accuracy variants:\n",
    "    - Exact: same number of morphemes as any gold variant\n",
    "    - +1: one more split than any gold variant\n",
    "    - -1: one fewer split than any gold variant\n",
    "    - ±1: difference ≤ 1 with any gold variant\n",
    "    \"\"\"\n",
    "    pred_count = len(predicted_segments)\n",
    "    gold_counts = [len(gold) for gold in gold_variants]\n",
    "\n",
    "    exact = any(pred_count == g for g in gold_counts)\n",
    "    plus1 = any(pred_count == g + 1 for g in gold_counts)\n",
    "    minus1 = any(pred_count == g - 1 for g in gold_counts)\n",
    "    pm1 = any(abs(pred_count - g) <= 1 for g in gold_counts)\n",
    "\n",
    "    return {\"Exact\": exact, \"+1\": plus1, \"-1\": minus1, \"±1\": pm1}\n",
    "\n",
    "\n",
    "# ---- Extend existing loop ----\n",
    "split_exact_flags = []\n",
    "split_plus1_flags = []\n",
    "split_minus1_flags = []\n",
    "split_pm1_flags = []\n",
    "overlap_flags = []\n",
    "\n",
    "for rec in records:\n",
    "    predicted_segments = rec[\"Prediction\"]\n",
    "    gold_variants = rec[\"Gold\"]\n",
    "    # Normalize gold_variants (convert numpy arrays to lists)\n",
    "    gold_variants = normalize_gold_variants(gold_variants)\n",
    "\n",
    "    split_metrics = split_count_metrics(predicted_segments, gold_variants)\n",
    "    rec[\"CorrectSplitCount\"] = split_metrics[\"Exact\"]\n",
    "    rec[\"SplitCount+1\"] = split_metrics[\"+1\"]\n",
    "    rec[\"SplitCount-1\"] = split_metrics[\"-1\"]\n",
    "    rec[\"SplitCount±1\"] = split_metrics[\"±1\"]\n",
    "\n",
    "    # Overlap: both segmentation and split count are correct\n",
    "    overlap = rec[\"CorrectExactSeg\"] and split_metrics[\"Exact\"]\n",
    "    rec[\"OverlapExactAndSplit\"] = overlap\n",
    "\n",
    "    split_exact_flags.append(split_metrics[\"Exact\"])\n",
    "    split_plus1_flags.append(split_metrics[\"+1\"])\n",
    "    split_minus1_flags.append(split_metrics[\"-1\"])\n",
    "    split_pm1_flags.append(split_metrics[\"±1\"])\n",
    "    overlap_flags.append(overlap)\n",
    "\n",
    "# ---- Aggregate split metrics ----\n",
    "split_exact_acc = np.mean(split_exact_flags)\n",
    "split_plus1_acc = np.mean(split_plus1_flags)\n",
    "split_minus1_acc = np.mean(split_minus1_flags)\n",
    "split_pm1_acc = np.mean(split_pm1_flags)\n",
    "overlap_accuracy = np.mean(overlap_flags)\n",
    "\n",
    "# ---- Add to existing summary printout ----\n",
    "print(\"\\n=== Split-count metrics ===\")\n",
    "print(f\"Split-count (Exact):          {split_exact_acc:.4f}\")\n",
    "print(f\"Split-count (+1):             {split_plus1_acc:.4f}\")\n",
    "print(f\"Split-count (−1):             {split_minus1_acc:.4f}\")\n",
    "print(f\"Split-count (±1):             {split_pm1_acc:.4f}\")\n",
    "print(f\"Overlap (Exact ∩ Split):      {overlap_accuracy:.4f}\")\n",
    "\n",
    "# =========================\n",
    "# SAVE EVALUATION RESULTS\n",
    "# =========================\n",
    "# Save evaluation results to the data folder with a descriptive filename\n",
    "results_df = pd.DataFrame(records)\n",
    "results_output_path = os.path.join(DATA_FOLDER, \"bilstm_grapheme_eval_results.csv\")\n",
    "results_df.to_csv(results_output_path, index=False)\n",
    "print(f\"\\nEvaluation results saved to {results_output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd44cd61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Gold</th>\n",
       "      <th>PredBoundaries(tok_idx)</th>\n",
       "      <th>GoldBoundaries(Chosen tok_idx)</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>P_word</th>\n",
       "      <th>R_word</th>\n",
       "      <th>F1_word</th>\n",
       "      <th>CorrectExactSeg</th>\n",
       "      <th>CorrectSplitCount</th>\n",
       "      <th>SplitCount+1</th>\n",
       "      <th>SplitCount-1</th>\n",
       "      <th>SplitCount±1</th>\n",
       "      <th>OverlapExactAndSplit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>unupas</td>\n",
       "      <td>[unupa, s]</td>\n",
       "      <td>[[unu, pas]]</td>\n",
       "      <td>[4]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>umankus</td>\n",
       "      <td>[uma, nku, s]</td>\n",
       "      <td>[[uma, nku, s]]</td>\n",
       "      <td>[2, 5]</td>\n",
       "      <td>[2, 5]</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hikurin</td>\n",
       "      <td>[hiku, ri, n]</td>\n",
       "      <td>[[hikuri, n]]</td>\n",
       "      <td>[3, 5]</td>\n",
       "      <td>[5]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sutipi</td>\n",
       "      <td>[suti, pi]</td>\n",
       "      <td>[[suti, pi]]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pikunas</td>\n",
       "      <td>[pi, kuna, s]</td>\n",
       "      <td>[[pi, kuna, s]]</td>\n",
       "      <td>[1, 5]</td>\n",
       "      <td>[1, 5]</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>atipaq</td>\n",
       "      <td>[atipa, q]</td>\n",
       "      <td>[[ati, paq], [ati, pa, q]]</td>\n",
       "      <td>[4]</td>\n",
       "      <td>[2, 4]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tomani</td>\n",
       "      <td>[toma, ni]</td>\n",
       "      <td>[[toma, ni]]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rantiq</td>\n",
       "      <td>[ranti, q]</td>\n",
       "      <td>[[ranti, q]]</td>\n",
       "      <td>[4]</td>\n",
       "      <td>[4]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>imakunas</td>\n",
       "      <td>[ima, kuna, s]</td>\n",
       "      <td>[[ima, kuna, s]]</td>\n",
       "      <td>[2, 6]</td>\n",
       "      <td>[2, 6]</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>chiqaq</td>\n",
       "      <td>[chiqa, q]</td>\n",
       "      <td>[[chiqaq]]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>hampichu</td>\n",
       "      <td>[hampi, chu]</td>\n",
       "      <td>[[hampi, chu]]</td>\n",
       "      <td>[4]</td>\n",
       "      <td>[4]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>misata</td>\n",
       "      <td>[misa, ta]</td>\n",
       "      <td>[[misa, ta]]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>nakuy</td>\n",
       "      <td>[na, ku, y]</td>\n",
       "      <td>[[na, ku, y]]</td>\n",
       "      <td>[1, 3]</td>\n",
       "      <td>[1, 3]</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>hampi</td>\n",
       "      <td>[hampi]</td>\n",
       "      <td>[[hampi]]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>hampiy</td>\n",
       "      <td>[hampi, y]</td>\n",
       "      <td>[[hampi, y]]</td>\n",
       "      <td>[4]</td>\n",
       "      <td>[4]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>limun</td>\n",
       "      <td>[limu, n]</td>\n",
       "      <td>[[limun]]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>hinas</td>\n",
       "      <td>[hina, s]</td>\n",
       "      <td>[[hina, s]]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>atispa</td>\n",
       "      <td>[ati, spa]</td>\n",
       "      <td>[[ati, spa]]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>riman</td>\n",
       "      <td>[rima, n]</td>\n",
       "      <td>[[rima, n]]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>matin</td>\n",
       "      <td>[mati, n]</td>\n",
       "      <td>[[mati, n]]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>yachani</td>\n",
       "      <td>[yacha, ni]</td>\n",
       "      <td>[[yacha, ni]]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>chaypiqa</td>\n",
       "      <td>[chay, pi, qa]</td>\n",
       "      <td>[[chay, pi, qa]]</td>\n",
       "      <td>[2, 4]</td>\n",
       "      <td>[2, 4]</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>niqchu</td>\n",
       "      <td>[ni, q, chu]</td>\n",
       "      <td>[[ni, q, chu]]</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>napiqa</td>\n",
       "      <td>[na, pi, qa]</td>\n",
       "      <td>[[na, pi, qa]]</td>\n",
       "      <td>[1, 3]</td>\n",
       "      <td>[1, 3]</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ripuni</td>\n",
       "      <td>[ri, puni]</td>\n",
       "      <td>[[ri, pu, ni]]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[1, 3]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>kapunchik</td>\n",
       "      <td>[ka, pu, nchik]</td>\n",
       "      <td>[[ka, pu, nchik]]</td>\n",
       "      <td>[1, 3]</td>\n",
       "      <td>[1, 3]</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>raykum</td>\n",
       "      <td>[raykum]</td>\n",
       "      <td>[[ima, rayku, m]]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[2, 7]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>nispam</td>\n",
       "      <td>[ni, spa, m]</td>\n",
       "      <td>[[ni, spa, m]]</td>\n",
       "      <td>[1, 4]</td>\n",
       "      <td>[1, 4]</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>rimanku</td>\n",
       "      <td>[ri, ma, nku]</td>\n",
       "      <td>[[rima, n, ku]]</td>\n",
       "      <td>[1, 3]</td>\n",
       "      <td>[3, 4]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>punchay</td>\n",
       "      <td>[puncha, y]</td>\n",
       "      <td>[[punchay]]</td>\n",
       "      <td>[4]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>tumankupas</td>\n",
       "      <td>[tuma, nkupa, s]</td>\n",
       "      <td>[[tuma, nku, pas]]</td>\n",
       "      <td>[3, 8]</td>\n",
       "      <td>[3, 6]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>chikan</td>\n",
       "      <td>[chika, n]</td>\n",
       "      <td>[[chikan]]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>munay</td>\n",
       "      <td>[muna, y]</td>\n",
       "      <td>[[muna, y]]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>atikun</td>\n",
       "      <td>[ati, ku, n]</td>\n",
       "      <td>[[ati, ku, n]]</td>\n",
       "      <td>[2, 4]</td>\n",
       "      <td>[2, 4]</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>chunka</td>\n",
       "      <td>[chunka]</td>\n",
       "      <td>[[chunka]]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>saniyapuni</td>\n",
       "      <td>[sani, ya, puni]</td>\n",
       "      <td>[[sani, ya, puni]]</td>\n",
       "      <td>[3, 5]</td>\n",
       "      <td>[3, 5]</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>hampusaq</td>\n",
       "      <td>[hampu, saq]</td>\n",
       "      <td>[[hampu, saq]]</td>\n",
       "      <td>[4]</td>\n",
       "      <td>[4]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>ñuqaykupis</td>\n",
       "      <td>[ñuqa, yku, pis]</td>\n",
       "      <td>[[ñuqa, yku, pis]]</td>\n",
       "      <td>[3, 6]</td>\n",
       "      <td>[3, 6]</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>chhikan</td>\n",
       "      <td>[chhika, n]</td>\n",
       "      <td>[[chhikan]]</td>\n",
       "      <td>[4]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>santusmi</td>\n",
       "      <td>[santu, s, mi]</td>\n",
       "      <td>[[santus, mi]]</td>\n",
       "      <td>[4, 5]</td>\n",
       "      <td>[5]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>sutin</td>\n",
       "      <td>[suti, n]</td>\n",
       "      <td>[[suti, n]]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>intirukunapi</td>\n",
       "      <td>[inti, ru, kuna, pi]</td>\n",
       "      <td>[[intiru, kuna, pi]]</td>\n",
       "      <td>[3, 5, 9]</td>\n",
       "      <td>[5, 9]</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>wasinta</td>\n",
       "      <td>[wasi, n, ta]</td>\n",
       "      <td>[[wasi, n, ta]]</td>\n",
       "      <td>[3, 4]</td>\n",
       "      <td>[3, 4]</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>tomansunchik</td>\n",
       "      <td>[toma, n, su, nchik]</td>\n",
       "      <td>[[toman, su, nchik]]</td>\n",
       "      <td>[3, 4, 6]</td>\n",
       "      <td>[4, 6]</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>uyarispaqa</td>\n",
       "      <td>[uyari, spa, qa]</td>\n",
       "      <td>[[uyari, spa, qa]]</td>\n",
       "      <td>[4, 7]</td>\n",
       "      <td>[4, 7]</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>turaykuna</td>\n",
       "      <td>[tura, y, kuna]</td>\n",
       "      <td>[[tura, y, kuna]]</td>\n",
       "      <td>[3, 4]</td>\n",
       "      <td>[3, 4]</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>aparuni</td>\n",
       "      <td>[apa, ru, ni]</td>\n",
       "      <td>[[apa, ru, ni]]</td>\n",
       "      <td>[2, 4]</td>\n",
       "      <td>[2, 4]</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>tiyakuq</td>\n",
       "      <td>[tiya, ku, q]</td>\n",
       "      <td>[[tiya, ku, q]]</td>\n",
       "      <td>[3, 5]</td>\n",
       "      <td>[3, 5]</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>tumani</td>\n",
       "      <td>[tuma, ni]</td>\n",
       "      <td>[[tuma, ni]]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>qhisakuwan</td>\n",
       "      <td>[qhisa, ku, wan]</td>\n",
       "      <td>[[qhisaku, wan]]</td>\n",
       "      <td>[3, 5]</td>\n",
       "      <td>[5]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Word            Prediction                        Gold  \\\n",
       "0         unupas            [unupa, s]                [[unu, pas]]   \n",
       "1        umankus         [uma, nku, s]             [[uma, nku, s]]   \n",
       "2        hikurin         [hiku, ri, n]               [[hikuri, n]]   \n",
       "3         sutipi            [suti, pi]                [[suti, pi]]   \n",
       "4        pikunas         [pi, kuna, s]             [[pi, kuna, s]]   \n",
       "5         atipaq            [atipa, q]  [[ati, paq], [ati, pa, q]]   \n",
       "6         tomani            [toma, ni]                [[toma, ni]]   \n",
       "7         rantiq            [ranti, q]                [[ranti, q]]   \n",
       "8       imakunas        [ima, kuna, s]            [[ima, kuna, s]]   \n",
       "9         chiqaq            [chiqa, q]                  [[chiqaq]]   \n",
       "10      hampichu          [hampi, chu]              [[hampi, chu]]   \n",
       "11        misata            [misa, ta]                [[misa, ta]]   \n",
       "12         nakuy           [na, ku, y]               [[na, ku, y]]   \n",
       "13         hampi               [hampi]                   [[hampi]]   \n",
       "14        hampiy            [hampi, y]                [[hampi, y]]   \n",
       "15         limun             [limu, n]                   [[limun]]   \n",
       "16         hinas             [hina, s]                 [[hina, s]]   \n",
       "17        atispa            [ati, spa]                [[ati, spa]]   \n",
       "18         riman             [rima, n]                 [[rima, n]]   \n",
       "19         matin             [mati, n]                 [[mati, n]]   \n",
       "20       yachani           [yacha, ni]               [[yacha, ni]]   \n",
       "21      chaypiqa        [chay, pi, qa]            [[chay, pi, qa]]   \n",
       "22        niqchu          [ni, q, chu]              [[ni, q, chu]]   \n",
       "23        napiqa          [na, pi, qa]              [[na, pi, qa]]   \n",
       "24        ripuni            [ri, puni]              [[ri, pu, ni]]   \n",
       "25     kapunchik       [ka, pu, nchik]           [[ka, pu, nchik]]   \n",
       "26        raykum              [raykum]           [[ima, rayku, m]]   \n",
       "27        nispam          [ni, spa, m]              [[ni, spa, m]]   \n",
       "28       rimanku         [ri, ma, nku]             [[rima, n, ku]]   \n",
       "29       punchay           [puncha, y]                 [[punchay]]   \n",
       "30    tumankupas      [tuma, nkupa, s]          [[tuma, nku, pas]]   \n",
       "31        chikan            [chika, n]                  [[chikan]]   \n",
       "32         munay             [muna, y]                 [[muna, y]]   \n",
       "33        atikun          [ati, ku, n]              [[ati, ku, n]]   \n",
       "34        chunka              [chunka]                  [[chunka]]   \n",
       "35    saniyapuni      [sani, ya, puni]          [[sani, ya, puni]]   \n",
       "36      hampusaq          [hampu, saq]              [[hampu, saq]]   \n",
       "37    ñuqaykupis      [ñuqa, yku, pis]          [[ñuqa, yku, pis]]   \n",
       "38       chhikan           [chhika, n]                 [[chhikan]]   \n",
       "39      santusmi        [santu, s, mi]              [[santus, mi]]   \n",
       "40         sutin             [suti, n]                 [[suti, n]]   \n",
       "41  intirukunapi  [inti, ru, kuna, pi]        [[intiru, kuna, pi]]   \n",
       "42       wasinta         [wasi, n, ta]             [[wasi, n, ta]]   \n",
       "43  tomansunchik  [toma, n, su, nchik]        [[toman, su, nchik]]   \n",
       "44    uyarispaqa      [uyari, spa, qa]          [[uyari, spa, qa]]   \n",
       "45     turaykuna       [tura, y, kuna]           [[tura, y, kuna]]   \n",
       "46       aparuni         [apa, ru, ni]             [[apa, ru, ni]]   \n",
       "47       tiyakuq         [tiya, ku, q]             [[tiya, ku, q]]   \n",
       "48        tumani            [tuma, ni]                [[tuma, ni]]   \n",
       "49    qhisakuwan      [qhisa, ku, wan]            [[qhisaku, wan]]   \n",
       "\n",
       "   PredBoundaries(tok_idx) GoldBoundaries(Chosen tok_idx)  TP  FP  FN  \\\n",
       "0                      [4]                            [2]   0   1   1   \n",
       "1                   [2, 5]                         [2, 5]   2   0   0   \n",
       "2                   [3, 5]                            [5]   1   1   0   \n",
       "3                      [3]                            [3]   1   0   0   \n",
       "4                   [1, 5]                         [1, 5]   2   0   0   \n",
       "5                      [4]                         [2, 4]   1   0   1   \n",
       "6                      [3]                            [3]   1   0   0   \n",
       "7                      [4]                            [4]   1   0   0   \n",
       "8                   [2, 6]                         [2, 6]   2   0   0   \n",
       "9                      [3]                             []   0   1   0   \n",
       "10                     [4]                            [4]   1   0   0   \n",
       "11                     [3]                            [3]   1   0   0   \n",
       "12                  [1, 3]                         [1, 3]   2   0   0   \n",
       "13                      []                             []   0   0   0   \n",
       "14                     [4]                            [4]   1   0   0   \n",
       "15                     [3]                             []   0   1   0   \n",
       "16                     [3]                            [3]   1   0   0   \n",
       "17                     [2]                            [2]   1   0   0   \n",
       "18                     [3]                            [3]   1   0   0   \n",
       "19                     [3]                            [3]   1   0   0   \n",
       "20                     [3]                            [3]   1   0   0   \n",
       "21                  [2, 4]                         [2, 4]   2   0   0   \n",
       "22                  [1, 2]                         [1, 2]   2   0   0   \n",
       "23                  [1, 3]                         [1, 3]   2   0   0   \n",
       "24                     [1]                         [1, 3]   1   0   1   \n",
       "25                  [1, 3]                         [1, 3]   2   0   0   \n",
       "26                      []                         [2, 7]   0   0   2   \n",
       "27                  [1, 4]                         [1, 4]   2   0   0   \n",
       "28                  [1, 3]                         [3, 4]   1   1   1   \n",
       "29                     [4]                             []   0   1   0   \n",
       "30                  [3, 8]                         [3, 6]   1   1   1   \n",
       "31                     [3]                             []   0   1   0   \n",
       "32                     [3]                            [3]   1   0   0   \n",
       "33                  [2, 4]                         [2, 4]   2   0   0   \n",
       "34                      []                             []   0   0   0   \n",
       "35                  [3, 5]                         [3, 5]   2   0   0   \n",
       "36                     [4]                            [4]   1   0   0   \n",
       "37                  [3, 6]                         [3, 6]   2   0   0   \n",
       "38                     [4]                             []   0   1   0   \n",
       "39                  [4, 5]                            [5]   1   1   0   \n",
       "40                     [3]                            [3]   1   0   0   \n",
       "41               [3, 5, 9]                         [5, 9]   2   1   0   \n",
       "42                  [3, 4]                         [3, 4]   2   0   0   \n",
       "43               [3, 4, 6]                         [4, 6]   2   1   0   \n",
       "44                  [4, 7]                         [4, 7]   2   0   0   \n",
       "45                  [3, 4]                         [3, 4]   2   0   0   \n",
       "46                  [2, 4]                         [2, 4]   2   0   0   \n",
       "47                  [3, 5]                         [3, 5]   2   0   0   \n",
       "48                     [3]                            [3]   1   0   0   \n",
       "49                  [3, 5]                            [5]   1   1   0   \n",
       "\n",
       "      P_word  R_word   F1_word  CorrectExactSeg  CorrectSplitCount  \\\n",
       "0   0.000000     0.0  0.000000            False               True   \n",
       "1   1.000000     1.0  1.000000             True               True   \n",
       "2   0.500000     1.0  0.666667            False              False   \n",
       "3   1.000000     1.0  1.000000             True               True   \n",
       "4   1.000000     1.0  1.000000             True               True   \n",
       "5   1.000000     0.5  0.666667            False               True   \n",
       "6   1.000000     1.0  1.000000             True               True   \n",
       "7   1.000000     1.0  1.000000             True               True   \n",
       "8   1.000000     1.0  1.000000             True               True   \n",
       "9   0.000000     0.0  0.000000            False              False   \n",
       "10  1.000000     1.0  1.000000             True               True   \n",
       "11  1.000000     1.0  1.000000             True               True   \n",
       "12  1.000000     1.0  1.000000             True               True   \n",
       "13  1.000000     1.0  1.000000             True               True   \n",
       "14  1.000000     1.0  1.000000             True               True   \n",
       "15  0.000000     0.0  0.000000            False              False   \n",
       "16  1.000000     1.0  1.000000             True               True   \n",
       "17  1.000000     1.0  1.000000             True               True   \n",
       "18  1.000000     1.0  1.000000             True               True   \n",
       "19  1.000000     1.0  1.000000             True               True   \n",
       "20  1.000000     1.0  1.000000             True               True   \n",
       "21  1.000000     1.0  1.000000             True               True   \n",
       "22  1.000000     1.0  1.000000             True               True   \n",
       "23  1.000000     1.0  1.000000             True               True   \n",
       "24  1.000000     0.5  0.666667            False              False   \n",
       "25  1.000000     1.0  1.000000             True               True   \n",
       "26  0.000000     0.0  0.000000            False              False   \n",
       "27  1.000000     1.0  1.000000             True               True   \n",
       "28  0.500000     0.5  0.500000            False               True   \n",
       "29  0.000000     0.0  0.000000            False              False   \n",
       "30  0.500000     0.5  0.500000            False               True   \n",
       "31  0.000000     0.0  0.000000            False              False   \n",
       "32  1.000000     1.0  1.000000             True               True   \n",
       "33  1.000000     1.0  1.000000             True               True   \n",
       "34  1.000000     1.0  1.000000             True               True   \n",
       "35  1.000000     1.0  1.000000             True               True   \n",
       "36  1.000000     1.0  1.000000             True               True   \n",
       "37  1.000000     1.0  1.000000             True               True   \n",
       "38  0.000000     0.0  0.000000            False              False   \n",
       "39  0.500000     1.0  0.666667            False              False   \n",
       "40  1.000000     1.0  1.000000             True               True   \n",
       "41  0.666667     1.0  0.800000            False              False   \n",
       "42  1.000000     1.0  1.000000             True               True   \n",
       "43  0.666667     1.0  0.800000            False              False   \n",
       "44  1.000000     1.0  1.000000             True               True   \n",
       "45  1.000000     1.0  1.000000             True               True   \n",
       "46  1.000000     1.0  1.000000             True               True   \n",
       "47  1.000000     1.0  1.000000             True               True   \n",
       "48  1.000000     1.0  1.000000             True               True   \n",
       "49  0.500000     1.0  0.666667            False              False   \n",
       "\n",
       "    SplitCount+1  SplitCount-1  SplitCount±1  OverlapExactAndSplit  \n",
       "0          False         False          True                 False  \n",
       "1          False         False          True                  True  \n",
       "2           True         False          True                 False  \n",
       "3          False         False          True                  True  \n",
       "4          False         False          True                  True  \n",
       "5          False          True          True                 False  \n",
       "6          False         False          True                  True  \n",
       "7          False         False          True                  True  \n",
       "8          False         False          True                  True  \n",
       "9           True         False          True                 False  \n",
       "10         False         False          True                  True  \n",
       "11         False         False          True                  True  \n",
       "12         False         False          True                  True  \n",
       "13         False         False          True                  True  \n",
       "14         False         False          True                  True  \n",
       "15          True         False          True                 False  \n",
       "16         False         False          True                  True  \n",
       "17         False         False          True                  True  \n",
       "18         False         False          True                  True  \n",
       "19         False         False          True                  True  \n",
       "20         False         False          True                  True  \n",
       "21         False         False          True                  True  \n",
       "22         False         False          True                  True  \n",
       "23         False         False          True                  True  \n",
       "24         False          True          True                 False  \n",
       "25         False         False          True                  True  \n",
       "26         False         False         False                 False  \n",
       "27         False         False          True                  True  \n",
       "28         False         False          True                 False  \n",
       "29          True         False          True                 False  \n",
       "30         False         False          True                 False  \n",
       "31          True         False          True                 False  \n",
       "32         False         False          True                  True  \n",
       "33         False         False          True                  True  \n",
       "34         False         False          True                  True  \n",
       "35         False         False          True                  True  \n",
       "36         False         False          True                  True  \n",
       "37         False         False          True                  True  \n",
       "38          True         False          True                 False  \n",
       "39          True         False          True                 False  \n",
       "40         False         False          True                  True  \n",
       "41          True         False          True                 False  \n",
       "42         False         False          True                  True  \n",
       "43          True         False          True                 False  \n",
       "44         False         False          True                  True  \n",
       "45         False         False          True                  True  \n",
       "46         False         False          True                  True  \n",
       "47         False         False          True                  True  \n",
       "48         False         False          True                  True  \n",
       "49          True         False          True                 False  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6939ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
