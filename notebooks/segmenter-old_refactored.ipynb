{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfd08938",
   "metadata": {},
   "source": [
    "# Segmenter-Old: Character-Level BiLSTM Morphology Parser\n",
    "\n",
    "Character-level BiLSTM for Quechua morphological segmentation. Predicts boundary positions at the character level, marking where morpheme boundaries occur within words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d6561a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import ast\n",
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "import pickle\n",
    "import math\n",
    "import random\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ML & DL\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3817e5e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "DATA_FOLDER = \"data\"\n",
    "MODEL_NAME = \"segmenter-old\"\n",
    "MODELS_FOLDER = f\"models_{MODEL_NAME}\"\n",
    "os.makedirs(DATA_FOLDER, exist_ok=True)\n",
    "os.makedirs(MODELS_FOLDER, exist_ok=True)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "# Special tokens\n",
    "PAD, UNK = \"<PAD>\", \"<UNK>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495464f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading gold data...\n",
      "got 6,896 gold examples\n"
     ]
    }
   ],
   "source": [
    "# Load gold standard data\n",
    "print(\"loading gold data...\")\n",
    "gold_df = pd.read_parquet(os.path.join(DATA_FOLDER, \"insert_your_data.parquet\"))\n",
    "gold_df['Word'] = gold_df['word']\n",
    "gold_df['morph'] = gold_df['morph'].str.replace('-', ' ')\n",
    "gold_df['Morph_split_str'] = gold_df['morph']\n",
    "gold_df['Morph_split'] = gold_df['morph'].str.split(' ')\n",
    "gold_df = gold_df[['Word', 'Morph_split', 'Morph_split_str']]\n",
    "gold_df.drop_duplicates(subset='Word', keep='first', inplace=True)\n",
    "gold_df.dropna(subset=['Word'], inplace=True)\n",
    "print(f\"got {len(gold_df):,} gold examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3582128a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract basic features\n",
    "gold_df['num_morphemes'] = gold_df['Morph_split'].apply(len)\n",
    "gold_df['word_len'] = gold_df['Word'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3f0b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_boundary_labels(word, split):\n",
    "    \"\"\"Generate binary boundary labels for a word given its morpheme split.\"\"\"\n",
    "    labels = [0] * len(word)\n",
    "    idx = 0\n",
    "    for morpheme in split[:-1]:\n",
    "        idx += len(morpheme)\n",
    "        if idx < len(word):\n",
    "            labels[idx - 1] = 1\n",
    "    return labels\n",
    "\n",
    "# Prepare training data\n",
    "gold_df['char_seq'] = gold_df['Word'].apply(list)\n",
    "gold_df['boundary_labels'] = gold_df.apply(\n",
    "    lambda row: get_boundary_labels(row['Word'], row['Morph_split']), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e94e3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 55 characters\n"
     ]
    }
   ],
   "source": [
    "def build_vocab(seqs: List[List[str]]):\n",
    "    \"\"\"Build vocabulary from character sequences.\"\"\"\n",
    "    chars = {c for seq in seqs for c in seq}\n",
    "    itos = [PAD, UNK] + sorted(chars)\n",
    "    stoi = {ch: i for i, ch in enumerate(itos)}\n",
    "    return stoi, itos\n",
    "\n",
    "stoi, itos = build_vocab(gold_df[\"char_seq\"].tolist())\n",
    "print(f\"vocab size: {len(itos)} characters\")\n",
    "\n",
    "def encode(seq: List[str]) -> List[int]:\n",
    "    \"\"\"Convert character sequence to integer IDs.\"\"\"\n",
    "    return [stoi.get(c, stoi[UNK]) for c in seq]\n",
    "\n",
    "def encode_labels(labels: List[int]) -> List[int]:\n",
    "    \"\"\"Labels are already 0/1.\"\"\"\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad2a8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharBoundaryDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for character-level boundary prediction.\"\"\"\n",
    "    def __init__(self, df):\n",
    "        self.x = df[\"char_seq\"].tolist()\n",
    "        self.y = df[\"boundary_labels\"].tolist()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "def pad_batch(batch, pad_id=0):\n",
    "    \"\"\"Collate function: pads sequences to the same length.\"\"\"\n",
    "    seqs, labels = zip(*batch)\n",
    "    x_ids = [encode(s) for s in seqs]\n",
    "    y_ids = [encode_labels(y) for y in labels]\n",
    "    lengths = [len(x) for x in x_ids]\n",
    "    maxlen = max(lengths)\n",
    "    \n",
    "    x_pad = [xi + [pad_id] * (maxlen - len(xi)) for xi in x_ids]\n",
    "    y_pad = [yi + [0] * (maxlen - len(yi)) for yi in y_ids]\n",
    "    mask = [[1] * len(xi) + [0] * (maxlen - len(xi)) for xi in x_ids]\n",
    "    \n",
    "    return (\n",
    "        torch.LongTensor(x_pad),\n",
    "        torch.FloatTensor(y_pad),\n",
    "        torch.BoolTensor(mask),\n",
    "        torch.LongTensor(lengths),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7417024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 6,206 samples\n",
      "validation: 690 samples\n"
     ]
    }
   ],
   "source": [
    "# Train/validation split\n",
    "rng = np.random.default_rng(42)\n",
    "indices = np.arange(len(gold_df))\n",
    "rng.shuffle(indices)\n",
    "split = int(0.9 * len(indices))\n",
    "train_idx, val_idx = indices[:split], indices[split:]\n",
    "\n",
    "train_df = gold_df.iloc[train_idx].reset_index(drop=True)\n",
    "val_df = gold_df.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "print(f\"training: {len(train_df):,} samples\")\n",
    "print(f\"validation: {len(val_df):,} samples\")\n",
    "\n",
    "train_ds = CharBoundaryDataset(train_df)\n",
    "val_ds = CharBoundaryDataset(val_df)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=pad_batch)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=pad_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b8c95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMBoundary(nn.Module):\n",
    "    \"\"\"Bidirectional LSTM for character-level boundary prediction.\"\"\"\n",
    "    def __init__(self, vocab_size: int, emb_dim: int = 16, hidden_size: int = 16,\n",
    "                 num_layers: int = 1, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=emb_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(hidden_size * 2, 1)\n",
    "    \n",
    "    def forward(self, x, lengths):\n",
    "        \"\"\"Forward pass through the model.\"\"\"\n",
    "        emb = self.emb(x)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(emb, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_out, _ = self.lstm(packed)\n",
    "        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)\n",
    "        out = self.dropout(out)\n",
    "        logits = self.out(out).squeeze(-1)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c092b5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_bce_loss(logits, targets, mask):\n",
    "    \"\"\"Compute masked binary cross-entropy loss.\"\"\"\n",
    "    loss_fn = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "    loss_per_token = loss_fn(logits, targets)\n",
    "    loss_per_token = loss_per_token * mask.float()\n",
    "    denom = mask.float().sum().clamp_min(1.0)\n",
    "    return loss_per_token.sum() / denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83007df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boundary_f1(logits, targets, mask, threshold=0.5):\n",
    "    \"\"\"Compute precision, recall, and F1 for boundary prediction.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        probs = torch.sigmoid(logits)\n",
    "        preds = (probs >= threshold).long()\n",
    "        t = targets.long()\n",
    "        m = mask.long()\n",
    "\n",
    "        tp = ((preds == 1) & (t == 1) & (m == 1)).sum().item()\n",
    "        fp = ((preds == 1) & (t == 0) & (m == 1)).sum().item()\n",
    "        fn = ((preds == 0) & (t == 1) & (m == 1)).sum().item()\n",
    "\n",
    "        prec = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        rec = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1 = 2 * prec * rec / (prec + rec) if (prec + rec) > 0 else 0.0\n",
    "        return prec, rec, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29d209c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_boundaries(words: List[str], model, stoi, threshold=0.5) -> List[List[int]]:\n",
    "    \"\"\"Predict boundary labels for a list of words.\"\"\"\n",
    "    model.eval()\n",
    "    char_lists = [list(w) for w in words]\n",
    "    x_ids = [[stoi.get(c, stoi[UNK]) for c in chars] for chars in char_lists]\n",
    "    lengths = [len(x) for x in x_ids]\n",
    "    maxlen = max(lengths)\n",
    "    pad_id = stoi[PAD]\n",
    "\n",
    "    x_pad = [xi + [pad_id] * (maxlen - len(xi)) for xi in x_ids]\n",
    "    mask = [[1] * len(xi) + [0] * (maxlen - len(xi)) for xi in x_ids]\n",
    "\n",
    "    x = torch.LongTensor(x_pad).to(device)\n",
    "    lengths_t = torch.LongTensor(lengths).to(device)\n",
    "    mask_t = torch.BoolTensor(mask).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(x, lengths_t)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        preds = (probs >= threshold) & mask_t\n",
    "\n",
    "    out = []\n",
    "    for i, L in enumerate(lengths):\n",
    "        out.append(preds[i, :L].int().tolist())\n",
    "    return out\n",
    "\n",
    "def apply_boundaries(word: str, boundary_labels: List[int]) -> List[str]:\n",
    "    \"\"\"Reconstruct morphemes from word and boundary labels.\"\"\"\n",
    "    segs = []\n",
    "    start = 0\n",
    "    for i, b in enumerate(boundary_labels):\n",
    "        if b == 1:\n",
    "            segs.append(word[start:i+1])\n",
    "            start = i + 1\n",
    "    if start < len(word):\n",
    "        segs.append(word[start:])\n",
    "    return segs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f2619f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_id(emb_dim, hidden_size, num_layers, dropout, epochs, batch_size, lr, weight_decay):\n",
    "    \"\"\"Hash training params to get unique model ID.\"\"\"\n",
    "    params_dict = {\n",
    "        'emb_dim': emb_dim,\n",
    "        'hidden_size': hidden_size,\n",
    "        'num_layers': num_layers,\n",
    "        'dropout': dropout,\n",
    "        'epochs': epochs,\n",
    "        'batch_size': batch_size,\n",
    "        'lr': lr,\n",
    "        'weight_decay': weight_decay,\n",
    "        'vocab_size': len(itos)\n",
    "    }\n",
    "    params_str = json.dumps(params_dict, sort_keys=True)\n",
    "    return hashlib.md5(params_str.encode()).hexdigest()[:16]\n",
    "\n",
    "def save_model_checkpoint(model, stoi, itos, model_id, models_folder=MODELS_FOLDER):\n",
    "    \"\"\"Save model checkpoint.\"\"\"\n",
    "    model_dir = os.path.join(models_folder, model_id)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    checkpoint_path = os.path.join(model_dir, \"bilstm_char_boundary.pt\")\n",
    "    torch.save({\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"stoi\": stoi,\n",
    "        \"itos\": itos\n",
    "    }, checkpoint_path)\n",
    "    \n",
    "    metadata_path = os.path.join(model_dir, \"metadata.json\")\n",
    "    with open(metadata_path, \"w\") as f:\n",
    "        json.dump({\n",
    "            'model_id': model_id,\n",
    "            'vocab_size': len(itos),\n",
    "            'model_name': MODEL_NAME\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    print(f\"saved checkpoint to {model_dir}\")\n",
    "    return model_dir\n",
    "\n",
    "def load_model_checkpoint(model_id, models_folder=MODELS_FOLDER):\n",
    "    \"\"\"Load model checkpoint.\"\"\"\n",
    "    model_dir = os.path.join(models_folder, model_id)\n",
    "    checkpoint_path = os.path.join(model_dir, \"bilstm_char_boundary.pt\")\n",
    "    \n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        return None\n",
    "    \n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    print(f\"loaded checkpoint from {model_dir}\")\n",
    "    return {\n",
    "        'model_state': checkpoint['model_state'],\n",
    "        'stoi': checkpoint['stoi'],\n",
    "        'itos': checkpoint['itos'],\n",
    "        'checkpoint_path': checkpoint_path,\n",
    "        'model_dir': model_dir\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ef8ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looking for model 6112ccdaef2e0c54...\n",
      "loaded checkpoint from models_segmenter-old\\6112ccdaef2e0c54\n",
      "found it! loading from models_segmenter-old\\6112ccdaef2e0c54\n",
      "skipping training, model ready\n"
     ]
    }
   ],
   "source": [
    "# Model hyperparameters\n",
    "EMB_DIM = 16\n",
    "HIDDEN_SIZE = 32\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.3\n",
    "EPOCHS = 35\n",
    "BATCH_SIZE = 64\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "# Generate model identifier\n",
    "model_id = generate_model_id(EMB_DIM, HIDDEN_SIZE, NUM_LAYERS, DROPOUT, EPOCHS, BATCH_SIZE, LR, WEIGHT_DECAY)\n",
    "\n",
    "# Try to load existing model\n",
    "print(f\"looking for model {model_id}...\")\n",
    "loaded = load_model_checkpoint(model_id, models_folder=MODELS_FOLDER)\n",
    "\n",
    "if loaded is not None:\n",
    "    print(f\"found it! loading from {loaded['model_dir']}\")\n",
    "    stoi = loaded['stoi']\n",
    "    itos = loaded['itos']\n",
    "    model = BiLSTMBoundary(vocab_size=len(itos), emb_dim=EMB_DIM, hidden_size=HIDDEN_SIZE,\n",
    "                           num_layers=NUM_LAYERS, dropout=DROPOUT).to(device)\n",
    "    model.load_state_dict(loaded['model_state'])\n",
    "    model.eval()\n",
    "    print(\"skipping training, model ready\")\n",
    "else:\n",
    "    print(f\"not found, training from scratch...\")\n",
    "    \n",
    "    model = BiLSTMBoundary(vocab_size=len(itos), emb_dim=EMB_DIM, hidden_size=HIDDEN_SIZE,\n",
    "                          num_layers=NUM_LAYERS, dropout=DROPOUT).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    \n",
    "    best_val_f1 = 0.0\n",
    "    \n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        total_tokens = 0\n",
    "        for x, y, mask, lengths in train_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            mask = mask.to(device)\n",
    "            lengths = lengths.to(device)\n",
    "\n",
    "            logits = model(x, lengths)\n",
    "            loss = masked_bce_loss(logits, y, mask)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * mask.sum().item()\n",
    "            total_tokens += mask.sum().item()\n",
    "\n",
    "        train_loss = total_loss / max(total_tokens, 1)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, val_tokens = 0.0, 0\n",
    "        all_prec, all_rec, all_f1 = [], [], []\n",
    "        with torch.no_grad():\n",
    "            for x, y, mask, lengths in val_loader:\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                mask = mask.to(device)\n",
    "                lengths = lengths.to(device)\n",
    "\n",
    "                logits = model(x, lengths)\n",
    "                loss = masked_bce_loss(logits, y, mask)\n",
    "                val_loss += loss.item() * mask.sum().item()\n",
    "                val_tokens += mask.sum().item()\n",
    "\n",
    "                p, r, f = boundary_f1(logits, y, mask, threshold=0.5)\n",
    "                all_prec.append(p)\n",
    "                all_rec.append(r)\n",
    "                all_f1.append(f)\n",
    "\n",
    "        val_loss = val_loss / max(val_tokens, 1)\n",
    "        prec = np.mean(all_prec) if all_prec else 0.0\n",
    "        rec = np.mean(all_rec) if all_rec else 0.0\n",
    "        f1 = np.mean(all_f1) if all_f1 else 0.0\n",
    "\n",
    "        print(f\"epoch {epoch:02d} | train_loss={train_loss:.4f}  val_loss={val_loss:.4f}  P={prec:.3f} R={rec:.3f} F1={f1:.3f}\")\n",
    "\n",
    "        if f1 > best_val_f1:\n",
    "            best_val_f1 = f1\n",
    "            save_model_checkpoint(model, stoi, itos, model_id, models_folder=MODELS_FOLDER)\n",
    "            print(\"  â†³ saved checkpoint (best F1 so far)\")\n",
    "    \n",
    "    print(f\"\\ntraining done! best validation F1: {best_val_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb1ee4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_kfold_cross_validation(\n",
    "    df,\n",
    "    n_folds=5,\n",
    "    emb_dim=16,\n",
    "    hidden_size=32,\n",
    "    num_layers=2,\n",
    "    dropout=0.3,\n",
    "    epochs=35,\n",
    "    batch_size=64,\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-4,\n",
    "    random_state=42,\n",
    "    device=device\n",
    "):\n",
    "    \"\"\"K-fold cross-validation for more robust evaluation.\"\"\"\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"K-FOLD CV (k={n_folds})\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    \n",
    "    kfold = KFold(n_splits=n_folds, shuffle=True, random_state=random_state)\n",
    "    indices = np.arange(len(df))\n",
    "    \n",
    "    fold_results = []\n",
    "    all_metrics = {\n",
    "        'boundary_precision': [],\n",
    "        'boundary_recall': [],\n",
    "        'boundary_f1': [],\n",
    "        'val_loss': []\n",
    "    }\n",
    "    \n",
    "    for fold_idx, (train_indices, val_indices) in enumerate(kfold.split(indices), 1):\n",
    "        print(f\"\\n--- fold {fold_idx}/{n_folds} ---\")\n",
    "        print(f\"train: {len(train_indices)}, val: {len(val_indices)}\")\n",
    "        \n",
    "        train_df_fold = df.iloc[train_indices].reset_index(drop=True)\n",
    "        val_df_fold = df.iloc[val_indices].reset_index(drop=True)\n",
    "        \n",
    "        stoi_fold, itos_fold = build_vocab(train_df_fold[\"char_seq\"].tolist())\n",
    "        vocab_size = len(itos_fold)\n",
    "        \n",
    "        # Create fold-specific collate function that uses fold vocabulary\n",
    "        pad_id_fold = stoi_fold[PAD]\n",
    "        unk_id_fold = stoi_fold[UNK]\n",
    "        \n",
    "        def pad_batch_fold(batch):\n",
    "            seqs, labels = zip(*batch)\n",
    "            x_ids = [[stoi_fold.get(c, unk_id_fold) for c in s] for s in seqs]\n",
    "            # Clamp indices to valid range (safety check)\n",
    "            x_ids = [[min(max(idx, 0), vocab_size - 1) for idx in seq] for seq in x_ids]\n",
    "            y_ids = [encode_labels(y) for y in labels]\n",
    "            lengths = [len(x) for x in x_ids]\n",
    "            maxlen = max(lengths) if lengths else 0\n",
    "            \n",
    "            x_pad = [xi + [pad_id_fold] * (maxlen - len(xi)) for xi in x_ids]\n",
    "            y_pad = [yi + [0] * (maxlen - len(yi)) for yi in y_ids]\n",
    "            mask = [[1] * len(xi) + [0] * (maxlen - len(xi)) for xi in x_ids]\n",
    "            \n",
    "            return (\n",
    "                torch.LongTensor(x_pad),\n",
    "                torch.FloatTensor(y_pad),\n",
    "                torch.BoolTensor(mask),\n",
    "                torch.LongTensor(lengths),\n",
    "            )\n",
    "        \n",
    "        train_ds_fold = CharBoundaryDataset(train_df_fold)\n",
    "        val_ds_fold = CharBoundaryDataset(val_df_fold)\n",
    "        train_loader_fold = DataLoader(train_ds_fold, batch_size=batch_size, shuffle=True, collate_fn=pad_batch_fold)\n",
    "        val_loader_fold = DataLoader(val_ds_fold, batch_size=batch_size, shuffle=False, collate_fn=pad_batch_fold)\n",
    "        \n",
    "        model_fold = BiLSTMBoundary(\n",
    "            vocab_size=vocab_size,\n",
    "            emb_dim=emb_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout\n",
    "        ).to(device)\n",
    "        \n",
    "        optimizer_fold = torch.optim.AdamW(model_fold.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        \n",
    "        best_val_f1 = 0.0\n",
    "        best_val_prec = 0.0\n",
    "        best_val_rec = 0.0\n",
    "        best_val_loss = float('inf')\n",
    "        best_epoch = 0\n",
    "        \n",
    "        for epoch in range(1, epochs + 1):\n",
    "            model_fold.train()\n",
    "            total_loss = 0.0\n",
    "            total_tokens = 0\n",
    "            for x, y, mask, lengths in train_loader_fold:\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                mask = mask.to(device)\n",
    "                lengths = lengths.to(device)\n",
    "                \n",
    "                logits = model_fold(x, lengths)\n",
    "                loss = masked_bce_loss(logits, y, mask)\n",
    "                \n",
    "                optimizer_fold.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(model_fold.parameters(), 1.0)\n",
    "                optimizer_fold.step()\n",
    "                \n",
    "                total_loss += loss.item() * mask.sum().item()\n",
    "                total_tokens += mask.sum().item()\n",
    "            \n",
    "            train_loss = total_loss / max(total_tokens, 1)\n",
    "            \n",
    "            model_fold.eval()\n",
    "            val_loss, val_tokens = 0.0, 0\n",
    "            all_prec, all_rec, all_f1 = [], [], []\n",
    "            with torch.no_grad():\n",
    "                for x, y, mask, lengths in val_loader_fold:\n",
    "                    x = x.to(device)\n",
    "                    y = y.to(device)\n",
    "                    mask = mask.to(device)\n",
    "                    lengths = lengths.to(device)\n",
    "                    \n",
    "                    logits = model_fold(x, lengths)\n",
    "                    loss = masked_bce_loss(logits, y, mask)\n",
    "                    val_loss += loss.item() * mask.sum().item()\n",
    "                    val_tokens += mask.sum().item()\n",
    "                    \n",
    "                    p, r, f = boundary_f1(logits, y, mask, threshold=0.5)\n",
    "                    all_prec.append(p)\n",
    "                    all_rec.append(r)\n",
    "                    all_f1.append(f)\n",
    "            \n",
    "            val_loss = val_loss / max(val_tokens, 1)\n",
    "            prec = np.mean(all_prec) if all_prec else 0.0\n",
    "            rec = np.mean(all_rec) if all_rec else 0.0\n",
    "            f1 = np.mean(all_f1) if all_f1 else 0.0\n",
    "            \n",
    "            print(f\"  ep {epoch:02d} | train_loss={train_loss:.4f}  val_loss={val_loss:.4f}  P={prec:.3f} R={rec:.3f} F1={f1:.3f}\")\n",
    "            \n",
    "            if f1 > best_val_f1 or (np.isclose(f1, best_val_f1) and val_loss < best_val_loss):\n",
    "                best_val_f1 = f1\n",
    "                best_val_prec = prec\n",
    "                best_val_rec = rec\n",
    "                best_val_loss = val_loss\n",
    "                best_epoch = epoch\n",
    "        \n",
    "        print(f\"\\n  best epoch: {best_epoch}\")\n",
    "        print(f\"  best validation: P={best_val_prec:.3f} R={best_val_rec:.3f} F1={best_val_f1:.3f} Loss={best_val_loss:.4f}\")\n",
    "        \n",
    "        fold_results.append({\n",
    "            'fold': fold_idx,\n",
    "            'boundary_precision': best_val_prec,\n",
    "            'boundary_recall': best_val_rec,\n",
    "            'boundary_f1': best_val_f1,\n",
    "            'val_loss': best_val_loss,\n",
    "            'best_epoch': best_epoch\n",
    "        })\n",
    "        \n",
    "        all_metrics['boundary_precision'].append(best_val_prec)\n",
    "        all_metrics['boundary_recall'].append(best_val_rec)\n",
    "        all_metrics['boundary_f1'].append(best_val_f1)\n",
    "        all_metrics['val_loss'].append(best_val_loss)\n",
    "    \n",
    "    mean_metrics = {k: np.mean(v) for k, v in all_metrics.items()}\n",
    "    std_metrics = {k: np.std(v) for k, v in all_metrics.items()}\n",
    "    best_fold_idx = max(range(len(fold_results)), key=lambda i: fold_results[i]['boundary_f1'])\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(\"CV SUMMARY\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    for r in fold_results:\n",
    "        print(f\"  fold {r['fold']}: P={r['boundary_precision']:.3f}, R={r['boundary_recall']:.3f}, \"\n",
    "              f\"F1={r['boundary_f1']:.3f}, Loss={r['val_loss']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nmean +/- std over {n_folds} folds:\")\n",
    "    print(f\"  precision: {mean_metrics['boundary_precision']:.3f} +/- {std_metrics['boundary_precision']:.3f}\")\n",
    "    print(f\"  recall:    {mean_metrics['boundary_recall']:.3f} +/- {std_metrics['boundary_recall']:.3f}\")\n",
    "    print(f\"  F1:        {mean_metrics['boundary_f1']:.3f} +/- {std_metrics['boundary_f1']:.3f}\")\n",
    "    print(f\"  loss:      {mean_metrics['val_loss']:.4f} +/- {std_metrics['val_loss']:.4f}\")\n",
    "    print(f\"\\nbest fold: {fold_results[best_fold_idx]['fold']} (F1={fold_results[best_fold_idx]['boundary_f1']:.3f})\")\n",
    "    print(f\"{'=' * 80}\\n\")\n",
    "    \n",
    "    return {\n",
    "        'fold_results': fold_results,\n",
    "        'mean_metrics': mean_metrics,\n",
    "        'std_metrics': std_metrics,\n",
    "        'best_fold_idx': best_fold_idx,\n",
    "        'all_metrics': all_metrics\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d40a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "K-FOLD CV (k=5)\n",
      "================================================================================\n",
      "\n",
      "--- fold 1/5 ---\n",
      "train: 5516, val: 1380\n",
      "  ep 01 | train_loss=0.5296  val_loss=0.4155  P=0.000 R=0.000 F1=0.000\n",
      "  ep 02 | train_loss=0.2724  val_loss=0.2029  P=0.809 R=0.815 F1=0.812\n",
      "  ep 03 | train_loss=0.1847  val_loss=0.1632  P=0.830 R=0.864 F1=0.846\n",
      "  ep 04 | train_loss=0.1524  val_loss=0.1379  P=0.840 R=0.927 F1=0.881\n",
      "  ep 05 | train_loss=0.1263  val_loss=0.1145  P=0.885 R=0.903 F1=0.894\n",
      "  ep 06 | train_loss=0.1104  val_loss=0.1040  P=0.872 R=0.935 F1=0.903\n",
      "  ep 07 | train_loss=0.0961  val_loss=0.0908  P=0.896 R=0.941 F1=0.918\n",
      "  ep 08 | train_loss=0.0882  val_loss=0.0824  P=0.921 R=0.927 F1=0.923\n",
      "  ep 09 | train_loss=0.0816  val_loss=0.0784  P=0.910 R=0.948 F1=0.929\n",
      "  ep 10 | train_loss=0.0762  val_loss=0.0756  P=0.912 R=0.952 F1=0.932\n",
      "  ep 11 | train_loss=0.0715  val_loss=0.0704  P=0.932 R=0.936 F1=0.934\n",
      "  ep 12 | train_loss=0.0671  val_loss=0.0684  P=0.950 R=0.917 F1=0.933\n",
      "  ep 13 | train_loss=0.0645  val_loss=0.0681  P=0.960 R=0.916 F1=0.937\n",
      "  ep 14 | train_loss=0.0621  val_loss=0.0635  P=0.945 R=0.933 F1=0.939\n",
      "  ep 15 | train_loss=0.0578  val_loss=0.0600  P=0.955 R=0.936 F1=0.945\n",
      "  ep 16 | train_loss=0.0562  val_loss=0.0600  P=0.928 R=0.961 F1=0.944\n",
      "  ep 17 | train_loss=0.0540  val_loss=0.0582  P=0.946 R=0.946 F1=0.946\n",
      "  ep 18 | train_loss=0.0506  val_loss=0.0581  P=0.948 R=0.945 F1=0.946\n",
      "  ep 19 | train_loss=0.0504  val_loss=0.0556  P=0.951 R=0.945 F1=0.948\n",
      "  ep 20 | train_loss=0.0464  val_loss=0.0624  P=0.915 R=0.973 F1=0.943\n",
      "  ep 21 | train_loss=0.0457  val_loss=0.0555  P=0.953 R=0.943 F1=0.948\n",
      "  ep 22 | train_loss=0.0439  val_loss=0.0517  P=0.949 R=0.952 F1=0.951\n",
      "  ep 23 | train_loss=0.0430  val_loss=0.0515  P=0.958 R=0.950 F1=0.954\n",
      "  ep 24 | train_loss=0.0419  val_loss=0.0509  P=0.960 R=0.942 F1=0.951\n",
      "  ep 25 | train_loss=0.0400  val_loss=0.0529  P=0.959 R=0.938 F1=0.948\n",
      "  ep 26 | train_loss=0.0383  val_loss=0.0515  P=0.959 R=0.941 F1=0.949\n",
      "  ep 27 | train_loss=0.0377  val_loss=0.0521  P=0.966 R=0.939 F1=0.952\n",
      "  ep 28 | train_loss=0.0357  val_loss=0.0504  P=0.948 R=0.962 F1=0.954\n",
      "  ep 29 | train_loss=0.0350  val_loss=0.0503  P=0.949 R=0.958 F1=0.953\n",
      "  ep 30 | train_loss=0.0347  val_loss=0.0504  P=0.957 R=0.947 F1=0.951\n",
      "  ep 31 | train_loss=0.0335  val_loss=0.0529  P=0.970 R=0.930 F1=0.949\n",
      "  ep 32 | train_loss=0.0323  val_loss=0.0518  P=0.939 R=0.973 F1=0.956\n",
      "  ep 33 | train_loss=0.0315  val_loss=0.0494  P=0.958 R=0.948 F1=0.953\n",
      "  ep 34 | train_loss=0.0307  val_loss=0.0507  P=0.963 R=0.946 F1=0.954\n",
      "  ep 35 | train_loss=0.0309  val_loss=0.0508  P=0.963 R=0.947 F1=0.955\n",
      "\n",
      "  best epoch: 32\n",
      "  best validation: P=0.939 R=0.973 F1=0.956 Loss=0.0518\n",
      "\n",
      "--- fold 2/5 ---\n",
      "train: 5517, val: 1379\n",
      "  ep 01 | train_loss=0.5497  val_loss=0.4343  P=0.000 R=0.000 F1=0.000\n",
      "  ep 02 | train_loss=0.2980  val_loss=0.2309  P=0.763 R=0.789 F1=0.775\n",
      "  ep 03 | train_loss=0.2044  val_loss=0.1851  P=0.781 R=0.892 F1=0.832\n",
      "  ep 04 | train_loss=0.1632  val_loss=0.1426  P=0.866 R=0.887 F1=0.876\n",
      "  ep 05 | train_loss=0.1340  val_loss=0.1233  P=0.858 R=0.919 F1=0.887\n",
      "  ep 06 | train_loss=0.1130  val_loss=0.1062  P=0.876 R=0.929 F1=0.902\n",
      "  ep 07 | train_loss=0.1023  val_loss=0.0960  P=0.901 R=0.927 F1=0.914\n",
      "  ep 08 | train_loss=0.0892  val_loss=0.0890  P=0.907 R=0.932 F1=0.919\n",
      "  ep 09 | train_loss=0.0838  val_loss=0.0830  P=0.926 R=0.920 F1=0.923\n",
      "  ep 10 | train_loss=0.0781  val_loss=0.0806  P=0.928 R=0.928 F1=0.928\n",
      "  ep 11 | train_loss=0.0766  val_loss=0.0754  P=0.941 R=0.925 F1=0.933\n",
      "  ep 12 | train_loss=0.0707  val_loss=0.0756  P=0.927 R=0.934 F1=0.930\n",
      "  ep 13 | train_loss=0.0666  val_loss=0.0741  P=0.946 R=0.918 F1=0.932\n",
      "  ep 14 | train_loss=0.0633  val_loss=0.0702  P=0.932 R=0.943 F1=0.938\n",
      "  ep 15 | train_loss=0.0610  val_loss=0.0676  P=0.943 R=0.931 F1=0.937\n",
      "  ep 16 | train_loss=0.0592  val_loss=0.0693  P=0.951 R=0.922 F1=0.936\n",
      "  ep 17 | train_loss=0.0575  val_loss=0.0691  P=0.942 R=0.925 F1=0.933\n",
      "  ep 18 | train_loss=0.0552  val_loss=0.0645  P=0.942 R=0.937 F1=0.939\n",
      "  ep 19 | train_loss=0.0526  val_loss=0.0675  P=0.915 R=0.962 F1=0.937\n",
      "  ep 20 | train_loss=0.0506  val_loss=0.0649  P=0.960 R=0.929 F1=0.944\n",
      "  ep 21 | train_loss=0.0496  val_loss=0.0608  P=0.936 R=0.951 F1=0.944\n",
      "  ep 22 | train_loss=0.0475  val_loss=0.0601  P=0.946 R=0.940 F1=0.943\n",
      "  ep 23 | train_loss=0.0453  val_loss=0.0630  P=0.948 R=0.934 F1=0.941\n",
      "  ep 24 | train_loss=0.0443  val_loss=0.0589  P=0.946 R=0.944 F1=0.945\n",
      "  ep 25 | train_loss=0.0418  val_loss=0.0648  P=0.951 R=0.926 F1=0.938\n",
      "  ep 26 | train_loss=0.0416  val_loss=0.0616  P=0.950 R=0.938 F1=0.944\n",
      "  ep 27 | train_loss=0.0397  val_loss=0.0633  P=0.953 R=0.932 F1=0.942\n",
      "  ep 28 | train_loss=0.0387  val_loss=0.0600  P=0.951 R=0.944 F1=0.948\n",
      "  ep 29 | train_loss=0.0379  val_loss=0.0587  P=0.949 R=0.942 F1=0.945\n",
      "  ep 30 | train_loss=0.0361  val_loss=0.0572  P=0.941 R=0.952 F1=0.947\n",
      "  ep 31 | train_loss=0.0365  val_loss=0.0595  P=0.947 R=0.943 F1=0.945\n",
      "  ep 32 | train_loss=0.0340  val_loss=0.0570  P=0.939 R=0.948 F1=0.943\n",
      "  ep 33 | train_loss=0.0335  val_loss=0.0603  P=0.942 R=0.948 F1=0.945\n",
      "  ep 34 | train_loss=0.0330  val_loss=0.0619  P=0.941 R=0.947 F1=0.944\n",
      "  ep 35 | train_loss=0.0321  val_loss=0.0596  P=0.950 R=0.943 F1=0.946\n",
      "\n",
      "  best epoch: 28\n",
      "  best validation: P=0.951 R=0.944 F1=0.948 Loss=0.0600\n",
      "\n",
      "--- fold 3/5 ---\n",
      "train: 5517, val: 1379\n",
      "  ep 01 | train_loss=0.5609  val_loss=0.4833  P=0.000 R=0.000 F1=0.000\n",
      "  ep 02 | train_loss=0.3382  val_loss=0.2264  P=0.776 R=0.750 F1=0.762\n",
      "  ep 03 | train_loss=0.2016  val_loss=0.1772  P=0.847 R=0.791 F1=0.817\n",
      "  ep 04 | train_loss=0.1577  val_loss=0.1369  P=0.892 R=0.844 F1=0.867\n",
      "  ep 05 | train_loss=0.1270  val_loss=0.1183  P=0.917 R=0.860 F1=0.887\n",
      "  ep 06 | train_loss=0.1086  val_loss=0.1052  P=0.917 R=0.879 F1=0.897\n",
      "  ep 07 | train_loss=0.0978  val_loss=0.0936  P=0.904 R=0.917 F1=0.910\n",
      "  ep 08 | train_loss=0.0889  val_loss=0.0861  P=0.911 R=0.927 F1=0.919\n",
      "  ep 09 | train_loss=0.0823  val_loss=0.0801  P=0.929 R=0.919 F1=0.924\n",
      "  ep 10 | train_loss=0.0757  val_loss=0.0784  P=0.917 R=0.941 F1=0.929\n",
      "  ep 11 | train_loss=0.0718  val_loss=0.0751  P=0.917 R=0.955 F1=0.936\n",
      "  ep 12 | train_loss=0.0683  val_loss=0.0705  P=0.931 R=0.943 F1=0.937\n",
      "  ep 13 | train_loss=0.0637  val_loss=0.0689  P=0.926 R=0.956 F1=0.940\n",
      "  ep 14 | train_loss=0.0600  val_loss=0.0651  P=0.939 R=0.945 F1=0.942\n",
      "  ep 15 | train_loss=0.0576  val_loss=0.0681  P=0.919 R=0.960 F1=0.939\n",
      "  ep 16 | train_loss=0.0555  val_loss=0.0635  P=0.929 R=0.960 F1=0.944\n",
      "  ep 17 | train_loss=0.0535  val_loss=0.0639  P=0.946 R=0.938 F1=0.942\n",
      "  ep 18 | train_loss=0.0501  val_loss=0.0617  P=0.937 R=0.954 F1=0.945\n",
      "  ep 19 | train_loss=0.0488  val_loss=0.0605  P=0.953 R=0.933 F1=0.943\n",
      "  ep 20 | train_loss=0.0470  val_loss=0.0598  P=0.951 R=0.943 F1=0.947\n",
      "  ep 21 | train_loss=0.0434  val_loss=0.0592  P=0.939 R=0.956 F1=0.948\n",
      "  ep 22 | train_loss=0.0439  val_loss=0.0619  P=0.927 R=0.967 F1=0.947\n",
      "  ep 23 | train_loss=0.0408  val_loss=0.0561  P=0.952 R=0.945 F1=0.948\n",
      "  ep 24 | train_loss=0.0393  val_loss=0.0584  P=0.949 R=0.948 F1=0.948\n",
      "  ep 25 | train_loss=0.0378  val_loss=0.0562  P=0.950 R=0.956 F1=0.952\n",
      "  ep 26 | train_loss=0.0357  val_loss=0.0548  P=0.957 R=0.950 F1=0.954\n",
      "  ep 27 | train_loss=0.0363  val_loss=0.0603  P=0.930 R=0.972 F1=0.950\n",
      "  ep 28 | train_loss=0.0341  val_loss=0.0539  P=0.949 R=0.961 F1=0.954\n",
      "  ep 29 | train_loss=0.0323  val_loss=0.0557  P=0.959 R=0.949 F1=0.954\n",
      "  ep 30 | train_loss=0.0323  val_loss=0.0558  P=0.944 R=0.966 F1=0.955\n",
      "  ep 31 | train_loss=0.0317  val_loss=0.0540  P=0.953 R=0.957 F1=0.955\n",
      "  ep 32 | train_loss=0.0313  val_loss=0.0553  P=0.955 R=0.954 F1=0.954\n",
      "  ep 33 | train_loss=0.0281  val_loss=0.0557  P=0.959 R=0.948 F1=0.953\n",
      "  ep 34 | train_loss=0.0290  val_loss=0.0555  P=0.948 R=0.959 F1=0.953\n",
      "  ep 35 | train_loss=0.0287  val_loss=0.0548  P=0.949 R=0.966 F1=0.957\n",
      "\n",
      "  best epoch: 35\n",
      "  best validation: P=0.949 R=0.966 F1=0.957 Loss=0.0548\n",
      "\n",
      "--- fold 4/5 ---\n",
      "train: 5517, val: 1379\n",
      "  ep 01 | train_loss=0.5393  val_loss=0.4457  P=0.000 R=0.000 F1=0.000\n",
      "  ep 02 | train_loss=0.3022  val_loss=0.2207  P=0.779 R=0.765 F1=0.771\n",
      "  ep 03 | train_loss=0.2027  val_loss=0.1798  P=0.818 R=0.857 F1=0.837\n",
      "  ep 04 | train_loss=0.1635  val_loss=0.1456  P=0.868 R=0.866 F1=0.867\n",
      "  ep 05 | train_loss=0.1335  val_loss=0.1193  P=0.891 R=0.893 F1=0.891\n",
      "  ep 06 | train_loss=0.1137  val_loss=0.1092  P=0.880 R=0.936 F1=0.907\n",
      "  ep 07 | train_loss=0.0985  val_loss=0.0938  P=0.897 R=0.930 F1=0.913\n",
      "  ep 08 | train_loss=0.0902  val_loss=0.0888  P=0.896 R=0.945 F1=0.920\n",
      "  ep 09 | train_loss=0.0813  val_loss=0.0859  P=0.929 R=0.909 F1=0.918\n",
      "  ep 10 | train_loss=0.0760  val_loss=0.0768  P=0.929 R=0.927 F1=0.928\n",
      "  ep 11 | train_loss=0.0701  val_loss=0.0747  P=0.924 R=0.939 F1=0.931\n",
      "  ep 12 | train_loss=0.0656  val_loss=0.0707  P=0.922 R=0.947 F1=0.934\n",
      "  ep 13 | train_loss=0.0633  val_loss=0.0733  P=0.949 R=0.915 F1=0.932\n",
      "  ep 14 | train_loss=0.0601  val_loss=0.0690  P=0.934 R=0.939 F1=0.936\n",
      "  ep 15 | train_loss=0.0568  val_loss=0.0673  P=0.928 R=0.949 F1=0.938\n",
      "  ep 16 | train_loss=0.0537  val_loss=0.0634  P=0.932 R=0.950 F1=0.941\n",
      "  ep 17 | train_loss=0.0521  val_loss=0.0618  P=0.935 R=0.947 F1=0.941\n",
      "  ep 18 | train_loss=0.0499  val_loss=0.0665  P=0.919 R=0.965 F1=0.941\n",
      "  ep 19 | train_loss=0.0482  val_loss=0.0615  P=0.941 R=0.945 F1=0.943\n",
      "  ep 20 | train_loss=0.0479  val_loss=0.0611  P=0.940 R=0.952 F1=0.946\n",
      "  ep 21 | train_loss=0.0452  val_loss=0.0600  P=0.944 R=0.948 F1=0.946\n",
      "  ep 22 | train_loss=0.0435  val_loss=0.0720  P=0.903 R=0.973 F1=0.936\n",
      "  ep 23 | train_loss=0.0424  val_loss=0.0603  P=0.935 R=0.958 F1=0.947\n",
      "  ep 24 | train_loss=0.0412  val_loss=0.0617  P=0.953 R=0.934 F1=0.943\n",
      "  ep 25 | train_loss=0.0398  val_loss=0.0684  P=0.914 R=0.969 F1=0.941\n",
      "  ep 26 | train_loss=0.0394  val_loss=0.0601  P=0.933 R=0.961 F1=0.947\n",
      "  ep 27 | train_loss=0.0381  val_loss=0.0619  P=0.928 R=0.963 F1=0.945\n",
      "  ep 28 | train_loss=0.0364  val_loss=0.0573  P=0.947 R=0.953 F1=0.950\n",
      "  ep 29 | train_loss=0.0342  val_loss=0.0598  P=0.942 R=0.949 F1=0.945\n",
      "  ep 30 | train_loss=0.0343  val_loss=0.0641  P=0.926 R=0.968 F1=0.946\n",
      "  ep 31 | train_loss=0.0329  val_loss=0.0585  P=0.950 R=0.944 F1=0.947\n",
      "  ep 32 | train_loss=0.0333  val_loss=0.0596  P=0.937 R=0.961 F1=0.949\n",
      "  ep 33 | train_loss=0.0309  val_loss=0.0615  P=0.938 R=0.956 F1=0.947\n",
      "  ep 34 | train_loss=0.0302  val_loss=0.0623  P=0.953 R=0.934 F1=0.943\n",
      "  ep 35 | train_loss=0.0300  val_loss=0.0621  P=0.937 R=0.960 F1=0.948\n",
      "\n",
      "  best epoch: 28\n",
      "  best validation: P=0.947 R=0.953 F1=0.950 Loss=0.0573\n",
      "\n",
      "--- fold 5/5 ---\n",
      "train: 5517, val: 1379\n",
      "  ep 01 | train_loss=0.5516  val_loss=0.4361  P=0.000 R=0.000 F1=0.000\n",
      "  ep 02 | train_loss=0.2941  val_loss=0.2094  P=0.792 R=0.831 F1=0.811\n",
      "  ep 03 | train_loss=0.1984  val_loss=0.1720  P=0.820 R=0.858 F1=0.838\n",
      "  ep 04 | train_loss=0.1644  val_loss=0.1469  P=0.871 R=0.841 F1=0.855\n",
      "  ep 05 | train_loss=0.1395  val_loss=0.1299  P=0.853 R=0.907 F1=0.879\n",
      "  ep 06 | train_loss=0.1207  val_loss=0.1169  P=0.897 R=0.870 F1=0.883\n",
      "  ep 07 | train_loss=0.1076  val_loss=0.1078  P=0.866 R=0.926 F1=0.895\n",
      "  ep 08 | train_loss=0.0967  val_loss=0.0979  P=0.912 R=0.904 F1=0.908\n",
      "  ep 09 | train_loss=0.0889  val_loss=0.0925  P=0.907 R=0.917 F1=0.911\n",
      "  ep 10 | train_loss=0.0819  val_loss=0.0894  P=0.924 R=0.908 F1=0.916\n",
      "  ep 11 | train_loss=0.0774  val_loss=0.0853  P=0.920 R=0.912 F1=0.916\n",
      "  ep 12 | train_loss=0.0739  val_loss=0.0801  P=0.921 R=0.927 F1=0.924\n",
      "  ep 13 | train_loss=0.0674  val_loss=0.0795  P=0.928 R=0.920 F1=0.924\n",
      "  ep 14 | train_loss=0.0647  val_loss=0.0792  P=0.924 R=0.932 F1=0.928\n",
      "  ep 15 | train_loss=0.0621  val_loss=0.0761  P=0.912 R=0.945 F1=0.928\n",
      "  ep 16 | train_loss=0.0577  val_loss=0.0724  P=0.920 R=0.946 F1=0.933\n",
      "  ep 17 | train_loss=0.0558  val_loss=0.0726  P=0.910 R=0.955 F1=0.932\n",
      "  ep 18 | train_loss=0.0537  val_loss=0.0720  P=0.915 R=0.955 F1=0.934\n",
      "  ep 19 | train_loss=0.0524  val_loss=0.0680  P=0.934 R=0.943 F1=0.938\n",
      "  ep 20 | train_loss=0.0490  val_loss=0.0688  P=0.936 R=0.938 F1=0.937\n",
      "  ep 21 | train_loss=0.0471  val_loss=0.0692  P=0.930 R=0.943 F1=0.936\n",
      "  ep 22 | train_loss=0.0463  val_loss=0.0685  P=0.942 R=0.932 F1=0.937\n",
      "  ep 23 | train_loss=0.0444  val_loss=0.0649  P=0.940 R=0.942 F1=0.941\n",
      "  ep 24 | train_loss=0.0420  val_loss=0.0676  P=0.946 R=0.932 F1=0.939\n",
      "  ep 25 | train_loss=0.0425  val_loss=0.0657  P=0.936 R=0.946 F1=0.941\n",
      "  ep 26 | train_loss=0.0400  val_loss=0.0668  P=0.944 R=0.936 F1=0.940\n",
      "  ep 27 | train_loss=0.0387  val_loss=0.0648  P=0.947 R=0.939 F1=0.943\n",
      "  ep 28 | train_loss=0.0377  val_loss=0.0650  P=0.939 R=0.948 F1=0.943\n",
      "  ep 29 | train_loss=0.0356  val_loss=0.0661  P=0.943 R=0.937 F1=0.940\n",
      "  ep 30 | train_loss=0.0348  val_loss=0.0664  P=0.952 R=0.926 F1=0.939\n",
      "  ep 31 | train_loss=0.0337  val_loss=0.0658  P=0.935 R=0.949 F1=0.942\n",
      "  ep 32 | train_loss=0.0322  val_loss=0.0747  P=0.960 R=0.912 F1=0.935\n",
      "  ep 33 | train_loss=0.0329  val_loss=0.0632  P=0.951 R=0.940 F1=0.946\n",
      "  ep 34 | train_loss=0.0315  val_loss=0.0635  P=0.940 R=0.951 F1=0.946\n",
      "  ep 35 | train_loss=0.0303  val_loss=0.0708  P=0.955 R=0.925 F1=0.940\n",
      "\n",
      "  best epoch: 33\n",
      "  best validation: P=0.951 R=0.940 F1=0.946 Loss=0.0632\n",
      "\n",
      "================================================================================\n",
      "CV SUMMARY\n",
      "================================================================================\n",
      "  fold 1: P=0.939, R=0.973, F1=0.956, Loss=0.0518\n",
      "  fold 2: P=0.951, R=0.944, F1=0.948, Loss=0.0600\n",
      "  fold 3: P=0.949, R=0.966, F1=0.957, Loss=0.0548\n",
      "  fold 4: P=0.947, R=0.953, F1=0.950, Loss=0.0573\n",
      "  fold 5: P=0.951, R=0.940, F1=0.946, Loss=0.0632\n",
      "\n",
      "mean +/- std over 5 folds:\n",
      "  precision: 0.948 +/- 0.004\n",
      "  recall:    0.955 +/- 0.013\n",
      "  F1:        0.951 +/- 0.005\n",
      "  loss:      0.0574 +/- 0.0040\n",
      "\n",
      "best fold: 3 (F1=0.957)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "avg boundary F1: 0.951 +/- 0.005\n",
      "avg precision: 0.948 +/- 0.004\n",
      "avg recall: 0.955 +/- 0.013\n"
     ]
    }
   ],
   "source": [
    "# Run k-fold cross-validation\n",
    "kfold_results = run_kfold_cross_validation(\n",
    "    df=gold_df,\n",
    "    n_folds=5,\n",
    "    emb_dim=EMB_DIM,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    lr=LR,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    random_state=42,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"\\navg boundary F1: {kfold_results['mean_metrics']['boundary_f1']:.3f} +/- {kfold_results['std_metrics']['boundary_f1']:.3f}\")\n",
    "print(f\"avg precision: {kfold_results['mean_metrics']['boundary_precision']:.3f} +/- {kfold_results['std_metrics']['boundary_precision']:.3f}\")\n",
    "print(f\"avg recall: {kfold_results['mean_metrics']['boundary_recall']:.3f} +/- {kfold_results['std_metrics']['boundary_recall']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e894a77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boundary_positions_from_labels(labels):\n",
    "    \"\"\"Convert per-char boundary labels to boundary positions.\"\"\"\n",
    "    if not labels:\n",
    "        return set()\n",
    "    L = len(labels)\n",
    "    return {i for i in range(min(L-1, len(labels))) if labels[i] == 1}\n",
    "\n",
    "def boundary_positions_from_segments(segments):\n",
    "    \"\"\"Convert a list of segments into boundary positions.\"\"\"\n",
    "    pos = set()\n",
    "    acc = 0\n",
    "    for k, seg in enumerate(segments):\n",
    "        acc += len(seg)\n",
    "        if k < len(segments) - 1:\n",
    "            pos.add(acc - 1)\n",
    "    return pos\n",
    "\n",
    "def prf_from_sets(pred_set, gold_set):\n",
    "    \"\"\"Compute precision, recall, F1 from boundary sets.\"\"\"\n",
    "    tp = len(pred_set & gold_set)\n",
    "    fp = len(pred_set - gold_set)\n",
    "    fn = len(gold_set - pred_set)\n",
    "    \n",
    "    if tp + fp == 0:\n",
    "        precision = 1.0 if tp + fp + fn == 0 else 0.0\n",
    "    else:\n",
    "        precision = tp / (tp + fp)\n",
    "    \n",
    "    if tp + fn == 0:\n",
    "        recall = 1.0 if tp + fp + fn == 0 else 0.0\n",
    "    else:\n",
    "        recall = tp / (tp + fn)\n",
    "    \n",
    "    if precision + recall == 0:\n",
    "        f1 = 1.0 if tp + fp + fn == 0 else 0.0\n",
    "    else:\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "    \n",
    "    return tp, fp, fn, precision, recall, f1\n",
    "\n",
    "def best_variant_metrics(pred_boundaries, gold_variants):\n",
    "    \"\"\"Among multiple gold segmentations, pick the one that maximizes F1.\"\"\"\n",
    "    best = None\n",
    "    for variant in gold_variants:\n",
    "        gold_b = boundary_positions_from_segments(variant)\n",
    "        tp, fp, fn, P, R, F1 = prf_from_sets(pred_boundaries, gold_b)\n",
    "        key = (F1, tp, -fn, -fp)\n",
    "        if (best is None) or (key > best[0]):\n",
    "            best = (key, gold_b, tp, fp, fn, P, R, F1)\n",
    "    \n",
    "    if best is None:\n",
    "        gold_b = set()\n",
    "        tp, fp, fn, P, R, F1 = prf_from_sets(pred_boundaries, gold_b)\n",
    "        return gold_b, tp, fp, fn, P, R, F1\n",
    "    \n",
    "    _, gold_b, tp, fp, fn, P, R, F1 = best\n",
    "    return gold_b, tp, fp, fn, P, R, F1\n",
    "\n",
    "def is_correct_prediction(predicted, gold_variants):\n",
    "    \"\"\"Check if predicted segmentation matches any gold variant.\"\"\"\n",
    "    return any(predicted == variant for variant in gold_variants)\n",
    "\n",
    "def normalize_gold_variants(gold_variants):\n",
    "    \"\"\"Convert gold_variants to proper list format.\"\"\"\n",
    "    if gold_variants is None:\n",
    "        return []\n",
    "    if isinstance(gold_variants, np.ndarray):\n",
    "        gold_variants = gold_variants.tolist()\n",
    "    if isinstance(gold_variants, list):\n",
    "        normalized = []\n",
    "        for variant in gold_variants:\n",
    "            if isinstance(variant, np.ndarray):\n",
    "                normalized.append(variant.tolist())\n",
    "            elif isinstance(variant, list):\n",
    "                normalized.append([item.tolist() if isinstance(item, np.ndarray) else item for item in variant])\n",
    "            else:\n",
    "                normalized.append(variant)\n",
    "        return normalized\n",
    "    return []\n",
    "\n",
    "def split_count_metrics(predicted_segments, gold_variants):\n",
    "    \"\"\"Compute split-count accuracy variants.\"\"\"\n",
    "    pred_count = len(predicted_segments)\n",
    "    gold_counts = [len(gold) for gold in gold_variants]\n",
    "\n",
    "    exact = any(pred_count == g for g in gold_counts)\n",
    "    plus1 = any(pred_count == g + 1 for g in gold_counts)\n",
    "    minus1 = any(pred_count == g - 1 for g in gold_counts)\n",
    "    pm1 = any(abs(pred_count - g) <= 1 for g in gold_counts)\n",
    "\n",
    "    return {\"Exact\": exact, \"+1\": plus1, \"-1\": minus1, \"Â±1\": pm1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9820f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading test data...\n",
      "loaded 913 test examples\n",
      "loaded checkpoint from models_segmenter-old\\6112ccdaef2e0c54\n",
      "model loaded for evaluation\n",
      "exact segmentation accuracy: 0.5268\n",
      "boundary metrics:\n",
      "  micro  - P: 0.7963  R: 0.8397  F1: 0.8174\n",
      "  macro  - P: 0.8072  R: 0.8279  F1: 0.7993\n",
      "\n",
      "=== split-count metrics ===\n",
      "split-count (exact):          0.6440\n",
      "split-count (+1):             0.1906\n",
      "split-count (âˆ’1):             0.1391\n",
      "split-count (Â±1):             0.9726\n",
      "overlap (exact âˆ© split):      0.5268\n",
      "\n",
      "evaluation results saved to data\\bilstm_eval_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Load test data\n",
    "print(\"loading test data...\")\n",
    "df = pd.read_parquet(os.path.join(DATA_FOLDER, \"cleaned_data_df.parquet\"))\n",
    "print(f\"loaded {len(df):,} test examples\")\n",
    "\n",
    "# Load trained model\n",
    "EMB_DIM = 16\n",
    "HIDDEN_SIZE = 32\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.3\n",
    "EPOCHS = 35\n",
    "BATCH_SIZE = 64\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "model_id = generate_model_id(EMB_DIM, HIDDEN_SIZE, NUM_LAYERS, DROPOUT, EPOCHS, BATCH_SIZE, LR, WEIGHT_DECAY)\n",
    "\n",
    "loaded = load_model_checkpoint(model_id, models_folder=MODELS_FOLDER)\n",
    "if loaded is None:\n",
    "    raise FileNotFoundError(f\"model checkpoint not found (model_id: {model_id})\")\n",
    "\n",
    "stoi, itos = loaded[\"stoi\"], loaded[\"itos\"]\n",
    "model = BiLSTMBoundary(vocab_size=len(itos), emb_dim=EMB_DIM, hidden_size=HIDDEN_SIZE,\n",
    "                       num_layers=NUM_LAYERS, dropout=DROPOUT).to(device)\n",
    "model.load_state_dict(loaded[\"model_state\"])\n",
    "model.eval()\n",
    "print(\"model loaded for evaluation\")\n",
    "\n",
    "# Evaluate on test set\n",
    "all_words = df[\"Word\"].tolist()\n",
    "all_boundaries = predict_boundaries(all_words, model, stoi, threshold=0.5)\n",
    "\n",
    "results = []\n",
    "micro_tp = micro_fp = micro_fn = 0\n",
    "macro_Ps, macro_Rs, macro_F1s = [], [], []\n",
    "\n",
    "for word, gold_variants, boundary_labels in zip(all_words, df[\"Gold\"], all_boundaries):\n",
    "    gold_variants = normalize_gold_variants(gold_variants)\n",
    "\n",
    "    pred_b = boundary_positions_from_labels(boundary_labels)\n",
    "    gold_b, tp, fp, fn, P, R, F1 = best_variant_metrics(pred_b, gold_variants)\n",
    "\n",
    "    predicted_segments = apply_boundaries(word, boundary_labels)\n",
    "    correct = is_correct_prediction(predicted_segments, gold_variants)\n",
    "\n",
    "    results.append({\n",
    "        \"Word\": word,\n",
    "        \"Prediction\": predicted_segments,\n",
    "        \"Gold\": gold_variants,\n",
    "        \"PredBoundaries\": sorted(pred_b),\n",
    "        \"GoldBoundaries(Chosen)\": sorted(gold_b),\n",
    "        \"TP\": tp, \"FP\": fp, \"FN\": fn,\n",
    "        \"P_word\": P, \"R_word\": R, \"F1_word\": F1,\n",
    "        \"CorrectExactSeg\": correct\n",
    "    })\n",
    "\n",
    "    micro_tp += tp\n",
    "    micro_fp += fp\n",
    "    micro_fn += fn\n",
    "    macro_Ps.append(P)\n",
    "    macro_Rs.append(R)\n",
    "    macro_F1s.append(F1)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "accuracy = results_df[\"CorrectExactSeg\"].mean()\n",
    "\n",
    "if micro_tp + micro_fp == 0:\n",
    "    P_micro = 1.0 if micro_tp + micro_fn == 0 else 0.0\n",
    "else:\n",
    "    P_micro = micro_tp / (micro_tp + micro_fp)\n",
    "\n",
    "if micro_tp + micro_fn == 0:\n",
    "    R_micro = 1.0 if micro_tp + micro_fp == 0 else 0.0\n",
    "else:\n",
    "    R_micro = micro_tp / (micro_tp + micro_fn)\n",
    "\n",
    "if P_micro + R_micro == 0:\n",
    "    F1_micro = 1.0 if (micro_tp + micro_fp + micro_fn) == 0 else 0.0\n",
    "else:\n",
    "    F1_micro = 2 * P_micro * R_micro / (P_micro + R_micro)\n",
    "\n",
    "P_macro = float(pd.Series(macro_Ps).mean()) if macro_Ps else 0.0\n",
    "R_macro = float(pd.Series(macro_Rs).mean()) if macro_Rs else 0.0\n",
    "F1_macro = float(pd.Series(macro_F1s).mean()) if macro_F1s else 0.0\n",
    "\n",
    "print(f\"exact segmentation accuracy: {accuracy:.4f}\")\n",
    "print(\"boundary metrics:\")\n",
    "print(f\"  micro  - P: {P_micro:.4f}  R: {R_micro:.4f}  F1: {F1_micro:.4f}\")\n",
    "print(f\"  macro  - P: {P_macro:.4f}  R: {R_macro:.4f}  F1: {F1_macro:.4f}\")\n",
    "\n",
    "# Split-count metrics\n",
    "split_exact_flags = []\n",
    "split_plus1_flags = []\n",
    "split_minus1_flags = []\n",
    "split_pm1_flags = []\n",
    "overlap_flags = []\n",
    "\n",
    "for rec in results:\n",
    "    predicted_segments = rec[\"Prediction\"]\n",
    "    gold_variants = rec[\"Gold\"]\n",
    "    gold_variants = normalize_gold_variants(gold_variants)\n",
    "\n",
    "    split_metrics = split_count_metrics(predicted_segments, gold_variants)\n",
    "    rec[\"CorrectSplitCount\"] = split_metrics[\"Exact\"]\n",
    "    rec[\"SplitCount+1\"] = split_metrics[\"+1\"]\n",
    "    rec[\"SplitCount-1\"] = split_metrics[\"-1\"]\n",
    "    rec[\"SplitCountÂ±1\"] = split_metrics[\"Â±1\"]\n",
    "\n",
    "    overlap = rec[\"CorrectExactSeg\"] and split_metrics[\"Exact\"]\n",
    "    rec[\"OverlapExactAndSplit\"] = overlap\n",
    "\n",
    "    split_exact_flags.append(split_metrics[\"Exact\"])\n",
    "    split_plus1_flags.append(split_metrics[\"+1\"])\n",
    "    split_minus1_flags.append(split_metrics[\"-1\"])\n",
    "    split_pm1_flags.append(split_metrics[\"Â±1\"])\n",
    "    overlap_flags.append(overlap)\n",
    "\n",
    "split_exact_acc = np.mean(split_exact_flags)\n",
    "split_plus1_acc = np.mean(split_plus1_flags)\n",
    "split_minus1_acc = np.mean(split_minus1_flags)\n",
    "split_pm1_acc = np.mean(split_pm1_flags)\n",
    "overlap_accuracy = np.mean(overlap_flags)\n",
    "\n",
    "print(\"\\n=== split-count metrics ===\")\n",
    "print(f\"split-count (exact):          {split_exact_acc:.4f}\")\n",
    "print(f\"split-count (+1):             {split_plus1_acc:.4f}\")\n",
    "print(f\"split-count (âˆ’1):             {split_minus1_acc:.4f}\")\n",
    "print(f\"split-count (Â±1):             {split_pm1_acc:.4f}\")\n",
    "print(f\"overlap (exact âˆ© split):      {overlap_accuracy:.4f}\")\n",
    "\n",
    "# Save results\n",
    "results_df = pd.DataFrame(results)\n",
    "results_output_path = os.path.join(DATA_FOLDER, \"bilstm_eval_results.csv\")\n",
    "results_df.to_csv(results_output_path, index=False)\n",
    "print(f\"\\nevaluation results saved to {results_output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
