{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7665ea90",
   "metadata": {},
   "source": [
    "# Synthetic Data Generation for Quechua Morphology Parser\n",
    "\n",
    "Generates synthetic morphological segmentation data using GPT models. Part 1: Data analysis and gold standard creation. Part 2: Synthetic data generation with few-shot learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42071696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import os\n",
    "import ast\n",
    "import time\n",
    "import random\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "# Data handling\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "\n",
    "# Unicode normalization\n",
    "import unicodedata\n",
    "from ftfy import fix_text\n",
    "\n",
    "# API\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from openai import RateLimitError, APIError, APITimeoutError, APIConnectionError\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79eba16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATA_FOLDER = \"data\"\n",
    "\n",
    "# Input files\n",
    "CORPUS_FILE = os.path.join(DATA_FOLDER, \"qu_merged_dump.txt\")\n",
    "GOLD_DF_FILE = os.path.join(DATA_FOLDER, \"Sue_kalt.parquet\")\n",
    "CLEANED_DF_FILE = os.path.join(DATA_FOLDER, \"cleaned_data_df.csv\")\n",
    "\n",
    "# Output files\n",
    "GOLD_OUTPUT_FILE = os.path.join(DATA_FOLDER, \"gold_df_common_words.csv\")\n",
    "CLEANED_OUTPUT_FILE = os.path.join(DATA_FOLDER, \"cleaned_data_df_common_words.csv\")\n",
    "COMMON_WORDS_OUTPUT_FILE = os.path.join(DATA_FOLDER, \"word_analysis_gold.csv\")\n",
    "OUTPUT_FILE_GPT4O = os.path.join(DATA_FOLDER, \"gpt4o_synthetic_segmentations.csv\")\n",
    "OUTPUT_FILE_GPT5MINI = os.path.join(DATA_FOLDER, \"gpt5mini_synthetic_segmentations.csv\")\n",
    "GOLD_DATA_FILE = os.path.join(DATA_FOLDER, \"word_analysis_gold.csv\")\n",
    "\n",
    "# Analysis parameters\n",
    "RARE_WORD_RANK_THRESHOLD = 100000\n",
    "LOWERCASE = True\n",
    "KEEP_APOSTROPHES = True\n",
    "\n",
    "# API parameters\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "MODELS_TO_PROCESS = [\"gpt-4o\", \"gpt-5-mini\"]\n",
    "NUM_FEW_SHOT_EXAMPLES = 37\n",
    "WORDS_TO_PROCESS_LIMIT = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00659bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quechua graphemes\n",
    "graphemes = [\n",
    "    \"ch\",\"ll\",\"rr\",\"tr\",\"kw\",\"ph\",\n",
    "    \"a\",\"b\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\",\"k\",\"l\",\"m\",\"n\",\"ñ\",\"o\",\"p\",\"q\",\n",
    "    \"r\",\"s\",\"t\",\"u\",\"v\",\"w\",\"x\",\"y\"\n",
    "]\n",
    "\n",
    "GRAPHEMES_BY_LEN = sorted(graphemes, key=len, reverse=True)\n",
    "SINGLE_CHARS = {g for g in graphemes if len(g) == 1}\n",
    "\n",
    "# Unicode normalization helper\n",
    "CTRL_RE = re.compile(r'[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F\\x7F-\\x9F]')\n",
    "def norm_unicode(x, form=\"NFC\"):\n",
    "    \"\"\"Normalize unicode text.\"\"\"\n",
    "    if pd.isna(x):\n",
    "        return x\n",
    "    s = x.decode(\"utf-8\", \"replace\") if isinstance(x, (bytes, bytearray)) else str(x)\n",
    "    s = fix_text(s)\n",
    "    s = CTRL_RE.sub('', s)\n",
    "    s = unicodedata.normalize(form, s)\n",
    "    s = s.replace('\\u00A0', ' ')\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "    return s\n",
    "\n",
    "def tokenize_graphemes(word: str):\n",
    "    \"\"\"Greedy longest-match tokenizer over allowed graphemes.\"\"\"\n",
    "    if not isinstance(word, str):\n",
    "        return None\n",
    "    w = word.strip()\n",
    "    if LOWERCASE:\n",
    "        w = w.lower()\n",
    "\n",
    "    if \"'\" in w or \"'\" in w:\n",
    "        return None\n",
    "\n",
    "    i = 0\n",
    "    toks = []\n",
    "    n = len(w)\n",
    "    while i < n:\n",
    "        matched = False\n",
    "        for g in GRAPHEMES_BY_LEN:\n",
    "            L = len(g)\n",
    "            if i + L <= n and w[i:i+L] == g:\n",
    "                toks.append(g)\n",
    "                i += L\n",
    "                matched = True\n",
    "                break\n",
    "        if not matched:\n",
    "            return None\n",
    "    return toks\n",
    "\n",
    "def is_valid_grapheme_word(word: str) -> bool:\n",
    "    \"\"\"Check if word can be fully segmented into allowed graphemes.\"\"\"\n",
    "    toks = tokenize_graphemes(word)\n",
    "    return toks is not None\n",
    "\n",
    "def first_four_graphemes_root(word: str) -> str:\n",
    "    \"\"\"Corpus root: concatenation of first 4 graphemes.\"\"\"\n",
    "    toks = tokenize_graphemes(word)\n",
    "    if toks is None or len(toks) == 0:\n",
    "        return ''\n",
    "    root = ''.join(toks[:4])\n",
    "    return root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c54f8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_first_segment(row, prefer_list_col=\"Morph_split\", fallback_str_col=\"Morph_split_str\"):\n",
    "    \"\"\"Extract first segment (root) from row, handling various formats.\"\"\"\n",
    "    if prefer_list_col in row:\n",
    "        val = row[prefer_list_col]\n",
    "        if isinstance(val, list) and len(val) > 0:\n",
    "            return str(val[0]).strip()\n",
    "        if isinstance(val, str):\n",
    "            s = val.strip()\n",
    "            try:\n",
    "                parsed = ast.literal_eval(s)\n",
    "                if isinstance(parsed, list) and len(parsed) > 0:\n",
    "                    return str(parsed[0]).strip()\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    if fallback_str_col in row:\n",
    "        s = row[fallback_str_col]\n",
    "        if isinstance(s, str) and s.strip():\n",
    "            return s.strip().split()[0]\n",
    "\n",
    "    return ''\n",
    "\n",
    "def robust_first_segment(row, prefer_list_col=\"Morph_split\", fallback_str_col=\"Morph_split_str\", alt_morph_col=\"morph\"):\n",
    "    \"\"\"Extract root as first segment, with multiple fallbacks.\"\"\"\n",
    "    if prefer_list_col in row:\n",
    "        val = row[prefer_list_col]\n",
    "        if isinstance(val, list) and val:\n",
    "            return str(val[0]).strip()\n",
    "        if isinstance(val, str):\n",
    "            s = val.strip()\n",
    "            try:\n",
    "                parsed = ast.literal_eval(s)\n",
    "                if isinstance(parsed, list) and parsed:\n",
    "                    return str(parsed[0]).strip()\n",
    "            except Exception:\n",
    "                if s:\n",
    "                    return s.split()[0].strip()\n",
    "\n",
    "    if fallback_str_col in row:\n",
    "        s = row[fallback_str_col]\n",
    "        if isinstance(s, str) and s.strip():\n",
    "            return s.strip().split()[0]\n",
    "\n",
    "    if alt_morph_col in row:\n",
    "        m = row[alt_morph_col]\n",
    "        if isinstance(m, str) and m.strip():\n",
    "            return m.replace('-', ' ').strip().split()[0]\n",
    "\n",
    "    return ''\n",
    "\n",
    "def process_corpus(file_path):\n",
    "    \"\"\"Read corpus, tokenize, and calculate word frequencies (valid grapheme words only).\"\"\"\n",
    "    print(f\"processing corpus file: {file_path}...\")\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"corpus file not found at: {file_path}\")\n",
    "\n",
    "    TOKEN_RE = re.compile(r\"[^\\W\\d_]+(?:[''][^\\W\\d_]+)?\", flags=re.UNICODE) if KEEP_APOSTROPHES \\\n",
    "                else re.compile(r\"[^\\W\\d_]+\", flags=re.UNICODE)\n",
    "\n",
    "    def iter_valid_tokens_from_file(path):\n",
    "        with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            for line in f:\n",
    "                if LOWERCASE:\n",
    "                    line = line.lower()\n",
    "                for m in TOKEN_RE.finditer(line):\n",
    "                    tok = m.group(0)\n",
    "                    if is_valid_grapheme_word(tok):\n",
    "                        yield tok\n",
    "\n",
    "    freq = Counter(iter_valid_tokens_from_file(file_path))\n",
    "\n",
    "    if not freq:\n",
    "        print(\"hmm, got zero tokens after grapheme filtering\")\n",
    "        return {}, {}\n",
    "        \n",
    "    print(f\"corpus processed. total unique valid grapheme-words: {len(freq):,}\")\n",
    "\n",
    "    sorted_words = [word for word, count in freq.most_common()]\n",
    "    rank_map = {word: i + 1 for i, word in enumerate(sorted_words)}\n",
    "    \n",
    "    return dict(freq), rank_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76650ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1: Data Analysis and Gold Standard Creation\n",
    "\n",
    "# Step 1: Process corpus\n",
    "corpus_freq, corpus_rank = process_corpus(CORPUS_FILE)\n",
    "\n",
    "# Step 2: Load dataframes\n",
    "gold_df = pd.read_parquet(GOLD_DF_FILE)\n",
    "gold_df['Word'] = gold_df['word']\n",
    "gold_df['morph'] = gold_df['morph'].str.replace('-', ' ')\n",
    "gold_df['Morph_split_str'] = gold_df['morph']\n",
    "gold_df['Morph_split'] = gold_df['morph'].str.split(' ')\n",
    "gold_df = gold_df[['Word', 'Morph_split', 'Morph_split_str']]\n",
    "\n",
    "cleaned_df = pd.read_csv(CLEANED_DF_FILE, encoding='windows-1252')\n",
    "\n",
    "# Extract word sets\n",
    "gold_words = set(gold_df['Word'].dropna().unique())\n",
    "cleaned_words = set(cleaned_df['Word'].dropna().unique())\n",
    "corpus_words = set(corpus_freq.keys())\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ANALYSIS RESULTS\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "# Step 3: Corpus coverage analysis\n",
    "print(\"--- 1. corpus coverage analysis (surface forms) ---\")\n",
    "gold_in_corpus = gold_words.intersection(corpus_words)\n",
    "coverage_percentage = (len(gold_in_corpus) / len(gold_words)) * 100 if gold_words else 0\n",
    "print(f\"[{GOLD_DF_FILE}]: found {len(gold_in_corpus):,} / {len(gold_words):,} words in corpus ({coverage_percentage:.2f}% coverage)\\n\")\n",
    "\n",
    "cleaned_in_corpus = cleaned_words.intersection(corpus_words)\n",
    "coverage_percentage = (len(cleaned_in_corpus) / len(cleaned_words)) * 100 if cleaned_words else 0\n",
    "print(f\"[{CLEANED_DF_FILE}]: found {len(cleaned_in_corpus):,} / {len(cleaned_words):,} words in corpus ({coverage_percentage:.2f}% coverage)\\n\")\n",
    "\n",
    "# Step 4: Dataset incongruity analysis\n",
    "print(\"--- 2. dataset incongruity analysis (surface forms) ---\")\n",
    "words_in_common = gold_words.intersection(cleaned_words)\n",
    "words_only_in_gold = gold_words.difference(cleaned_words)\n",
    "words_only_in_cleaned = cleaned_words.difference(gold_words)\n",
    "common_and_in_corpus = words_in_common.intersection(corpus_words)\n",
    "\n",
    "print(f\"words common to both datasets: {len(words_in_common):,}\")\n",
    "print(f\"words in the corpus and common to both datasets: {len(common_and_in_corpus):,}\")\n",
    "print(f\"words only in '{GOLD_DF_FILE}': {len(words_only_in_gold):,}\")\n",
    "print(f\"words only in '{CLEANED_DF_FILE}': {len(words_only_in_cleaned):,}\\n\")\n",
    "\n",
    "# Step 4b: Root-level analysis\n",
    "print(\"--- 2b. root-level analysis ---\")\n",
    "\n",
    "corpus_roots = set()\n",
    "for w in corpus_words:\n",
    "    r = first_four_graphemes_root(w)\n",
    "    if r:\n",
    "        corpus_roots.add(r)\n",
    "\n",
    "gold_df = gold_df.copy()\n",
    "gold_df['Root'] = gold_df.apply(lambda row: safe_first_segment(row, \"Morph_split\", \"Morph_split_str\"), axis=1)\n",
    "gold_roots = set([r for r in gold_df['Root'].dropna().map(str).map(str.strip) if r])\n",
    "\n",
    "cleaned_df = cleaned_df.copy()\n",
    "if 'Morph_split_str' not in cleaned_df.columns:\n",
    "    if 'Morph_split' in cleaned_df.columns:\n",
    "        def to_str_split(val):\n",
    "            if isinstance(val, list):\n",
    "                return ' '.join(map(str, val))\n",
    "            if isinstance(val, str):\n",
    "                try:\n",
    "                    parsed = ast.literal_eval(val)\n",
    "                    if isinstance(parsed, list):\n",
    "                        return ' '.join(map(str, parsed))\n",
    "                except Exception:\n",
    "                    return val\n",
    "            return ''\n",
    "        cleaned_df['Morph_split_str'] = cleaned_df['Morph_split'].apply(to_str_split)\n",
    "    else:\n",
    "        cleaned_df['Morph_split_str'] = ''\n",
    "\n",
    "cleaned_df['Root'] = cleaned_df.apply(lambda row: safe_first_segment(row, \"Morph_split\", \"Morph_split_str\"), axis=1)\n",
    "cleaned_roots = set([r for r in cleaned_df['Root'].dropna().map(str).map(str.strip) if r])\n",
    "\n",
    "print(f\"unique roots in corpus (first 4 graphemes): {len(corpus_roots):,}\")\n",
    "print(f\"unique roots in {GOLD_DF_FILE} (first segment): {len(gold_roots):,}\")\n",
    "print(f\"unique roots in {CLEANED_DF_FILE} (first segment): {len(cleaned_roots):,}\")\n",
    "\n",
    "roots_gold_cleaned = gold_roots.intersection(cleaned_roots)\n",
    "roots_gold_corpus = gold_roots.intersection(corpus_roots)\n",
    "roots_cleaned_corpus = cleaned_roots.intersection(corpus_roots)\n",
    "roots_all_three = gold_roots.intersection(cleaned_roots).intersection(corpus_roots)\n",
    "\n",
    "print(f\"overlapping roots (gold ∩ cleaned): {len(roots_gold_cleaned):,}\")\n",
    "print(f\"overlapping roots (gold ∩ corpus): {len(roots_gold_corpus):,}\")\n",
    "print(f\"overlapping roots (cleaned ∩ corpus): {len(roots_cleaned_corpus):,}\")\n",
    "print(f\"overlapping roots (gold ∩ cleaned ∩ corpus): {len(roots_all_three):,}\\n\")\n",
    "\n",
    "# Step 5: Rarity analysis\n",
    "print(f\"--- 3. rarity analysis (threshold: top {RARE_WORD_RANK_THRESHOLD:,} words) ---\")\n",
    "rare_words_in_gold = {word for word in gold_words if corpus_rank.get(word, float('inf')) > RARE_WORD_RANK_THRESHOLD}\n",
    "print(f\"[{GOLD_DF_FILE}]: {len(rare_words_in_gold):,} words are 'rare' (rank > {RARE_WORD_RANK_THRESHOLD:,})\")\n",
    "\n",
    "rare_words_in_cleaned = {word for word in cleaned_words if corpus_rank.get(word, float('inf')) > RARE_WORD_RANK_THRESHOLD}\n",
    "print(f\"[{CLEANED_DF_FILE}]: {len(rare_words_in_cleaned):,} words are 'rare' (rank > {RARE_WORD_RANK_THRESHOLD:,})\\n\")\n",
    "\n",
    "# Step 6: Coverage of non-rare words\n",
    "print(\"--- 4. coverage of non-rare words ---\")\n",
    "common_gold = gold_words - rare_words_in_gold\n",
    "common_cleaned = cleaned_words - rare_words_in_cleaned\n",
    "\n",
    "common_gold_in_corpus = common_gold.intersection(corpus_words)\n",
    "coverage_perc = (len(common_gold_in_corpus) / len(common_gold)) * 100 if common_gold else 0\n",
    "print(f\"[{GOLD_DF_FILE}]: of its {len(common_gold):,} non-rare words, {len(common_gold_in_corpus):,} ({coverage_perc:.2f}%) are in the corpus\")\n",
    "\n",
    "common_cleaned_in_corpus = common_cleaned.intersection(corpus_words)\n",
    "coverage_perc = (len(common_cleaned_in_corpus) / len(common_cleaned)) * 100 if common_cleaned else 0\n",
    "print(f\"[{CLEANED_DF_FILE}]: of its {len(common_cleaned):,} non-rare words, {len(common_cleaned_in_corpus):,} ({coverage_perc:.2f}%) are in the corpus\\n\")\n",
    "\n",
    "# Step 7: Remove rare words and save\n",
    "print(\"--- 5. removing rare words and saving new CSVs ---\")\n",
    "\n",
    "if not gold_df.empty:\n",
    "    filtered_gold_df = gold_df[~gold_df['Word'].isin(rare_words_in_gold)]\n",
    "    filtered_gold_df.to_csv(GOLD_OUTPUT_FILE, index=False, encoding='utf-8')\n",
    "    print(f\"removed {len(rare_words_in_gold)} rare words from '{GOLD_DF_FILE}'\")\n",
    "    print(f\"-> saved {len(filtered_gold_df)} rows to '{GOLD_OUTPUT_FILE}'\\n\")\n",
    "\n",
    "if not cleaned_df.empty:\n",
    "    filtered_cleaned_df = cleaned_df[~cleaned_df['Word'].isin(rare_words_in_cleaned)]\n",
    "    filtered_cleaned_df.to_csv(CLEANED_OUTPUT_FILE, index=False, encoding='utf-8')\n",
    "    print(f\"removed {len(rare_words_in_cleaned)} rare words from '{CLEANED_DF_FILE}'\")\n",
    "    print(f\"-> saved {len(filtered_cleaned_df)} rows to '{CLEANED_OUTPUT_FILE}'\\n\")\n",
    "\n",
    "# Step 2c: Word-level gold (common words with common roots)\n",
    "print(\"--- 2c. word-level gold (common words with common roots) ---\")\n",
    "\n",
    "def _seg_str_from_row(row):\n",
    "    if 'Morph_split' in row:\n",
    "        ms = row['Morph_split']\n",
    "        if isinstance(ms, list):\n",
    "            s = ' '.join(map(str, ms)).strip()\n",
    "            if s: return s\n",
    "        if isinstance(ms, str):\n",
    "            s = ms.strip()\n",
    "            try:\n",
    "                parsed = ast.literal_eval(s)\n",
    "                if isinstance(parsed, list) and parsed:\n",
    "                    s2 = ' '.join(map(str, parsed)).strip()\n",
    "                    if s2: return s2\n",
    "            except Exception:\n",
    "                if s: return s\n",
    "    if 'Morph_split_str' in row and isinstance(row['Morph_split_str'], str):\n",
    "        s = row['Morph_split_str'].strip()\n",
    "        if s: return s\n",
    "    if 'morph' in row and isinstance(row['morph'], str):\n",
    "        s = row['morph'].replace('-', ' ').strip()\n",
    "        if s: return s\n",
    "    return ''\n",
    "\n",
    "def _first_nonempty_map(df, value_col):\n",
    "    tmp = (\n",
    "        df[['Word', value_col]]\n",
    "        .copy()\n",
    "        .dropna(subset=['Word'])\n",
    "    )\n",
    "    tmp['Word'] = tmp['Word'].astype(str).str.strip()\n",
    "    tmp[value_col] = tmp[value_col].astype(str).str.strip()\n",
    "    tmp = tmp[tmp['Word'] != '']\n",
    "    tmp = tmp[tmp[value_col] != '']\n",
    "    return tmp.drop_duplicates(subset=['Word']).set_index('Word')[value_col].to_dict()\n",
    "\n",
    "gold_root_map = _first_nonempty_map(gold_df.rename(columns={'Root':'__Root'}), '__Root')\n",
    "cleaned_root_map = _first_nonempty_map(cleaned_df.rename(columns={'Root':'__Root'}), '__Root')\n",
    "\n",
    "if not gold_df.empty:\n",
    "    _gdf = gold_df.copy()\n",
    "    _gdf['__Seg'] = _gdf.apply(_seg_str_from_row, axis=1)\n",
    "    gold_seg_map = _first_nonempty_map(_gdf, '__Seg')\n",
    "else:\n",
    "    gold_seg_map = {}\n",
    "\n",
    "if not cleaned_df.empty:\n",
    "    _cldf = cleaned_df.copy()\n",
    "    _cldf['__Seg'] = _cldf.apply(_seg_str_from_row, axis=1)\n",
    "    cleaned_seg_map = _first_nonempty_map(_cldf, '__Seg')\n",
    "else:\n",
    "    cleaned_seg_map = {}\n",
    "\n",
    "words_all_three = gold_words.intersection(cleaned_words).intersection(corpus_words)\n",
    "print(f\"surface-overlap across all three datasets: {len(words_all_three):,} words\")\n",
    "\n",
    "rows = []\n",
    "kept = 0\n",
    "for w in words_all_three:\n",
    "    c_root = first_four_graphemes_root(w) or ''\n",
    "    r_gold = gold_root_map.get(w, '')\n",
    "    r_clean = cleaned_root_map.get(w, '')\n",
    "\n",
    "    if c_root and r_gold and r_clean and (c_root == r_gold == r_clean) and (c_root in roots_all_three):\n",
    "        seg = cleaned_seg_map.get(w, '') or gold_seg_map.get(w, '')\n",
    "        if seg:\n",
    "            rows.append({'Word': w, 'Morph_split': seg})\n",
    "            kept += 1\n",
    "\n",
    "word_level_gold_df = pd.DataFrame(rows).sort_values('Word')\n",
    "word_level_gold_df.to_csv(COMMON_WORDS_OUTPUT_FILE, index=False, encoding='utf-8')\n",
    "print(f\"-> saved {kept:,} rows to '{COMMON_WORDS_OUTPUT_FILE}' (columns: Word, Morph_split)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1715232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2: Synthetic Data Generation Using GPT Models\n",
    "\n",
    "def load_all_data():\n",
    "    \"\"\"Load data files and identify words needing segmentation.\"\"\"\n",
    "    print(\"--- step 1: loading all data files ---\")\n",
    "\n",
    "    if not os.path.exists(GOLD_DATA_FILE):\n",
    "        raise FileNotFoundError(f\"gold data file not found: '{GOLD_DATA_FILE}'. need to run the previous script first\")\n",
    "    gold_df = pd.read_csv(GOLD_DATA_FILE)\n",
    "\n",
    "    if 'Morph_split_str' not in gold_df.columns:\n",
    "        gold_df['Morph_split_str'] = ''\n",
    "    def _mk_str(val):\n",
    "        if isinstance(val, list):\n",
    "            return ' '.join(map(str, val))\n",
    "        if isinstance(val, str):\n",
    "            s = val.strip()\n",
    "            try:\n",
    "                parsed = ast.literal_eval(s)\n",
    "                if isinstance(parsed, list):\n",
    "                    return ' '.join(map(str, parsed))\n",
    "            except Exception:\n",
    "                return s\n",
    "        return ''\n",
    "    if 'Morph_split' in gold_df.columns:\n",
    "        gold_df['Morph_split_str'] = gold_df['Morph_split'].apply(_mk_str)\n",
    "    print(f\"loaded {len(gold_df):,} 'gold' examples for few-shot learning\")\n",
    "\n",
    "    combined_df = pd.read_parquet(GOLD_DF_FILE)\n",
    "    combined_df['Word'] = combined_df['word']\n",
    "    combined_df['morph'] = combined_df['morph'].str.replace('-', ' ')\n",
    "    combined_df['Morph_split_str'] = combined_df['morph']\n",
    "    combined_df['Morph_split'] = combined_df['morph'].str.split(' ')\n",
    "    combined_df = combined_df[['Word', 'Morph_split', 'Morph_split_str']]\n",
    "    cleaned_df = pd.read_csv(CLEANED_DF_FILE, encoding='windows-1252')\n",
    "\n",
    "    if 'Morph_split_str' not in combined_df.columns and 'Morph_split' in combined_df.columns:\n",
    "        def _to_str_split(val):\n",
    "            if isinstance(val, list):\n",
    "                return ' '.join(map(str, val))\n",
    "            if isinstance(val, str):\n",
    "                try:\n",
    "                    parsed = ast.literal_eval(val)\n",
    "                    if isinstance(parsed, list):\n",
    "                        return ' '.join(map(str, parsed))\n",
    "                except Exception:\n",
    "                    return val\n",
    "            return ''\n",
    "        combined_df['Morph_split_str'] = combined_df['Morph_split'].apply(_to_str_split) if 'Morph_split' in combined_df.columns else ''\n",
    "\n",
    "    if 'Morph_split_str' not in cleaned_df.columns and 'Morph_split' in cleaned_df.columns:\n",
    "        def _to_str_split2(val):\n",
    "            if isinstance(val, list):\n",
    "                return ' '.join(map(str, val))\n",
    "            if isinstance(val, str):\n",
    "                try:\n",
    "                    parsed = ast.literal_eval(val)\n",
    "                    if isinstance(parsed, list):\n",
    "                        return ' '.join(map(str, parsed))\n",
    "                except Exception:\n",
    "                    return val\n",
    "            return ''\n",
    "        cleaned_df['Morph_split_str'] = cleaned_df['Morph_split'].apply(_to_str_split2) if 'Morph_split' in cleaned_df.columns else ''\n",
    "\n",
    "    existing_words = set(combined_df['Word'].dropna()) | set(cleaned_df['Word'].dropna())\n",
    "    print(f\"found {len(existing_words):,} unique words across existing datasets\")\n",
    "\n",
    "    print(\"reading full corpus to find target words...\")\n",
    "    if not os.path.exists(CORPUS_FILE):\n",
    "        raise FileNotFoundError(f\"corpus file not found: {CORPUS_FILE}\")\n",
    "    TOKEN_RE = re.compile(r\"[^\\W\\d_]+(?:[''][^\\W\\d_]+)?\", flags=re.UNICODE)\n",
    "    with open(CORPUS_FILE, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        corpus_text = f.read().lower()\n",
    "    corpus_words_all = set(TOKEN_RE.findall(corpus_text))\n",
    "    print(f\"found {len(corpus_words_all):,} unique words in the corpus\")\n",
    "\n",
    "    corpus_roots = set()\n",
    "    for w in corpus_words_all:\n",
    "        r = first_four_graphemes_root(w)\n",
    "        if r:\n",
    "            corpus_roots.add(r)\n",
    "\n",
    "    combined_roots = set()\n",
    "    if not combined_df.empty:\n",
    "        combined_df = combined_df.copy()\n",
    "        combined_df['__root__'] = combined_df.apply(\n",
    "            lambda row: robust_first_segment(row, \"Morph_split\", \"Morph_split_str\", \"morph\"), axis=1\n",
    "        )\n",
    "        combined_roots = set([r for r in combined_df['__root__'].dropna().map(str).map(str.strip) if r])\n",
    "\n",
    "    cleaned_roots = set()\n",
    "    if not cleaned_df.empty:\n",
    "        cleaned_df = cleaned_df.copy()\n",
    "        cleaned_df['__root__'] = cleaned_df.apply(\n",
    "            lambda row: robust_first_segment(row, \"Morph_split\", \"Morph_split_str\", \"morph\"), axis=1\n",
    "        )\n",
    "        cleaned_roots = set([r for r in cleaned_df['__root__'].dropna().map(str).map(str.strip) if r])\n",
    "\n",
    "    gold_roots = combined_roots\n",
    "\n",
    "    common_roots_all_three = corpus_roots.intersection(gold_roots).intersection(cleaned_roots)\n",
    "    print(f\"roots common to all three datasets: {len(common_roots_all_three):,}\")\n",
    "\n",
    "    candidate_words = sorted(list(corpus_words_all - existing_words))\n",
    "    print(f\"-> initially identified {len(candidate_words):,} new corpus words (not in existing datasets)\")\n",
    "\n",
    "    words_to_segment = []\n",
    "    for w in candidate_words:\n",
    "        root = first_four_graphemes_root(w)\n",
    "        if root and root in common_roots_all_three:\n",
    "            words_to_segment.append(w)\n",
    "\n",
    "    print(f\"-> filtered to {len(words_to_segment):,} words whose roots are common to all three datasets\\n\")\n",
    "\n",
    "    return gold_df, words_to_segment\n",
    "\n",
    "def construct_few_shot_prompt(target_word, gold_df, num_examples):\n",
    "    \"\"\"Create prompt with few-shot examples.\"\"\"\n",
    "    examples = gold_df.sample(n=min(num_examples, len(gold_df)), random_state=random.randint(0, 10_000))\n",
    "\n",
    "    system_message = (\n",
    "        \"You are an expert in Quechua linguistics. Your task is to segment a given Quechua word into its constituent morphemes. \"\n",
    "        \"The morphemes should be separated by spaces. Provide only the segmented output, with no additional explanation or commentary.\"\n",
    "    )\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}]\n",
    "    for _, row in examples.iterrows():\n",
    "        s = row.get('Morph_split_str', '')\n",
    "        if not isinstance(s, str) or not s.strip():\n",
    "            s = ''\n",
    "            if 'Morph_split' in row and isinstance(row['Morph_split'], str):\n",
    "                try:\n",
    "                    parsed = ast.literal_eval(row['Morph_split'])\n",
    "                    if isinstance(parsed, list):\n",
    "                        s = ' '.join(map(str, parsed))\n",
    "                except Exception:\n",
    "                    s = row['Morph_split']\n",
    "        messages.append({\"role\": \"user\", \"content\": str(row['Word'])})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": s})\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": target_word})\n",
    "    return messages\n",
    "\n",
    "def get_model_params(model_name):\n",
    "    \"\"\"Get model-specific API parameters.\"\"\"\n",
    "    if \"gpt-5\" in model_name.lower() or \"gpt5\" in model_name.lower():\n",
    "        return {\n",
    "            \"reasoning_effort\": \"minimal\",\n",
    "            \"verbosity\": \"low\",\n",
    "        }\n",
    "    elif \"gpt-4o-mini\" in model_name.lower() or \"gpt-4o-mini\" in model_name:\n",
    "        return {\n",
    "            \"max_tokens\": 50,\n",
    "            \"temperature\": 0,\n",
    "            \"top_p\": 1,\n",
    "            \"frequency_penalty\": 0,\n",
    "            \"presence_penalty\": 0,\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"max_tokens\": 50,\n",
    "            \"temperature\": 0,\n",
    "            \"top_p\": 1,\n",
    "            \"frequency_penalty\": 0,\n",
    "            \"presence_penalty\": 0,\n",
    "        }\n",
    "\n",
    "def get_llm_segmentation(prompt_messages, model_name, retries=3, delay=5):\n",
    "    \"\"\"Call LLM API to get word segmentation, with rate limit handling.\"\"\"\n",
    "    def _retry_after_seconds(err, fallback):\n",
    "        try:\n",
    "            resp = getattr(err, \"response\", None)\n",
    "            if resp and getattr(resp, \"headers\", None):\n",
    "                ra = resp.headers.get(\"retry-after\") or resp.headers.get(\"Retry-After\")\n",
    "                if ra:\n",
    "                    return float(ra)\n",
    "        except Exception:\n",
    "            pass\n",
    "        return fallback\n",
    "\n",
    "    api_params = get_model_params(model_name)\n",
    "    api_params[\"model\"] = model_name\n",
    "    api_params[\"messages\"] = prompt_messages\n",
    "\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = client.chat.completions.create(**api_params)\n",
    "            return (response.choices[0].message.content or \"\").strip()\n",
    "\n",
    "        except RateLimitError as e:\n",
    "            base = delay * (2 ** attempt)\n",
    "            wait = _retry_after_seconds(e, base) + random.uniform(0, 0.5)\n",
    "            print(f\"  [ratelimit] hit 429. waiting {wait:.2f}s before retry {attempt+1}/{retries}...\")\n",
    "            time.sleep(wait)\n",
    "\n",
    "        except (APITimeoutError, APIConnectionError) as e:\n",
    "            wait = delay * (2 ** attempt) + random.uniform(0, 0.5)\n",
    "            print(f\"  [transient] {type(e).__name__}: {e}. waiting {wait:.2f}s (retry {attempt+1}/{retries})...\")\n",
    "            time.sleep(wait)\n",
    "\n",
    "        except APIError as e:\n",
    "            status = getattr(e, \"status_code\", None)\n",
    "            if status == 429:\n",
    "                base = delay * (2 ** attempt)\n",
    "                wait = _retry_after_seconds(e, base) + random.uniform(0, 0.5)\n",
    "                print(f\"  [api 429] waiting {wait:.2f}s before retry {attempt+1}/{retries}...\")\n",
    "                time.sleep(wait)\n",
    "            elif status == 400:\n",
    "                error_msg = str(e)\n",
    "                if \"parameter\" in error_msg.lower() or \"invalid\" in error_msg.lower():\n",
    "                    print(f\"  [api parameter error] model '{model_name}' may not support some parameters\")\n",
    "                    print(f\"  error: {error_msg}\")\n",
    "                    print(f\"  maybe check get_model_params() and adjust for this model\")\n",
    "                    print(f\"  might need to remove top_p, frequency_penalty, or presence_penalty\")\n",
    "                else:\n",
    "                    print(f\"  [api error 400]: {e}\")\n",
    "                break\n",
    "            else:\n",
    "                print(f\"  [api error] {status}: {e}\")\n",
    "                break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  [unhandled error]: {e}\")\n",
    "            break\n",
    "\n",
    "    return \"[API_FAILED]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a7384b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution: Generate synthetic data\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    print(\"error: OPENAI_API_KEY not set\")\n",
    "    print(\"need to set it before running\")\n",
    "else:\n",
    "    print(\"=\"*70)\n",
    "    print(\"SYNTHETIC DATA GENERATION FOR QUECHUA MORPHOLOGY\")\n",
    "    print(\"=\"*70)\n",
    "    gold_df, words_to_segment = load_all_data()\n",
    "\n",
    "    if WORDS_TO_PROCESS_LIMIT is not None:\n",
    "        print(f\"\\n--- applying processing limit: selecting {WORDS_TO_PROCESS_LIMIT} words randomly ---\")\n",
    "        if len(words_to_segment) > WORDS_TO_PROCESS_LIMIT:\n",
    "            words_to_segment = random.sample(words_to_segment, WORDS_TO_PROCESS_LIMIT)\n",
    "        else:\n",
    "            print(\"limit is larger than the number of available words. processing all\")\n",
    "\n",
    "    model_output_map = {\n",
    "        \"gpt-4o\": OUTPUT_FILE_GPT4O,\n",
    "        \"gpt-5-mini\": OUTPUT_FILE_GPT5MINI\n",
    "    }\n",
    "\n",
    "    for model_name in MODELS_TO_PROCESS:\n",
    "        if model_name not in model_output_map:\n",
    "            print(f\"unknown model '{model_name}', skipping...\")\n",
    "            continue\n",
    "            \n",
    "        output_file = model_output_map[model_name]\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"processing {len(words_to_segment):,} words using '{model_name}'\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        results = []\n",
    "        for word in tqdm(words_to_segment, desc=f\"segmenting with {model_name}\"):\n",
    "            prompt = construct_few_shot_prompt(word, gold_df, NUM_FEW_SHOT_EXAMPLES)\n",
    "            segmented_word = get_llm_segmentation(prompt, model_name)\n",
    "            results.append({\n",
    "                'Original_Word': word,\n",
    "                'Segmented_Morphemes': segmented_word,\n",
    "                'Source': f'LLM_FewShot_{model_name}',\n",
    "                'Model': model_name\n",
    "            })\n",
    "\n",
    "        print(f\"\\n--- saving results for {model_name} ---\")\n",
    "        results_df = pd.DataFrame(results)\n",
    "        results_df = results_df[results_df['Segmented_Morphemes'] != '[API_FAILED]']\n",
    "        results_df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "        print(f\"done! processed {len(results_df)} words with {model_name}\")\n",
    "        print(f\"   saved to '{output_file}'\")\n",
    "        print(f\"   failed calls: {len(results) - len(results_df)}\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"all done!\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"got segmentations for {len(words_to_segment):,} words using {len(MODELS_TO_PROCESS)} models\")\n",
    "    print(f\"files saved to {DATA_FOLDER}/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
