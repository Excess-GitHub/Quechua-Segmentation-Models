{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1af41d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing corpus file: data\\qu_merged_dump.txt...\n",
      "Corpus processed. Total unique valid grapheme-words (vocabulary size): 155,616\n",
      "\n",
      "==================================================\n",
      "ANALYSIS RESULTS\n",
      "==================================================\n",
      "\n",
      "--- 1. Corpus Coverage Analysis (surface forms) ---\n",
      "[data\\Sue_kalt.parquet]: Found 2,105 / 6,896 words in corpus (30.52% coverage).\n",
      "\n",
      "[data\\cleaned_data_df.csv]: Found 392 / 913 words in corpus (42.94% coverage).\n",
      "\n",
      "--- 2. Dataset Incongruity Analysis (surface forms) ---\n",
      "Words common to BOTH datasets: 89\n",
      "Words in the corpus AND common to both datasets: 85\n",
      "Words ONLY in 'data\\Sue_kalt.parquet': 6,807\n",
      "Words ONLY in 'data\\cleaned_data_df.csv': 824\n",
      "\n",
      "--- 2b. Root-Level Analysis (as requested) ---\n",
      "Unique roots in CORPUS (first 4 graphemes): 14,282\n",
      "Unique roots in data\\Sue_kalt.parquet (first segment): 1,357\n",
      "Unique roots in data\\cleaned_data_df.csv (first segment): 376\n",
      "Overlapping roots (gold ∩ cleaned): 125\n",
      "Overlapping roots (gold ∩ corpus): 408\n",
      "Overlapping roots (cleaned ∩ corpus): 157\n",
      "Overlapping roots (gold ∩ cleaned ∩ corpus): 78\n",
      "\n",
      "--- 3. Rarity Analysis (Threshold: Top 100,000 words) ---\n",
      "[data\\Sue_kalt.parquet]: 5,066 words are 'rare' (rank > 100,000).\n",
      "[data\\cleaned_data_df.csv]: 559 words are 'rare' (rank > 100,000).\n",
      "\n",
      "--- 4. Coverage of Non-Rare Words ---\n",
      "[data\\Sue_kalt.parquet]: Of its 1,830 non-rare words, 1,830 (100.00%) are in the corpus.\n",
      "[data\\cleaned_data_df.csv]: Of its 354 non-rare words, 354 (100.00%) are in the corpus.\n",
      "\n",
      "--- 5. Removing Rare Words and Saving New CSVs ---\n",
      "Removed 5066 rare words from 'data\\Sue_kalt.parquet'.\n",
      "-> Saved 2174 rows to 'data\\gold_df_common_words.csv'\n",
      "\n",
      "Removed 559 rare words from 'data\\cleaned_data_df.csv'.\n",
      "-> Saved 354 rows to 'data\\cleaned_data_df_common_words.csv'\n",
      "\n",
      "--- 2c. Word-Level GOLD (common words with common roots) ---\n",
      "Surface-overlap across all three datasets: 85 words\n",
      "-> Saved 37 rows to 'data\\word_analysis_gold.csv' (columns: Word, Morph_split)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SYNTHETIC DATA GENERATION FOR QUECHUA MORPHOLOGY PARSER\n",
    "=======================================================\n",
    "\n",
    "This notebook generates synthetic morphological segmentation data using GPT models.\n",
    "It consists of two main parts:\n",
    "\n",
    "PART 1 (Cell 0): Data Analysis and Gold Standard Creation\n",
    "- Analyzes corpus coverage and word rarity\n",
    "- Creates a \"gold standard\" dataset of high-quality segmentations\n",
    "- Filters words based on corpus frequency and root consistency\n",
    "\n",
    "PART 2 (Cell 1+): Synthetic Data Generation\n",
    "- Uses GPT-4o and GPT-5-mini to generate morphological segmentations\n",
    "- Applies few-shot learning with gold standard examples\n",
    "- Saves results to appropriately named files in the data folder\n",
    "\n",
    "The synthetic data is used to augment training data for the morphology parser.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import regex as re\n",
    "from collections import Counter\n",
    "import ast\n",
    "\n",
    "# These imports are needed for the norm_unicode function\n",
    "import unicodedata\n",
    "from ftfy import fix_text\n",
    "\n",
    "# =========================\n",
    "# DATA FOLDER CONFIGURATION\n",
    "# =========================\n",
    "# All data files should be read from and saved to the data folder\n",
    "DATA_FOLDER = \"data\"\n",
    "\n",
    "# =========================\n",
    "# CONFIGURATION\n",
    "# =========================\n",
    "# --- Input File Paths (read from data folder) ---\n",
    "CORPUS_FILE = os.path.join(DATA_FOLDER, \"qu_merged_dump.txt\")\n",
    "GOLD_DF_FILE = os.path.join(DATA_FOLDER, \"Sue_kalt.parquet\")  # Gold standard dataset (formerly called combined_df)\n",
    "CLEANED_DF_FILE = os.path.join(DATA_FOLDER, \"cleaned_data_df.csv\")\n",
    "\n",
    "# --- Output File Names (save to data folder) ---\n",
    "GOLD_OUTPUT_FILE = os.path.join(DATA_FOLDER, \"gold_df_common_words.csv\")\n",
    "CLEANED_OUTPUT_FILE = os.path.join(DATA_FOLDER, \"cleaned_data_df_common_words.csv\")\n",
    "# Gold standard dataset: high-quality examples for few-shot learning\n",
    "COMMON_WORDS_OUTPUT_FILE = os.path.join(DATA_FOLDER, \"word_analysis_gold.csv\") \n",
    "\n",
    "# --- Analysis Parameters ---\n",
    "RARE_WORD_RANK_THRESHOLD = 100000\n",
    "LOWERCASE = True\n",
    "KEEP_APOSTROPHES = True  # apostrophes will be considered invalid for grapheme filtering (so tokens with them are dropped)\n",
    "\n",
    "# =========================\n",
    "# GRAPHEMES\n",
    "# =========================\n",
    "graphemes = [\n",
    "    \"ch\",\"ll\",\"rr\",\"tr\",\"kw\",\"ph\",  # digraphs/trigraphs\n",
    "    \"a\",\"b\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\",\"k\",\"l\",\"m\",\"n\",\"ñ\",\"o\",\"p\",\"q\",\n",
    "    \"r\",\"s\",\"t\",\"u\",\"v\",\"w\",\"x\",\"y\"\n",
    "]\n",
    "\n",
    "# Precompute sorted-by-length graphemes for greedy matching (longest first)\n",
    "GRAPHEMES_BY_LEN = sorted(graphemes, key=len, reverse=True)\n",
    "SINGLE_CHARS = {g for g in graphemes if len(g) == 1}\n",
    "\n",
    "# =========================\n",
    "# HELPER FUNCTIONS\n",
    "# =========================\n",
    "\n",
    "# This utility function was part of your original zipf.py context and is needed here\n",
    "CTRL_RE = re.compile(r'[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F\\x7F-\\x9F]')\n",
    "def norm_unicode(x, form=\"NFC\"):\n",
    "    if pd.isna(x):\n",
    "        return x\n",
    "    s = x.decode(\"utf-8\", \"replace\") if isinstance(x, (bytes, bytearray)) else str(x)\n",
    "    s = fix_text(s)\n",
    "    s = CTRL_RE.sub('', s)\n",
    "    s = unicodedata.normalize(form, s)\n",
    "    s = s.replace('\\u00A0', ' ')\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "    return s\n",
    "\n",
    "def tokenize_graphemes(word: str):\n",
    "    \"\"\"\n",
    "    Greedy longest-match tokenizer over the allowed graphemes.\n",
    "    Returns a list of graphemes if fully tokenized, else None.\n",
    "    \"\"\"\n",
    "    if not isinstance(word, str):\n",
    "        return None\n",
    "    w = word.strip()\n",
    "    if LOWERCASE:\n",
    "        w = w.lower()\n",
    "\n",
    "    # If we allow apostrophes in tokenization stage, we still reject them for grapheme validity.\n",
    "    # So any apostrophe in the surface string => invalid grapheme word.\n",
    "    if \"'\" in w or \"’\" in w:\n",
    "        return None\n",
    "\n",
    "    i = 0\n",
    "    toks = []\n",
    "    n = len(w)\n",
    "    while i < n:\n",
    "        matched = False\n",
    "        # Try longest graphemes first (trigraph/digraph)\n",
    "        for g in GRAPHEMES_BY_LEN:\n",
    "            L = len(g)\n",
    "            if i + L <= n and w[i:i+L] == g:\n",
    "                toks.append(g)\n",
    "                i += L\n",
    "                matched = True\n",
    "                break\n",
    "        if not matched:\n",
    "            # No grapheme matched: invalid word\n",
    "            return None\n",
    "    return toks\n",
    "\n",
    "def is_valid_grapheme_word(word: str) -> bool:\n",
    "    \"\"\"A word is valid iff it can be fully segmented into the allowed graphemes.\"\"\"\n",
    "    toks = tokenize_graphemes(word)\n",
    "    return toks is not None\n",
    "\n",
    "def first_four_graphemes_root(word: str) -> str:\n",
    "    \"\"\"\n",
    "    Compute the corpus root as the concatenation of the first 4 graphemes.\n",
    "    If fewer than 4 graphemes, use whatever is available (empty if none).\n",
    "    Returns '' if the word is not a valid grapheme word.\n",
    "    \"\"\"\n",
    "    toks = tokenize_graphemes(word)\n",
    "    if toks is None or len(toks) == 0:\n",
    "        return ''\n",
    "    root = ''.join(toks[:4])\n",
    "    return root\n",
    "\n",
    "def safe_first_segment(row, prefer_list_col=\"Morph_split\", fallback_str_col=\"Morph_split_str\"):\n",
    "    \"\"\"\n",
    "    For gold_df / cleaned_df: return the first segment (root) robustly.\n",
    "    - If Morph_split is a list, use its first element.\n",
    "    - If Morph_split is a string representation of a list, try literal_eval.\n",
    "    - Else, fall back to splitting Morph_split_str (if present) on whitespace.\n",
    "    Returns '' if none available.\n",
    "    \"\"\"\n",
    "    # Try list column\n",
    "    if prefer_list_col in row:\n",
    "        val = row[prefer_list_col]\n",
    "        # Already a list\n",
    "        if isinstance(val, list) and len(val) > 0:\n",
    "            return str(val[0]).strip()\n",
    "        # String representation of a list\n",
    "        if isinstance(val, str):\n",
    "            s = val.strip()\n",
    "            # Try to parse as list\n",
    "            try:\n",
    "                parsed = ast.literal_eval(s)\n",
    "                if isinstance(parsed, list) and len(parsed) > 0:\n",
    "                    return str(parsed[0]).strip()\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    # Fallback to a plain string split column\n",
    "    if fallback_str_col in row:\n",
    "        s = row[fallback_str_col]\n",
    "        if isinstance(s, str) and s.strip():\n",
    "            return s.strip().split()[0]\n",
    "\n",
    "    return ''\n",
    "\n",
    "def process_corpus(file_path):\n",
    "    \"\"\"\n",
    "    Reads a large text corpus, tokenizes it, and calculates word frequencies and ranks.\n",
    "    **Updated**: Only counts tokens that are fully valid Quechua grapheme words.\n",
    "    \"\"\"\n",
    "    print(f\"Processing corpus file: {file_path}...\")\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"Corpus file not found at: {file_path}\")\n",
    "\n",
    "    # Base tokenization (word-like). Apostrophes may be included in the token,\n",
    "    # but we will drop any token that contains them since they're not allowed graphemes.\n",
    "    TOKEN_RE = re.compile(r\"[^\\W\\d_]+(?:['’][^\\W\\d_]+)?\", flags=re.UNICODE) if KEEP_APOSTROPHES \\\n",
    "                else re.compile(r\"[^\\W\\d_]+\", flags=re.UNICODE)\n",
    "\n",
    "    def iter_valid_tokens_from_file(path):\n",
    "        with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            for line in f:\n",
    "                if LOWERCASE:\n",
    "                    line = line.lower()\n",
    "                for m in TOKEN_RE.finditer(line):\n",
    "                    tok = m.group(0)\n",
    "                    # Keep only tokens that fully segment into allowed graphemes\n",
    "                    if is_valid_grapheme_word(tok):\n",
    "                        yield tok\n",
    "\n",
    "    freq = Counter(iter_valid_tokens_from_file(file_path))\n",
    "\n",
    "    if not freq:\n",
    "        print(\"Warning: Corpus processing resulted in zero tokens after grapheme filtering.\")\n",
    "        return {}, {}\n",
    "        \n",
    "    print(f\"Corpus processed. Total unique valid grapheme-words (vocabulary size): {len(freq):,}\")\n",
    "\n",
    "    sorted_words = [word for word, count in freq.most_common()]\n",
    "    rank_map = {word: i + 1 for i, word in enumerate(sorted_words)}\n",
    "    \n",
    "    return dict(freq), rank_map\n",
    "\n",
    "# =========================\n",
    "# MAIN ANALYSIS\n",
    "# =========================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Step 1: Process the corpus to get word frequencies and ranks (valid grapheme-words only)\n",
    "    corpus_freq, corpus_rank = process_corpus(CORPUS_FILE)\n",
    "\n",
    "    # Step 2: Load the full, prepped DataFrames\n",
    "    gold_df = pd.read_parquet(GOLD_DF_FILE)\n",
    "    gold_df['Word'] = gold_df['word']\n",
    "    gold_df['morph'] = gold_df['morph'].str.replace('-', ' ')\n",
    "    gold_df['Morph_split_str'] = gold_df['morph']\n",
    "    gold_df['Morph_split'] = gold_df['morph'].str.split(' ')\n",
    "    gold_df = gold_df[['Word', 'Morph_split', 'Morph_split_str']]\n",
    "\n",
    "    cleaned_df = pd.read_csv(CLEANED_DF_FILE, encoding='windows-1252')\n",
    "\n",
    "    # Extract sets of unique words for analysis (exact surface forms)\n",
    "    gold_words = set(gold_df['Word'].dropna().unique())\n",
    "    cleaned_words = set(cleaned_df['Word'].dropna().unique())\n",
    "    corpus_words = set(corpus_freq.keys())  # Already filtered to allowed grapheme-words\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ANALYSIS RESULTS\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "    # Step 3: Corpus Coverage Analysis (surface word coverage, unchanged)\n",
    "    print(\"--- 1. Corpus Coverage Analysis (surface forms) ---\")\n",
    "    gold_in_corpus = gold_words.intersection(corpus_words)\n",
    "    coverage_percentage = (len(gold_in_corpus) / len(gold_words)) * 100 if gold_words else 0\n",
    "    print(f\"[{GOLD_DF_FILE}]: Found {len(gold_in_corpus):,} / {len(gold_words):,} words in corpus ({coverage_percentage:.2f}% coverage).\\n\")\n",
    "\n",
    "    cleaned_in_corpus = cleaned_words.intersection(corpus_words)\n",
    "    coverage_percentage = (len(cleaned_in_corpus) / len(cleaned_words)) * 100 if cleaned_words else 0\n",
    "    print(f\"[{CLEANED_DF_FILE}]: Found {len(cleaned_in_corpus):,} / {len(cleaned_words):,} words in corpus ({coverage_percentage:.2f}% coverage).\\n\")\n",
    "\n",
    "    # Step 4: Dataset Incongruity Analysis (surface forms, unchanged for backward compatibility)\n",
    "    print(\"--- 2. Dataset Incongruity Analysis (surface forms) ---\")\n",
    "    words_in_common = gold_words.intersection(cleaned_words)\n",
    "    words_only_in_gold = gold_words.difference(cleaned_words)\n",
    "    words_only_in_cleaned = cleaned_words.difference(gold_words)\n",
    "    common_and_in_corpus = words_in_common.intersection(corpus_words)\n",
    "\n",
    "    print(f\"Words common to BOTH datasets: {len(words_in_common):,}\")\n",
    "    print(f\"Words in the corpus AND common to both datasets: {len(common_and_in_corpus):,}\")\n",
    "\n",
    "    print(f\"Words ONLY in '{GOLD_DF_FILE}': {len(words_only_in_gold):,}\")\n",
    "    print(f\"Words ONLY in '{CLEANED_DF_FILE}': {len(words_only_in_cleaned):,}\\n\")\n",
    "\n",
    "    # =========================\n",
    "    # NEW: ROOT-LEVEL ANALYSIS\n",
    "    # =========================\n",
    "    print(\"--- 2b. Root-Level Analysis (as requested) ---\")\n",
    "\n",
    "    # Corpus roots: first 4 graphemes\n",
    "    corpus_roots = set()\n",
    "    for w in corpus_words:\n",
    "        r = first_four_graphemes_root(w)\n",
    "        if r:\n",
    "            corpus_roots.add(r)\n",
    "\n",
    "    # Gold_df roots: first segment of segmentation\n",
    "    gold_df = gold_df.copy()\n",
    "    gold_df['Root'] = gold_df.apply(lambda row: safe_first_segment(row, \"Morph_split\", \"Morph_split_str\"), axis=1)\n",
    "    gold_roots = set([r for r in gold_df['Root'].dropna().map(str).map(str.strip) if r])\n",
    "\n",
    "    # Cleaned_df roots: same logic (handle list/str robustly)\n",
    "    cleaned_df = cleaned_df.copy()\n",
    "    # Ensure helpful fallback column for cleaned_df if not present\n",
    "    if 'Morph_split_str' not in cleaned_df.columns:\n",
    "        # try to form from Morph_split if it exists (string or list)\n",
    "        if 'Morph_split' in cleaned_df.columns:\n",
    "            def to_str_split(val):\n",
    "                if isinstance(val, list):\n",
    "                    return ' '.join(map(str, val))\n",
    "                if isinstance(val, str):\n",
    "                    try:\n",
    "                        parsed = ast.literal_eval(val)\n",
    "                        if isinstance(parsed, list):\n",
    "                            return ' '.join(map(str, parsed))\n",
    "                    except Exception:\n",
    "                        return val\n",
    "                return ''\n",
    "            cleaned_df['Morph_split_str'] = cleaned_df['Morph_split'].apply(to_str_split)\n",
    "        else:\n",
    "            cleaned_df['Morph_split_str'] = ''\n",
    "\n",
    "    cleaned_df['Root'] = cleaned_df.apply(lambda row: safe_first_segment(row, \"Morph_split\", \"Morph_split_str\"), axis=1)\n",
    "    cleaned_roots = set([r for r in cleaned_df['Root'].dropna().map(str).map(str.strip) if r])\n",
    "\n",
    "    # Report unique root counts\n",
    "    print(f\"Unique roots in CORPUS (first 4 graphemes): {len(corpus_roots):,}\")\n",
    "    print(f\"Unique roots in {GOLD_DF_FILE} (first segment): {len(gold_roots):,}\")\n",
    "    print(f\"Unique roots in {CLEANED_DF_FILE} (first segment): {len(cleaned_roots):,}\")\n",
    "\n",
    "    # Overlaps on roots\n",
    "    roots_gold_cleaned = gold_roots.intersection(cleaned_roots)\n",
    "    roots_gold_corpus  = gold_roots.intersection(corpus_roots)\n",
    "    roots_cleaned_corpus   = cleaned_roots.intersection(corpus_roots)\n",
    "    roots_all_three        = gold_roots.intersection(cleaned_roots).intersection(corpus_roots)\n",
    "\n",
    "    print(f\"Overlapping roots (gold ∩ cleaned): {len(roots_gold_cleaned):,}\")\n",
    "    print(f\"Overlapping roots (gold ∩ corpus): {len(roots_gold_corpus):,}\")\n",
    "    print(f\"Overlapping roots (cleaned ∩ corpus): {len(roots_cleaned_corpus):,}\")\n",
    "    print(f\"Overlapping roots (gold ∩ cleaned ∩ corpus): {len(roots_all_three):,}\\n\")\n",
    "\n",
    "    # Step 5: Rarity Analysis (unchanged; still on surface forms)\n",
    "    print(f\"--- 3. Rarity Analysis (Threshold: Top {RARE_WORD_RANK_THRESHOLD:,} words) ---\")\n",
    "    rare_words_in_gold = {word for word in gold_words if corpus_rank.get(word, float('inf')) > RARE_WORD_RANK_THRESHOLD}\n",
    "    print(f\"[{GOLD_DF_FILE}]: {len(rare_words_in_gold):,} words are 'rare' (rank > {RARE_WORD_RANK_THRESHOLD:,}).\")\n",
    "    \n",
    "    rare_words_in_cleaned = {word for word in cleaned_words if corpus_rank.get(word, float('inf')) > RARE_WORD_RANK_THRESHOLD}\n",
    "    print(f\"[{CLEANED_DF_FILE}]: {len(rare_words_in_cleaned):,} words are 'rare' (rank > {RARE_WORD_RANK_THRESHOLD:,}).\\n\")\n",
    "    \n",
    "    # Step 6: Coverage of Non-Rare Words (unchanged; still on surface forms)\n",
    "    print(\"--- 4. Coverage of Non-Rare Words ---\")\n",
    "    common_gold = gold_words - rare_words_in_gold\n",
    "    common_cleaned = cleaned_words - rare_words_in_cleaned\n",
    "    \n",
    "    common_gold_in_corpus = common_gold.intersection(corpus_words)\n",
    "    coverage_perc = (len(common_gold_in_corpus) / len(common_gold)) * 100 if common_gold else 0\n",
    "    print(f\"[{GOLD_DF_FILE}]: Of its {len(common_gold):,} non-rare words, {len(common_gold_in_corpus):,} ({coverage_perc:.2f}%) are in the corpus.\")\n",
    "\n",
    "    common_cleaned_in_corpus = common_cleaned.intersection(corpus_words)\n",
    "    coverage_perc = (len(common_cleaned_in_corpus) / len(common_cleaned)) * 100 if common_cleaned else 0\n",
    "    print(f\"[{CLEANED_DF_FILE}]: Of its {len(common_cleaned):,} non-rare words, {len(common_cleaned_in_corpus):,} ({coverage_perc:.2f}%) are in the corpus.\\n\")\n",
    "\n",
    "    # Step 7: Removing Rare Words and Saving New CSVs (unchanged)\n",
    "    print(\"--- 5. Removing Rare Words and Saving New CSVs ---\")\n",
    "\n",
    "    # Filter the gold_df DataFrame\n",
    "    if not gold_df.empty:\n",
    "        filtered_gold_df = gold_df[~gold_df['Word'].isin(rare_words_in_gold)]\n",
    "        filtered_gold_df.to_csv(GOLD_OUTPUT_FILE, index=False, encoding='utf-8')\n",
    "        print(f\"Removed {len(rare_words_in_gold)} rare words from '{GOLD_DF_FILE}'.\")\n",
    "        print(f\"-> Saved {len(filtered_gold_df)} rows to '{GOLD_OUTPUT_FILE}'\\n\")\n",
    "\n",
    "    # Filter the cleaned_data_df DataFrame\n",
    "    if not cleaned_df.empty:\n",
    "        filtered_cleaned_df = cleaned_df[~cleaned_df['Word'].isin(rare_words_in_cleaned)]\n",
    "        filtered_cleaned_df.to_csv(CLEANED_OUTPUT_FILE, index=False, encoding='utf-8')\n",
    "        print(f\"Removed {len(rare_words_in_cleaned)} rare words from '{CLEANED_DF_FILE}'.\")\n",
    "        print(f\"-> Saved {len(filtered_cleaned_df)} rows to '{CLEANED_OUTPUT_FILE}'\\n\")\n",
    "\n",
    "        # =========================\n",
    "    # 2c. WORD-LEVEL GOLD (common words with common roots) -> Word + Segmentation only\n",
    "    # =========================\n",
    "    print(\"--- 2c. Word-Level GOLD (common words with common roots) ---\")\n",
    "\n",
    "    # Helper to turn any Morph_split / Morph_split_str / morph into a clean space-separated string\n",
    "    def _seg_str_from_row(row):\n",
    "        # Prefer Morph_split if it's a list or list-like string\n",
    "        if 'Morph_split' in row:\n",
    "            ms = row['Morph_split']\n",
    "            if isinstance(ms, list):\n",
    "                s = ' '.join(map(str, ms)).strip()\n",
    "                if s: return s\n",
    "            if isinstance(ms, str):\n",
    "                s = ms.strip()\n",
    "                # try to parse list literal\n",
    "                try:\n",
    "                    parsed = ast.literal_eval(s)\n",
    "                    if isinstance(parsed, list) and parsed:\n",
    "                        s2 = ' '.join(map(str, parsed)).strip()\n",
    "                        if s2: return s2\n",
    "                except Exception:\n",
    "                    # assume it's already space-separated pieces\n",
    "                    if s: return s\n",
    "        # Fallback to Morph_split_str\n",
    "        if 'Morph_split_str' in row and isinstance(row['Morph_split_str'], str):\n",
    "            s = row['Morph_split_str'].strip()\n",
    "            if s: return s\n",
    "        # Last resort: 'morph' (replace '-' with space)\n",
    "        if 'morph' in row and isinstance(row['morph'], str):\n",
    "            s = row['morph'].replace('-', ' ').strip()\n",
    "            if s: return s\n",
    "        return ''\n",
    "\n",
    "    # Build fast lookups for roots and segmentations by Word from both labeled datasets\n",
    "    # (take the first non-empty per Word)\n",
    "    def _first_nonempty_map(df, value_col):\n",
    "        tmp = (\n",
    "            df[['Word', value_col]]\n",
    "            .copy()\n",
    "            .dropna(subset=['Word'])\n",
    "        )\n",
    "        tmp['Word'] = tmp['Word'].astype(str).str.strip()\n",
    "        tmp[value_col] = tmp[value_col].astype(str).str.strip()\n",
    "        tmp = tmp[tmp['Word'] != '']\n",
    "        tmp = tmp[tmp[value_col] != '']\n",
    "        return tmp.drop_duplicates(subset=['Word']).set_index('Word')[value_col].to_dict()\n",
    "\n",
    "    # Ensure we have a Root column in both frames (already computed above)\n",
    "    gold_root_map = _first_nonempty_map(gold_df.rename(columns={'Root':'__Root'}), '__Root')\n",
    "    cleaned_root_map  = _first_nonempty_map(cleaned_df .rename(columns={'Root':'__Root'}), '__Root')\n",
    "\n",
    "    # Build segmentation maps (string) from both frames\n",
    "    gold_seg_series = []\n",
    "    if not gold_df.empty:\n",
    "        _gdf = gold_df.copy()\n",
    "        _gdf['__Seg'] = _gdf.apply(_seg_str_from_row, axis=1)\n",
    "        gold_seg_map = _first_nonempty_map(_gdf, '__Seg')\n",
    "    else:\n",
    "        gold_seg_map = {}\n",
    "\n",
    "    cleaned_seg_series = []\n",
    "    if not cleaned_df.empty:\n",
    "        _cldf = cleaned_df.copy()\n",
    "        _cldf['__Seg'] = _cldf.apply(_seg_str_from_row, axis=1)\n",
    "        cleaned_seg_map = _first_nonempty_map(_cldf, '__Seg')\n",
    "    else:\n",
    "        cleaned_seg_map = {}\n",
    "\n",
    "    # Words present across all three datasets (surface overlap)\n",
    "    words_all_three = gold_words.intersection(cleaned_words).intersection(corpus_words)\n",
    "    print(f\"Surface-overlap across all three datasets: {len(words_all_three):,} words\")\n",
    "\n",
    "    rows = []\n",
    "    kept = 0\n",
    "    for w in words_all_three:\n",
    "        # Roots from each source\n",
    "        c_root = first_four_graphemes_root(w) or ''\n",
    "        r_gold = gold_root_map.get(w, '')\n",
    "        r_clean = cleaned_root_map.get(w, '')\n",
    "\n",
    "        # Keep only if non-empty, equal, and in the triple-overlap root set\n",
    "        if c_root and r_gold and r_clean and (c_root == r_gold == r_clean) and (c_root in roots_all_three):\n",
    "            # Prefer cleaned segmentation; fallback to gold\n",
    "            seg = cleaned_seg_map.get(w, '') or gold_seg_map.get(w, '')\n",
    "            if seg:\n",
    "                rows.append({'Word': w, 'Morph_split': seg})\n",
    "                kept += 1\n",
    "\n",
    "    word_level_gold_df = pd.DataFrame(rows).sort_values('Word')\n",
    "    word_level_gold_df.to_csv(COMMON_WORDS_OUTPUT_FILE, index=False, encoding='utf-8')\n",
    "    print(f\"-> Saved {kept:,} rows to '{COMMON_WORDS_OUTPUT_FILE}' (columns: Word, Morph_split)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2c95cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SYNTHETIC DATA GENERATION FOR QUECHUA MORPHOLOGY\n",
      "======================================================================\n",
      "--- Step 1: Loading all data files ---\n",
      "Loaded 37 'gold' examples for few-shot learning.\n",
      "Found 7,720 unique words across existing datasets.\n",
      "Reading full corpus to find target words...\n",
      "Found 208,684 unique words in the corpus.\n",
      "Roots common to all three datasets: 78\n",
      "-> Initially identified 206,081 new corpus words (not in existing datasets).\n",
      "-> Filtered to 24,343 words whose roots are common to all three datasets.\n",
      "\n",
      "\n",
      "--- Applying processing limit: selecting 5 words randomly. ---\n",
      "\n",
      "======================================================================\n",
      "Processing 5 words using 'gpt-4o'\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Segmenting with gpt-4o: 100%|██████████| 5/5 [00:05<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Saving results for gpt-4o ---\n",
      "✅ Successfully processed 5 words with gpt-4o.\n",
      "   Results saved to 'data\\gpt4o_synthetic_segmentations.csv'\n",
      "   Failed API calls: 0\n",
      "\n",
      "======================================================================\n",
      "Processing 5 words using 'gpt-5-mini'\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Segmenting with gpt-5-mini: 100%|██████████| 5/5 [00:05<00:00,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Saving results for gpt-5-mini ---\n",
      "✅ Successfully processed 5 words with gpt-5-mini.\n",
      "   Results saved to 'data\\gpt5mini_synthetic_segmentations.csv'\n",
      "   Failed API calls: 0\n",
      "\n",
      "======================================================================\n",
      "SYNTHETIC DATA GENERATION COMPLETE\n",
      "======================================================================\n",
      "Generated segmentations for 5 words using 2 models.\n",
      "Output files saved to data/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "PART 2: SYNTHETIC DATA GENERATION USING GPT MODELS\n",
    "==================================================\n",
    "\n",
    "This cell generates synthetic morphological segmentations using GPT-4o and GPT-5-mini.\n",
    "It uses few-shot learning with gold standard examples to guide the models.\n",
    "\n",
    "The process:\n",
    "1. Loads gold standard examples and identifies words needing segmentation\n",
    "2. For each model (gpt4o and gpt5mini):\n",
    "   - Constructs few-shot prompts with examples\n",
    "   - Calls the API to get segmentations\n",
    "   - Handles rate limits and errors gracefully\n",
    "3. Saves results to separate files for each model in the data folder\n",
    "\n",
    "IMPORTANT: Different models may require different API parameters\n",
    "---------------------------------------------------------------\n",
    "If you encounter parameter errors (400 Bad Request), you may need to adjust the\n",
    "get_model_params() function. Some models (especially newer ones) may not support\n",
    "all standard parameters like top_p, frequency_penalty, or presence_penalty.\n",
    "\n",
    "To fix parameter errors:\n",
    "1. Check the error message - it will indicate which parameter is invalid\n",
    "2. Edit the get_model_params() function to remove unsupported parameters for that model\n",
    "3. For GPT-5 models, you may need to use only: max_tokens and temperature\n",
    "4. Consult OpenAI API documentation for the specific model you're using\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import regex as re\n",
    "from collections import Counter\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Note: You must install the 'openai' library for this script to work.\n",
    "# Run: pip install openai\n",
    "import time, random\n",
    "from openai import OpenAI\n",
    "from openai import RateLimitError, APIError, APITimeoutError, APIConnectionError\n",
    "import ast\n",
    "\n",
    "# =========================\n",
    "# DATA FOLDER CONFIGURATION\n",
    "# =========================\n",
    "DATA_FOLDER = \"data\"\n",
    "\n",
    "# =========================\n",
    "# CONFIGURATION\n",
    "# =========================\n",
    "# --- Input File Paths (read from data folder) ---\n",
    "CORPUS_FILE = os.path.join(DATA_FOLDER, \"qu_merged_dump.txt\")\n",
    "GOLD_DF_FILE = os.path.join(DATA_FOLDER, \"Sue_kalt.parquet\")  # Gold standard dataset (formerly called combined_df)\n",
    "CLEANED_DF_FILE = os.path.join(DATA_FOLDER, \"cleaned_data_df.csv\")\n",
    "GOLD_DATA_FILE = os.path.join(DATA_FOLDER, \"word_analysis_gold.csv\")  # High-quality examples for few-shot learning\n",
    "\n",
    "# --- Output File Names (save to data folder) ---\n",
    "# Files will be named based on the model used\n",
    "OUTPUT_FILE_GPT4O = os.path.join(DATA_FOLDER, \"gpt4o_synthetic_segmentations.csv\")\n",
    "OUTPUT_FILE_GPT5MINI = os.path.join(DATA_FOLDER, \"gpt5mini_synthetic_segmentations.csv\")\n",
    "\n",
    "# --- API & Analysis Parameters ---\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Models to use for synthetic data generation\n",
    "# Note: Model names must match OpenAI API model identifiers\n",
    "# Common model names:\n",
    "#   - \"gpt-4o\" (standard GPT-4o)\n",
    "#   - \"gpt-4o-mini\" (smaller, faster version)\n",
    "#   - \"gpt-5-mini\" or \"gpt-5mini\" (if available in your API)\n",
    "# Check OpenAI API documentation for current model names and their supported parameters\n",
    "# Different models may require different API parameters - see get_model_params() function\n",
    "MODELS_TO_PROCESS = [\"gpt-4o\", \"gpt-5-mini\"]  # Process both models\n",
    "# Alternative: Use \"gpt-4o-mini\" if \"gpt-5-mini\" is not available:\n",
    "# MODELS_TO_PROCESS = [\"gpt-4o\", \"gpt-4o-mini\"]\n",
    "\n",
    "# --- Few-Shot Learning Parameters ---\n",
    "NUM_FEW_SHOT_EXAMPLES = 37  # How many examples to show the model in each prompt\n",
    "WORDS_TO_PROCESS_LIMIT = 5  # Set a limit to avoid high API costs during testing. Set to None to process all.\n",
    "\n",
    "# =========================\n",
    "# QUECHUA GRAPHEMES + HELPERS\n",
    "# =========================\n",
    "# Quechua uses a specific set of graphemes (letters and letter combinations)\n",
    "# These are used to validate words and extract roots for filtering\n",
    "graphemes = [\n",
    "    \"ch\",\"ll\",\"rr\",\"tr\",\"kw\",\"ph\",  # digraphs/trigraphs (multi-character graphemes)\n",
    "    \"a\",\"b\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\",\"k\",\"l\",\"m\",\"n\",\"ñ\",\"o\",\"p\",\"q\",\n",
    "    \"r\",\"s\",\"t\",\"u\",\"v\",\"w\",\"x\",\"y\"\n",
    "]\n",
    "# Sort by length (longest first) for greedy matching\n",
    "GRAPHEMES_BY_LEN = sorted(graphemes, key=len, reverse=True)\n",
    "\n",
    "def tokenize_graphemes(word: str):\n",
    "    \"\"\"\n",
    "    Greedy longest-match tokenizer over the allowed graphemes.\n",
    "    Returns a list of graphemes if fully tokenized, else None.\n",
    "    \"\"\"\n",
    "    if not isinstance(word, str):\n",
    "        return None\n",
    "    w = word.strip().lower()\n",
    "    if not w: \n",
    "        return None\n",
    "    # Apostrophes are not in the inventory -> reject\n",
    "    if \"'\" in w or \"’\" in w:\n",
    "        return None\n",
    "    i = 0\n",
    "    toks = []\n",
    "    n = len(w)\n",
    "    while i < n:\n",
    "        matched = False\n",
    "        for g in GRAPHEMES_BY_LEN:\n",
    "            L = len(g)\n",
    "            if i + L <= n and w[i:i+L] == g:\n",
    "                toks.append(g)\n",
    "                i += L\n",
    "                matched = True\n",
    "                break\n",
    "        if not matched:\n",
    "            return None\n",
    "    return toks\n",
    "\n",
    "def first_four_graphemes_root(word: str) -> str:\n",
    "    \"\"\"\n",
    "    Root for corpus words: concatenation of the first 4 graphemes.\n",
    "    Returns '' if not tokenizable.\n",
    "    \"\"\"\n",
    "    toks = tokenize_graphemes(word)\n",
    "    if not toks:\n",
    "        return ''\n",
    "    return ''.join(toks[:4])\n",
    "\n",
    "def robust_first_segment(row, prefer_list_col=\"Morph_split\", fallback_str_col=\"Morph_split_str\", alt_morph_col=\"morph\"):\n",
    "    \"\"\"\n",
    "    For gold_df / cleaned_df: extract the 'root' as the first segment.\n",
    "    - If Morph_split is a list -> take [0]\n",
    "    - If Morph_split is a string representation of a list -> literal_eval then [0]\n",
    "    - Else try splitting Morph_split_str (space)\n",
    "    - Else if 'morph' is present, replace hyphens with spaces and take first token\n",
    "    Returns '' if not found.\n",
    "    \"\"\"\n",
    "    # Try Morph_split as actual list\n",
    "    if prefer_list_col in row:\n",
    "        val = row[prefer_list_col]\n",
    "        if isinstance(val, list) and val:\n",
    "            return str(val[0]).strip()\n",
    "        if isinstance(val, str):\n",
    "            s = val.strip()\n",
    "            # Attempt to parse as list\n",
    "            try:\n",
    "                parsed = ast.literal_eval(s)\n",
    "                if isinstance(parsed, list) and parsed:\n",
    "                    return str(parsed[0]).strip()\n",
    "            except Exception:\n",
    "                # treat as plain string with spaces\n",
    "                if s:\n",
    "                    return s.split()[0].strip()\n",
    "\n",
    "    # Try Morph_split_str\n",
    "    if fallback_str_col in row:\n",
    "        s = row[fallback_str_col]\n",
    "        if isinstance(s, str) and s.strip():\n",
    "            return s.strip().split()[0]\n",
    "\n",
    "    # Try 'morph' column (hyphen-separated)\n",
    "    if alt_morph_col in row:\n",
    "        m = row[alt_morph_col]\n",
    "        if isinstance(m, str) and m.strip():\n",
    "            return m.replace('-', ' ').strip().split()[0]\n",
    "\n",
    "    return ''\n",
    "\n",
    "# =========================\n",
    "# HELPER FUNCTIONS (I/O + filtering)\n",
    "# =========================\n",
    "def load_all_data():\n",
    "    \"\"\"\n",
    "    Loads all necessary data files and identifies words that need segmentation.\n",
    "    \n",
    "    This function:\n",
    "    1. Loads the gold standard dataset (high-quality examples for few-shot learning)\n",
    "    2. Loads existing segmented datasets (gold_df and cleaned_df)\n",
    "    3. Processes the corpus to find words not yet segmented\n",
    "    4. Filters words to only those whose roots are common across all three datasets\n",
    "    \n",
    "    Root definitions:\n",
    "      - Corpus root = first 4 graphemes of the word\n",
    "      - Gold/cleaned root = first segment from morphological segmentation\n",
    "    \n",
    "    This filtering ensures we only segment words that are likely to be valid\n",
    "    and consistent with existing data.\n",
    "    \n",
    "    Returns:\n",
    "        gold_df: DataFrame with gold standard examples\n",
    "        words_to_segment: List of words that need segmentation\n",
    "    \"\"\"\n",
    "    print(\"--- Step 1: Loading all data files ---\")\n",
    "\n",
    "    # ---- GOLD (few-shot) ----\n",
    "    if not os.path.exists(GOLD_DATA_FILE):\n",
    "        raise FileNotFoundError(f\"Gold data file not found: '{GOLD_DATA_FILE}'. Please run the previous script first.\")\n",
    "    gold_df = pd.read_csv(GOLD_DATA_FILE)\n",
    "\n",
    "    # Ensure Morph_split_str exists and is usable\n",
    "    if 'Morph_split_str' not in gold_df.columns:\n",
    "        gold_df['Morph_split_str'] = ''\n",
    "    def _mk_str(val):\n",
    "        if isinstance(val, list):\n",
    "            return ' '.join(map(str, val))\n",
    "        if isinstance(val, str):\n",
    "            s = val.strip()\n",
    "            try:\n",
    "                parsed = ast.literal_eval(s)\n",
    "                if isinstance(parsed, list):\n",
    "                    return ' '.join(map(str, parsed))\n",
    "            except Exception:\n",
    "                # Already a plain string of splits\n",
    "                return s\n",
    "        return ''\n",
    "    if 'Morph_split' in gold_df.columns:\n",
    "        gold_df['Morph_split_str'] = gold_df['Morph_split'].apply(_mk_str)\n",
    "    print(f\"Loaded {len(gold_df):,} 'gold' examples for few-shot learning.\")\n",
    "\n",
    "    # ---- EXISTING SEGMENTED DATASETS ----\n",
    "    # Read full files (not just 'Word') so we can derive roots robustly\n",
    "    combined_df = pd.read_parquet(COMBINED_DF_FILE)\n",
    "    combined_df['Word'] = combined_df['word']\n",
    "    combined_df['morph'] = combined_df['morph'].str.replace('-', ' ')\n",
    "    combined_df['Morph_split_str'] = combined_df['morph']\n",
    "    combined_df['Morph_split'] = combined_df['morph'].str.split(' ')\n",
    "    combined_df = combined_df[['Word', 'Morph_split', 'Morph_split_str']]\n",
    "    cleaned_df  = pd.read_csv(CLEANED_DF_FILE,  encoding='windows-1252')\n",
    "\n",
    "    # Normalize helpful columns if missing\n",
    "    if 'Morph_split_str' not in combined_df.columns and 'Morph_split' in combined_df.columns:\n",
    "        def _to_str_split(val):\n",
    "            if isinstance(val, list):\n",
    "                return ' '.join(map(str, val))\n",
    "            if isinstance(val, str):\n",
    "                try:\n",
    "                    parsed = ast.literal_eval(val)\n",
    "                    if isinstance(parsed, list):\n",
    "                        return ' '.join(map(str, parsed))\n",
    "                except Exception:\n",
    "                    return val\n",
    "            return ''\n",
    "        combined_df['Morph_split_str'] = combined_df['Morph_split'].apply(_to_str_split) if 'Morph_split' in combined_df.columns else ''\n",
    "\n",
    "    if 'Morph_split_str' not in cleaned_df.columns and 'Morph_split' in cleaned_df.columns:\n",
    "        def _to_str_split2(val):\n",
    "            if isinstance(val, list):\n",
    "                return ' '.join(map(str, val))\n",
    "            if isinstance(val, str):\n",
    "                try:\n",
    "                    parsed = ast.literal_eval(val)\n",
    "                    if isinstance(parsed, list):\n",
    "                        return ' '.join(map(str, parsed))\n",
    "                except Exception:\n",
    "                    return val\n",
    "            return ''\n",
    "        cleaned_df['Morph_split_str'] = cleaned_df['Morph_split'].apply(_to_str_split2) if 'Morph_split' in cleaned_df.columns else ''\n",
    "\n",
    "    # Build sets of existing words\n",
    "    existing_words = set(combined_df['Word'].dropna()) | set(cleaned_df['Word'].dropna())\n",
    "    print(f\"Found {len(existing_words):,} unique words across existing datasets.\")\n",
    "\n",
    "    # ---- CORPUS WORDS (unique) ----\n",
    "    print(\"Reading full corpus to find target words...\")\n",
    "    if not os.path.exists(CORPUS_FILE):\n",
    "        raise FileNotFoundError(f\"Corpus file not found: {CORPUS_FILE}\")\n",
    "    TOKEN_RE = re.compile(r\"[^\\W\\d_]+(?:['’][^\\W\\d_]+)?\", flags=re.UNICODE)\n",
    "    with open(CORPUS_FILE, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        corpus_text = f.read().lower()\n",
    "    corpus_words_all = set(TOKEN_RE.findall(corpus_text))\n",
    "    print(f\"Found {len(corpus_words_all):,} unique words in the corpus.\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # ROOTS FOR ALL THREE DATASETS\n",
    "    # -----------------------------\n",
    "    # Corpus roots via first 4 graphemes (skip words that don't tokenize)\n",
    "    corpus_roots = set()\n",
    "    for w in corpus_words_all:\n",
    "        r = first_four_graphemes_root(w)\n",
    "        if r:\n",
    "            corpus_roots.add(r)\n",
    "\n",
    "    # Combined_df roots via first segment\n",
    "    combined_roots = set()\n",
    "    if not combined_df.empty:\n",
    "        combined_df = combined_df.copy()\n",
    "        combined_df['__root__'] = combined_df.apply(\n",
    "            lambda row: robust_first_segment(row, \"Morph_split\", \"Morph_split_str\", \"morph\"), axis=1\n",
    "        )\n",
    "        combined_roots = set([r for r in combined_df['__root__'].dropna().map(str).map(str.strip) if r])\n",
    "\n",
    "    # Cleaned_df roots via first segment\n",
    "    cleaned_roots = set()\n",
    "    if not cleaned_df.empty:\n",
    "        cleaned_df = cleaned_df.copy()\n",
    "        cleaned_df['__root__'] = cleaned_df.apply(\n",
    "            lambda row: robust_first_segment(row, \"Morph_split\", \"Morph_split_str\", \"morph\"), axis=1\n",
    "        )\n",
    "        cleaned_roots = set([r for r in cleaned_df['__root__'].dropna().map(str).map(str.strip) if r])\n",
    "\n",
    "    # Intersection of roots present in ALL THREE\n",
    "    common_roots_all_three = corpus_roots.intersection(gold_roots).intersection(cleaned_roots)\n",
    "    print(f\"Roots common to all three datasets: {len(common_roots_all_three):,}\")\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # Determine corpus words needing segmentation\n",
    "    # --------------------------------------------\n",
    "    # Only words not already in existing datasets...\n",
    "    candidate_words = sorted(list(corpus_words_all - existing_words))\n",
    "    print(f\"-> Initially identified {len(candidate_words):,} new corpus words (not in existing datasets).\")\n",
    "\n",
    "    # ...and whose corpus-root (first 4 graphemes) is in the intersection across all three datasets\n",
    "    words_to_segment = []\n",
    "    for w in candidate_words:\n",
    "        root = first_four_graphemes_root(w)\n",
    "        if root and root in common_roots_all_three:\n",
    "            words_to_segment.append(w)\n",
    "\n",
    "    print(f\"-> Filtered to {len(words_to_segment):,} words whose roots are common to all three datasets.\\n\")\n",
    "\n",
    "    return gold_df, words_to_segment\n",
    "\n",
    "def construct_few_shot_prompt(target_word, gold_df, num_examples):\n",
    "    \"\"\"\n",
    "    Creates a detailed prompt for the API with few-shot examples.\n",
    "    \"\"\"\n",
    "    examples = gold_df.sample(n=min(num_examples, len(gold_df)), random_state=random.randint(0, 10_000))\n",
    "\n",
    "    system_message = (\n",
    "        \"You are an expert in Quechua linguistics. Your task is to segment a given Quechua word into its constituent morphemes. \"\n",
    "        \"The morphemes should be separated by spaces. Please provide only the segmented output, with no additional explanation or commentary.\"\n",
    "    )\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}]\n",
    "    for _, row in examples.iterrows():\n",
    "        # fallbacks in case Morph_split_str wasn't constructed above for some row\n",
    "        s = row.get('Morph_split_str', '')\n",
    "        if not isinstance(s, str) or not s.strip():\n",
    "            s = ''\n",
    "            if 'Morph_split' in row and isinstance(row['Morph_split'], str):\n",
    "                try:\n",
    "                    parsed = ast.literal_eval(row['Morph_split'])\n",
    "                    if isinstance(parsed, list):\n",
    "                        s = ' '.join(map(str, parsed))\n",
    "                except Exception:\n",
    "                    s = row['Morph_split']\n",
    "        messages.append({\"role\": \"user\", \"content\": str(row['Word'])})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": s})\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": target_word})\n",
    "    return messages\n",
    "\n",
    "def get_model_params(model_name):\n",
    "    \"\"\"\n",
    "    Returns model-specific API parameters.\n",
    "    Different models support different parameters, so we need to customize them.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name of the model (e.g., \"gpt-4o\", \"gpt-5-mini\", \"gpt-4o-mini\")\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of parameters to use for the API call (excluding 'model' and 'messages' which are set separately)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Model-specific parameter configurations\n",
    "    if \"gpt-5\" in model_name.lower() or \"gpt5\" in model_name.lower():\n",
    "        # GPT-5 models may support reasoning_effort and verbosity\n",
    "        # Note: Adjust these based on actual API documentation\n",
    "        # Some GPT-5 models may not support all standard parameters\n",
    "        return {\n",
    "            # GPT-5 specific parameters\n",
    "            \"reasoning_effort\": \"minimal\",  # Options: \"minimal\", \"low\", \"medium\", \"high\"\n",
    "            \"verbosity\": \"low\",  # Options: \"low\", \"medium\", \"high\"\n",
    "        }\n",
    "    elif \"gpt-4o-mini\" in model_name.lower() or \"gpt-4o-mini\" in model_name:\n",
    "        # GPT-4o-mini uses standard parameters (same as GPT-4o)\n",
    "        return {\n",
    "            \"max_tokens\": 50,\n",
    "            \"temperature\": 0,\n",
    "            \"top_p\": 1,\n",
    "            \"frequency_penalty\": 0,\n",
    "            \"presence_penalty\": 0,\n",
    "        }\n",
    "    else:\n",
    "        # Default for GPT-4o and other standard models\n",
    "        return {\n",
    "            \"max_tokens\": 50,\n",
    "            \"temperature\": 0,\n",
    "            \"top_p\": 1,\n",
    "            \"frequency_penalty\": 0,\n",
    "            \"presence_penalty\": 0,\n",
    "        }\n",
    "\n",
    "def get_llm_segmentation(prompt_messages, model_name, retries=3, delay=5):\n",
    "    \"\"\"\n",
    "    Calls the LLM API to get the word segmentation, with robust rate-limit handling.\n",
    "    Retries on 429s (RateLimitError) and transient network/timeouts with exponential backoff + jitter.\n",
    "    \n",
    "    This function automatically adjusts API parameters based on the model being used.\n",
    "    Different models (GPT-4o, GPT-5-mini, etc.) may require different parameters.\n",
    "    \n",
    "    Args:\n",
    "        prompt_messages: List of message dictionaries for the API\n",
    "        model_name: Name of the model to use (e.g., \"gpt-4o\", \"gpt-5-mini\", \"gpt-4o-mini\")\n",
    "        retries: Number of retry attempts\n",
    "        delay: Base delay in seconds for exponential backoff\n",
    "    \n",
    "    Returns:\n",
    "        Segmented word string, or \"[API_FAILED]\" if all retries fail\n",
    "    \"\"\"\n",
    "    def _retry_after_seconds(err, fallback):\n",
    "        # Respect Retry-After header if the server provides it\n",
    "        try:\n",
    "            resp = getattr(err, \"response\", None)\n",
    "            if resp and getattr(resp, \"headers\", None):\n",
    "                ra = resp.headers.get(\"retry-after\") or resp.headers.get(\"Retry-After\")\n",
    "                if ra:\n",
    "                    return float(ra)\n",
    "        except Exception:\n",
    "            pass\n",
    "        return fallback\n",
    "\n",
    "    # Get model-specific parameters\n",
    "    api_params = get_model_params(model_name)\n",
    "    api_params[\"model\"] = model_name\n",
    "    api_params[\"messages\"] = prompt_messages\n",
    "\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            # Create API call with model-specific parameters\n",
    "            response = client.chat.completions.create(**api_params)\n",
    "            return (response.choices[0].message.content or \"\").strip()\n",
    "\n",
    "        except RateLimitError as e:\n",
    "            # 429 rate limit -> exponential backoff + jitter, honoring Retry-After if present\n",
    "            base = delay * (2 ** attempt)\n",
    "            wait = _retry_after_seconds(e, base) + random.uniform(0, 0.5)\n",
    "            print(f\"  [RateLimit] Hit 429. Waiting {wait:.2f}s before retry {attempt+1}/{retries}...\")\n",
    "            time.sleep(wait)\n",
    "\n",
    "        except (APITimeoutError, APIConnectionError) as e:\n",
    "            # Transient connectivity/timeouts\n",
    "            wait = delay * (2 ** attempt) + random.uniform(0, 0.5)\n",
    "            print(f\"  [Transient] {type(e).__name__}: {e}. Waiting {wait:.2f}s (retry {attempt+1}/{retries})...\")\n",
    "            time.sleep(wait)\n",
    "\n",
    "        except APIError as e:\n",
    "            # Some APIError instances may also be 429; treat similarly\n",
    "            status = getattr(e, \"status_code\", None)\n",
    "            if status == 429:\n",
    "                base = delay * (2 ** attempt)\n",
    "                wait = _retry_after_seconds(e, base) + random.uniform(0, 0.5)\n",
    "                print(f\"  [API 429] Waiting {wait:.2f}s before retry {attempt+1}/{retries}...\")\n",
    "                time.sleep(wait)\n",
    "            elif status == 400:\n",
    "                # 400 Bad Request often means invalid parameters\n",
    "                error_msg = str(e)\n",
    "                if \"parameter\" in error_msg.lower() or \"invalid\" in error_msg.lower():\n",
    "                    print(f\"  [API Parameter Error] Model '{model_name}' may not support some parameters.\")\n",
    "                    print(f\"  Error: {error_msg}\")\n",
    "                    print(f\"  Suggestion: Check get_model_params() function and adjust parameters for this model.\")\n",
    "                    print(f\"  You may need to remove top_p, frequency_penalty, or presence_penalty for this model.\")\n",
    "                else:\n",
    "                    print(f\"  [API Error 400]: {e}\")\n",
    "                # Don't retry on parameter errors - they won't be fixed by retrying\n",
    "                break\n",
    "            else:\n",
    "                # Non-retryable API errors: surface and stop\n",
    "                print(f\"  [API Error] {status}: {e}\")\n",
    "                break\n",
    "\n",
    "        except Exception as e:\n",
    "            # Unknown error: log and stop retrying\n",
    "            print(f\"  [Unhandled Error]: {e}\")\n",
    "            break\n",
    "\n",
    "    return \"[API_FAILED]\"\n",
    "\n",
    "# =========================\n",
    "# MAIN EXECUTION\n",
    "# =========================\n",
    "# Process synthetic data generation for both GPT-4o and GPT-5-mini\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "        print(\"FATAL ERROR: The 'OPENAI_API_KEY' environment variable is not set.\")\n",
    "        print(\"Please set it before running the script.\")\n",
    "    else:\n",
    "        # Step 1: Load data and find words to process (filtered by common roots)\n",
    "        print(\"=\"*70)\n",
    "        print(\"SYNTHETIC DATA GENERATION FOR QUECHUA MORPHOLOGY\")\n",
    "        print(\"=\"*70)\n",
    "        gold_df, words_to_segment = load_all_data()\n",
    "\n",
    "        # Apply limit if one is set\n",
    "        if WORDS_TO_PROCESS_LIMIT is not None:\n",
    "            print(f\"\\n--- Applying processing limit: selecting {WORDS_TO_PROCESS_LIMIT} words randomly. ---\")\n",
    "            if len(words_to_segment) > WORDS_TO_PROCESS_LIMIT:\n",
    "                words_to_segment = random.sample(words_to_segment, WORDS_TO_PROCESS_LIMIT)\n",
    "            else:\n",
    "                print(\"Limit is larger than the number of available words. Processing all.\")\n",
    "\n",
    "        # Step 2: Process words using each model\n",
    "        # Map model names to output file paths\n",
    "        model_output_map = {\n",
    "            \"gpt-4o\": OUTPUT_FILE_GPT4O,\n",
    "            \"gpt-5-mini\": OUTPUT_FILE_GPT5MINI\n",
    "        }\n",
    "\n",
    "        for model_name in MODELS_TO_PROCESS:\n",
    "            if model_name not in model_output_map:\n",
    "                print(f\"Warning: Unknown model '{model_name}', skipping...\")\n",
    "                continue\n",
    "                \n",
    "            output_file = model_output_map[model_name]\n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(f\"Processing {len(words_to_segment):,} words using '{model_name}'\")\n",
    "            print(f\"{'='*70}\")\n",
    "            \n",
    "            results = []\n",
    "            for word in tqdm(words_to_segment, desc=f\"Segmenting with {model_name}\"):\n",
    "                prompt = construct_few_shot_prompt(word, gold_df, NUM_FEW_SHOT_EXAMPLES)\n",
    "                segmented_word = get_llm_segmentation(prompt, model_name)\n",
    "                results.append({\n",
    "                    'Original_Word': word,\n",
    "                    'Segmented_Morphemes': segmented_word,\n",
    "                    'Source': f'LLM_FewShot_{model_name}',\n",
    "                    'Model': model_name\n",
    "                })\n",
    "\n",
    "            # Step 3: Save results to a CSV file (filter out API failures)\n",
    "            print(f\"\\n--- Saving results for {model_name} ---\")\n",
    "            results_df = pd.DataFrame(results)\n",
    "            # Filter out API failures\n",
    "            results_df = results_df[results_df['Segmented_Morphemes'] != '[API_FAILED]']\n",
    "            results_df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "            print(f\"✅ Successfully processed {len(results_df)} words with {model_name}.\")\n",
    "            print(f\"   Results saved to '{output_file}'\")\n",
    "            print(f\"   Failed API calls: {len(results) - len(results_df)}\")\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"SYNTHETIC DATA GENERATION COMPLETE\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"Generated segmentations for {len(words_to_segment):,} words using {len(MODELS_TO_PROCESS)} models.\")\n",
    "        print(f\"Output files saved to {DATA_FOLDER}/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5ae80b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Loading all data files ---\n",
      "Loaded 37 'gold' examples for few-shot learning.\n",
      "Found 7,720 unique words across existing datasets.\n",
      "Reading full corpus to find target words...\n",
      "Found 208,684 unique words in the corpus.\n",
      "Roots common to all three datasets: 78\n",
      "-> Initially identified 206,081 new corpus words (not in existing datasets).\n",
      "-> Filtered to 24,343 words whose roots are common to all three datasets.\n",
      "\n",
      "--- Selected 10 words from Sue Kalt not in few-shot examples. ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Segmenting Words: 100%|██████████| 10/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== FEW-SHOT PROMPT =====\n",
      "\n",
      "You are an expert in Quechua linguistics. Your task is to segment a given Quechua word into its constituent morphemes.\n",
      "The morphemes should be separated by spaces. Please provide only the segmented output, with no additional explanation or commentary.\n",
      "\n",
      "Examples:\n",
      "mamantachá: mama n ta chá\n",
      "tio: tio\n",
      "pukllakuchkanman: puklla ku chka nman\n",
      "wawitawanchu: waw ita wan chu\n",
      "bañakuychik: baña ku ychik\n",
      "pilkanturaqa: pilkantura qa\n",
      "hinanchu: hina n chu\n",
      "haytachkan: hayta chka n\n",
      "wakninpitaq: wak ni n pi taq\n",
      "kaypi: kay pi\n",
      "cambian: cambia n\n",
      "purichkan: puri chka n\n",
      "parlankuchá: parla nku chá\n",
      "wawakuna: wawa kuna\n",
      "suyachipun: suya chi pu n\n",
      "imitatapis: im ita ta pis\n",
      "urqukunayan: urqu ku naya n\n",
      "harkayukuspa: harka yu ku spa\n",
      "yaykuptiyki: yayku pti yki\n",
      "manchariptin: mancha ri pti n\n",
      "kaysitus: kay situ s\n",
      "cuñaday: cuñada y\n",
      "así: así\n",
      "chitiqa: chiti qa\n",
      "qallarikunña: qallari ku n ña\n",
      "rikhuriyunku: rikhuri yu nku\n",
      "rikhuripusqa: rikhuri pu sqa\n",
      "pasan: pasa n\n",
      "kirullañachá: kiru lla ña chá\n",
      "ovejatapuni: oveja ta puni\n",
      "mikhunayan: mikhu naya n\n",
      "maymantaq: may man taq\n",
      "hampatitustachá: hampatu it s ta chá\n",
      "tukuyusqa: tuku yu sqa\n",
      "este: este\n",
      "huk: huk\n",
      "ñapis: ña pis\n",
      "\n",
      "Words to Segment:\n",
      "kachituta\n",
      "correnallanpaqtaq\n",
      "ñispataqchá\n",
      "uñitasnin\n",
      "willani\n",
      "partepitaq\n",
      "patitapiñaqa\n",
      "kaytataq\n",
      "allqituswan\n",
      "makisnin\n",
      "\n",
      "\n",
      "===== GOLD SEGMENTATIONS (for eval) =====\n",
      "\n",
      "kachituta: kachu it ta\n",
      "correnallanpaqtaq: corre na lla n paq taq\n",
      "ñispataqchá: ñi spa taq chá\n",
      "uñitasnin: uña it s ni n\n",
      "willani: willa ni\n",
      "partepitaq: parte pi taq\n",
      "patitapiñaqa: pata it pi ña qa\n",
      "kaytataq: kay ta taq\n",
      "allqituswan: allqu it s wan\n",
      "makisnin: maki s ni n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TEST/ALTERNATIVE VERSION: PROMPT GENERATION FOR MANUAL TESTING\n",
    "================================================================\n",
    "\n",
    "This cell is an alternative approach for testing prompt generation.\n",
    "It creates prompts that can be manually tested or used for batch processing.\n",
    "This is useful for debugging or when you want to see the prompts before sending to API.\n",
    "\n",
    "Note: This cell is for testing purposes. Use Cell 1 for actual synthetic data generation.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import regex as re\n",
    "from collections import Counter\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Note: You must install the 'openai' library for this script to work.\n",
    "# Run: pip install openai\n",
    "import time, random\n",
    "from openai import OpenAI\n",
    "from openai import RateLimitError, APIError, APITimeoutError, APIConnectionError\n",
    "import ast\n",
    "\n",
    "# =========================\n",
    "# DATA FOLDER CONFIGURATION\n",
    "# =========================\n",
    "DATA_FOLDER = \"data\"\n",
    "\n",
    "# =========================\n",
    "# CONFIGURATION\n",
    "# =========================\n",
    "# --- Input File Paths (read from data folder) ---\n",
    "CORPUS_FILE = os.path.join(DATA_FOLDER, \"qu_merged_dump.txt\")\n",
    "GOLD_DF_FILE = os.path.join(DATA_FOLDER, \"Sue_kalt.parquet\")  # Gold standard dataset (formerly called combined_df)\n",
    "CLEANED_DF_FILE = os.path.join(DATA_FOLDER, \"cleaned_data_df.csv\")\n",
    "GOLD_DATA_FILE = os.path.join(DATA_FOLDER, \"word_analysis_gold.csv\")  # High-quality examples for few-shot learning\n",
    "\n",
    "# --- Few-Shot Learning Parameters ---\n",
    "NUM_FEW_SHOT_EXAMPLES = 37  # How many examples to show the model in each prompt\n",
    "WORDS_TO_PROCESS_LIMIT = 10  # Set a limit to avoid high API costs during testing. Set to None to process all.\n",
    "\n",
    "# =========================\n",
    "# QUECHUA GRAPHEMES + HELPERS\n",
    "# =========================\n",
    "graphemes = [\n",
    "    \"ch\",\"ll\",\"rr\",\"tr\",\"kw\",\"ph\",  # digraphs/trigraphs\n",
    "    \"a\",\"b\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\",\"k\",\"l\",\"m\",\"n\",\"ñ\",\"o\",\"p\",\"q\",\n",
    "    \"r\",\"s\",\"t\",\"u\",\"v\",\"w\",\"x\",\"y\"\n",
    "]\n",
    "GRAPHEMES_BY_LEN = sorted(graphemes, key=len, reverse=True)\n",
    "\n",
    "def tokenize_graphemes(word: str):\n",
    "    \"\"\"\n",
    "    Greedy longest-match tokenizer over the allowed graphemes.\n",
    "    Returns a list of graphemes if fully tokenized, else None.\n",
    "    \"\"\"\n",
    "    if not isinstance(word, str):\n",
    "        return None\n",
    "    w = word.strip().lower()\n",
    "    if not w: \n",
    "        return None\n",
    "    # Apostrophes are not in the inventory -> reject\n",
    "    if \"'\" in w or \"’\" in w:\n",
    "        return None\n",
    "    i = 0\n",
    "    toks = []\n",
    "    n = len(w)\n",
    "    while i < n:\n",
    "        matched = False\n",
    "        for g in GRAPHEMES_BY_LEN:\n",
    "            L = len(g)\n",
    "            if i + L <= n and w[i:i+L] == g:\n",
    "                toks.append(g)\n",
    "                i += L\n",
    "                matched = True\n",
    "                break\n",
    "        if not matched:\n",
    "            return None\n",
    "    return toks\n",
    "\n",
    "def first_four_graphemes_root(word: str) -> str:\n",
    "    \"\"\"\n",
    "    Root for corpus words: concatenation of the first 4 graphemes.\n",
    "    Returns '' if not tokenizable.\n",
    "    \"\"\"\n",
    "    toks = tokenize_graphemes(word)\n",
    "    if not toks:\n",
    "        return ''\n",
    "    return ''.join(toks[:4])\n",
    "\n",
    "def robust_first_segment(row, prefer_list_col=\"Morph_split\", fallback_str_col=\"Morph_split_str\", alt_morph_col=\"morph\"):\n",
    "    \"\"\"\n",
    "    For gold_df / cleaned_df: extract the 'root' as the first segment.\n",
    "    - If Morph_split is a list -> take [0]\n",
    "    - If Morph_split is a string representation of a list -> literal_eval then [0]\n",
    "    - Else try splitting Morph_split_str (space)\n",
    "    - Else if 'morph' is present, replace hyphens with spaces and take first token\n",
    "    Returns '' if not found.\n",
    "    \"\"\"\n",
    "    # Try Morph_split as actual list\n",
    "    if prefer_list_col in row:\n",
    "        val = row[prefer_list_col]\n",
    "        if isinstance(val, list) and val:\n",
    "            return str(val[0]).strip()\n",
    "        if isinstance(val, str):\n",
    "            s = val.strip()\n",
    "            # Attempt to parse as list\n",
    "            try:\n",
    "                parsed = ast.literal_eval(s)\n",
    "                if isinstance(parsed, list) and parsed:\n",
    "                    return str(parsed[0]).strip()\n",
    "            except Exception:\n",
    "                # treat as plain string with spaces\n",
    "                if s:\n",
    "                    return s.split()[0].strip()\n",
    "\n",
    "    # Try Morph_split_str\n",
    "    if fallback_str_col in row:\n",
    "        s = row[fallback_str_col]\n",
    "        if isinstance(s, str) and s.strip():\n",
    "            return s.strip().split()[0]\n",
    "\n",
    "    # Try 'morph' column (hyphen-separated)\n",
    "    if alt_morph_col in row:\n",
    "        m = row[alt_morph_col]\n",
    "        if isinstance(m, str) and m.strip():\n",
    "            return m.replace('-', ' ').strip().split()[0]\n",
    "\n",
    "    return ''\n",
    "\n",
    "# =========================\n",
    "# HELPER FUNCTIONS (I/O + filtering)\n",
    "# =========================\n",
    "def load_all_data():\n",
    "    \"\"\"\n",
    "    Loads all necessary data files and identifies words that need segmentation.\n",
    "    NOW: Filters words_to_segment to only those whose corpus roots are COMMON\n",
    "    to all three datasets (corpus, gold_df, cleaned_df), where:\n",
    "      - corpus root = first 4 graphemes\n",
    "      - gold/cleaned root = first segment\n",
    "    \"\"\"\n",
    "    print(\"--- Step 1: Loading all data files ---\")\n",
    "\n",
    "    # ---- GOLD (few-shot) ----\n",
    "    if not os.path.exists(GOLD_DATA_FILE):\n",
    "        raise FileNotFoundError(f\"Gold data file not found: '{GOLD_DATA_FILE}'. Please run the previous script first.\")\n",
    "    gold_df = pd.read_csv(GOLD_DATA_FILE)\n",
    "\n",
    "    # Ensure Morph_split_str exists and is usable\n",
    "    if 'Morph_split_str' not in gold_df.columns:\n",
    "        gold_df['Morph_split_str'] = ''\n",
    "    def _mk_str(val):\n",
    "        if isinstance(val, list):\n",
    "            return ' '.join(map(str, val))\n",
    "        if isinstance(val, str):\n",
    "            s = val.strip()\n",
    "            try:\n",
    "                parsed = ast.literal_eval(s)\n",
    "                if isinstance(parsed, list):\n",
    "                    return ' '.join(map(str, parsed))\n",
    "            except Exception:\n",
    "                # Already a plain string of splits\n",
    "                return s\n",
    "        return ''\n",
    "    if 'Morph_split' in gold_df.columns:\n",
    "        gold_df['Morph_split_str'] = gold_df['Morph_split'].apply(_mk_str)\n",
    "    print(f\"Loaded {len(gold_df):,} 'gold' examples for few-shot learning.\")\n",
    "\n",
    "    # ---- EXISTING SEGMENTED DATASETS ----\n",
    "    # Read full files (not just 'Word') so we can derive roots robustly\n",
    "    combined_df = pd.read_parquet(COMBINED_DF_FILE)\n",
    "    combined_df['Word'] = combined_df['word']\n",
    "    combined_df['morph'] = combined_df['morph'].str.replace('-', ' ')\n",
    "    combined_df['Morph_split_str'] = combined_df['morph']\n",
    "    combined_df['Morph_split'] = combined_df['morph'].str.split(' ')\n",
    "    combined_df = combined_df[['Word', 'Morph_split', 'Morph_split_str']]\n",
    "    cleaned_df  = pd.read_csv(CLEANED_DF_FILE,  encoding='windows-1252')\n",
    "\n",
    "    # Normalize helpful columns if missing\n",
    "    if 'Morph_split_str' not in combined_df.columns and 'Morph_split' in combined_df.columns:\n",
    "        def _to_str_split(val):\n",
    "            if isinstance(val, list):\n",
    "                return ' '.join(map(str, val))\n",
    "            if isinstance(val, str):\n",
    "                try:\n",
    "                    parsed = ast.literal_eval(val)\n",
    "                    if isinstance(parsed, list):\n",
    "                        return ' '.join(map(str, parsed))\n",
    "                except Exception:\n",
    "                    return val\n",
    "            return ''\n",
    "        combined_df['Morph_split_str'] = combined_df['Morph_split'].apply(_to_str_split) if 'Morph_split' in combined_df.columns else ''\n",
    "\n",
    "    if 'Morph_split_str' not in cleaned_df.columns and 'Morph_split' in cleaned_df.columns:\n",
    "        def _to_str_split2(val):\n",
    "            if isinstance(val, list):\n",
    "                return ' '.join(map(str, val))\n",
    "            if isinstance(val, str):\n",
    "                try:\n",
    "                    parsed = ast.literal_eval(val)\n",
    "                    if isinstance(parsed, list):\n",
    "                        return ' '.join(map(str, parsed))\n",
    "                except Exception:\n",
    "                    return val\n",
    "            return ''\n",
    "        cleaned_df['Morph_split_str'] = cleaned_df['Morph_split'].apply(_to_str_split2) if 'Morph_split' in cleaned_df.columns else ''\n",
    "\n",
    "    # Build sets of existing words\n",
    "    existing_words = set(combined_df['Word'].dropna()) | set(cleaned_df['Word'].dropna())\n",
    "    print(f\"Found {len(existing_words):,} unique words across existing datasets.\")\n",
    "\n",
    "    # ---- CORPUS WORDS (unique) ----\n",
    "    print(\"Reading full corpus to find target words...\")\n",
    "    if not os.path.exists(CORPUS_FILE):\n",
    "        raise FileNotFoundError(f\"Corpus file not found: {CORPUS_FILE}\")\n",
    "    TOKEN_RE = re.compile(r\"[^\\W\\d_]+(?:['’][^\\W\\d_]+)?\", flags=re.UNICODE)\n",
    "    with open(CORPUS_FILE, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        corpus_text = f.read().lower()\n",
    "    corpus_words_all = set(TOKEN_RE.findall(corpus_text))\n",
    "    print(f\"Found {len(corpus_words_all):,} unique words in the corpus.\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # ROOTS FOR ALL THREE DATASETS\n",
    "    # -----------------------------\n",
    "    # Corpus roots via first 4 graphemes (skip words that don't tokenize)\n",
    "    corpus_roots = set()\n",
    "    for w in corpus_words_all:\n",
    "        r = first_four_graphemes_root(w)\n",
    "        if r:\n",
    "            corpus_roots.add(r)\n",
    "\n",
    "    # Combined_df roots via first segment\n",
    "    combined_roots = set()\n",
    "    if not combined_df.empty:\n",
    "        combined_df = combined_df.copy()\n",
    "        combined_df['__root__'] = combined_df.apply(\n",
    "            lambda row: robust_first_segment(row, \"Morph_split\", \"Morph_split_str\", \"morph\"), axis=1\n",
    "        )\n",
    "        combined_roots = set([r for r in combined_df['__root__'].dropna().map(str).map(str.strip) if r])\n",
    "\n",
    "    # Cleaned_df roots via first segment\n",
    "    cleaned_roots = set()\n",
    "    if not cleaned_df.empty:\n",
    "        cleaned_df = cleaned_df.copy()\n",
    "        cleaned_df['__root__'] = cleaned_df.apply(\n",
    "            lambda row: robust_first_segment(row, \"Morph_split\", \"Morph_split_str\", \"morph\"), axis=1\n",
    "        )\n",
    "        cleaned_roots = set([r for r in cleaned_df['__root__'].dropna().map(str).map(str.strip) if r])\n",
    "\n",
    "    # Intersection of roots present in ALL THREE\n",
    "    common_roots_all_three = corpus_roots.intersection(combined_roots).intersection(cleaned_roots)\n",
    "    print(f\"Roots common to all three datasets: {len(common_roots_all_three):,}\")\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # Determine corpus words needing segmentation\n",
    "    # --------------------------------------------\n",
    "    # Only words not already in existing datasets...\n",
    "    candidate_words = sorted(list(corpus_words_all - existing_words))\n",
    "    print(f\"-> Initially identified {len(candidate_words):,} new corpus words (not in existing datasets).\")\n",
    "\n",
    "    # ...and whose corpus-root (first 4 graphemes) is in the intersection across all three datasets\n",
    "    words_to_segment = []\n",
    "    for w in candidate_words:\n",
    "        root = first_four_graphemes_root(w)\n",
    "        if root and root in common_roots_all_three:\n",
    "            words_to_segment.append(w)\n",
    "\n",
    "    print(f\"-> Filtered to {len(words_to_segment):,} words whose roots are common to all three datasets.\\n\")\n",
    "\n",
    "    return gold_df, words_to_segment\n",
    "\n",
    "def construct_few_shot_prompt(target_word, gold_df, num_examples):\n",
    "    \"\"\"\n",
    "    Creates a detailed prompt for the API with few-shot examples.\n",
    "    \"\"\"\n",
    "    examples = gold_df.sample(n=min(num_examples, len(gold_df)), random_state=random.randint(0, 10_000))\n",
    "\n",
    "    prompt = \"\"\"You are an expert in Quechua linguistics. Your task is to segment a given Quechua word into its constituent morphemes.\n",
    "    The morphemes should be separated by spaces. Please provide only the segmented output, with no additional explanation or commentary.\n",
    "    \n",
    "    Examples:\n",
    "    \"\"\"\n",
    "\n",
    "    for _, row in examples.iterrows():\n",
    "        # fallbacks in case Morph_split_str wasn't constructed above for some row\n",
    "        s = row.get('Morph_split_str', '')\n",
    "        if not isinstance(s, str) or not s.strip():\n",
    "            s = ''\n",
    "            if 'Morph_split' in row and isinstance(row['Morph_split'], str):\n",
    "                try:\n",
    "                    parsed = ast.literal_eval(row['Morph_split'])\n",
    "                    if isinstance(parsed, list):\n",
    "                        s = ' '.join(map(str, parsed))\n",
    "                except Exception:\n",
    "                    s = row['Morph_split']\n",
    "        prompt += f\"{row['Word']}: {s}\\n\"\n",
    "\n",
    "    prompt += f\"{target_word}: \"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "# =========================\n",
    "# MAIN EXECUTION\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "        print(\"FATAL ERROR: The 'OPENAI_API_KEY' environment variable is not set.\")\n",
    "        print(\"Please set it before running the script.\")\n",
    "    else:\n",
    "        # Step 1: Load gold data and all words\n",
    "        gold_df, _ = load_all_data()\n",
    "\n",
    "        # Load Sue Kalt dataset (gold_df)\n",
    "        gold_df = pd.read_parquet(GOLD_DF_FILE)\n",
    "        gold_df['Word'] = gold_df['word']\n",
    "        gold_df['morph'] = gold_df['morph'].str.replace('-', ' ')\n",
    "        gold_df['Morph_split_str'] = gold_df['morph']\n",
    "        gold_df['Morph_split'] = gold_df['morph'].str.split(' ')\n",
    "\n",
    "        # Step 2: Select few-shot examples\n",
    "        examples = gold_df.sample(\n",
    "            n=min(NUM_FEW_SHOT_EXAMPLES, len(gold_df)),\n",
    "            random_state=random.randint(0, 10_000)\n",
    "        )\n",
    "        few_shot_words = set(examples['Word'].tolist())\n",
    "\n",
    "        # Step 3: Pick 10 Sue Kalt words not in examples\n",
    "        sue_candidates = gold_df[~gold_df['Word'].isin(few_shot_words)]\n",
    "        words_to_segment = sue_candidates.sample(\n",
    "            n=min(WORDS_TO_PROCESS_LIMIT, len(sue_candidates)),\n",
    "            random_state=random.randint(0, 10_000)\n",
    "        )\n",
    "\n",
    "        print(f\"--- Selected {len(words_to_segment)} words from Sue Kalt not in few-shot examples. ---\")\n",
    "\n",
    "        # Step 4: Build few-shot prompt\n",
    "        prompt = \"\"\"You are an expert in Quechua linguistics. Your task is to segment a given Quechua word into its constituent morphemes.\n",
    "The morphemes should be separated by spaces. Please provide only the segmented output, with no additional explanation or commentary.\n",
    "\n",
    "Examples:\\n\"\"\"\n",
    "        for _, row in examples.iterrows():\n",
    "            s = row.get('Morph_split_str', '')\n",
    "            if not isinstance(s, str) or not s.strip():\n",
    "                s = ''\n",
    "                if 'Morph_split' in row and isinstance(row['Morph_split'], str):\n",
    "                    try:\n",
    "                        parsed = ast.literal_eval(row['Morph_split'])\n",
    "                        if isinstance(parsed, list):\n",
    "                            s = ' '.join(map(str, parsed))\n",
    "                    except Exception:\n",
    "                        s = row['Morph_split']\n",
    "            prompt += f\"{row['Word']}: {s}\\n\"\n",
    "\n",
    "        prompt += \"\\nWords to Segment:\\n\"\n",
    "        for word in tqdm(words_to_segment['Word'], desc=\"Segmenting Words\"):\n",
    "            prompt += f\"{word}\\n\"\n",
    "\n",
    "        # Step 5: Prepare gold segmentations string for evaluation\n",
    "        gold_segmentations_str = \"\\n\".join(\n",
    "            f\"{row['Word']}: {row['Morph_split_str']}\" for _, row in words_to_segment.iterrows()\n",
    "        )\n",
    "\n",
    "        # Output\n",
    "        print(\"\\n===== FEW-SHOT PROMPT =====\\n\")\n",
    "        print(prompt)\n",
    "        print(\"\\n===== GOLD SEGMENTATIONS (for eval) =====\\n\")\n",
    "        print(gold_segmentations_str)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7fc0d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
