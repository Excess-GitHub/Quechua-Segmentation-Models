{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02d08b1d",
   "metadata": {},
   "source": [
    "# Statistical Analysis of Quechua Morphology and Corpus\n",
    "\n",
    "Analyzes Zipf's law, Heaps' law, and Zipf-Mandelbrot fitting on Quechua data. Helps understand morpheme distributions and vocabulary growth patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28688487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import os\n",
    "import ast\n",
    "import math\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad5e34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATA_FOLDER = \"data\"\n",
    "FILE_PATH = os.path.join(DATA_FOLDER, \"qu_merged_dump.txt\")\n",
    "\n",
    "# Tokenization options\n",
    "lowercase = True\n",
    "keep_apostrophes = True\n",
    "\n",
    "# Analysis parameters\n",
    "tail_ignore = 50\n",
    "tail_min_len = 100\n",
    "heaps_stride = 50\n",
    "min_rank_for_fit = 1\n",
    "max_rank_for_fit = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a81d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1: Zipf's law on morphological tokens\n",
    "\n",
    "# Load gold standard data\n",
    "df = pd.read_parquet(os.path.join(DATA_FOLDER, \"Sue_kalt.parquet\"))\n",
    "df['Word'] = df['word']\n",
    "df['morph'] = df['morph'].str.replace('-', ' ')\n",
    "df['Morph_split_str'] = df['morph']\n",
    "df['Morph_split'] = df['morph'].str.split(' ')\n",
    "df = df[['Word', 'Morph_split', 'Morph_split_str']]\n",
    "\n",
    "# Extract all morpheme tokens\n",
    "tokens = []\n",
    "for toks in df[\"Morph_split\"]:\n",
    "    tokens.extend(t for t in toks if isinstance(t, str) and t.strip() != \"\")\n",
    "\n",
    "# Count frequencies\n",
    "freq = Counter(tokens)\n",
    "counts = np.array(sorted(freq.values(), reverse=True), dtype=np.int64)\n",
    "\n",
    "# Zipf plot for morphemes\n",
    "ranks = np.arange(1, len(counts) + 1, dtype=np.int64)\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.loglog(ranks, counts, marker='o', linestyle='none', markersize=3)\n",
    "plt.xlabel(\"Rank (log)\")\n",
    "plt.ylabel(\"Frequency (log)\")\n",
    "plt.title(\"Zipf plot for morph tokens (Morph_split)\")\n",
    "\n",
    "# Fit line on tail to estimate exponent\n",
    "k = min(50, len(counts) // 10 if len(counts) > 1000 else 10)\n",
    "if len(counts) > k + 10:\n",
    "    x = np.log(ranks[k:])\n",
    "    y = np.log(counts[k:])\n",
    "    slope, intercept = np.polyfit(x, y, 1)\n",
    "    y_fit = slope * x + intercept\n",
    "    plt.loglog(ranks[k:], np.exp(y_fit), linewidth=1)\n",
    "\n",
    "    s_est = -slope\n",
    "    print(f\"estimated zipf exponent s (tail fit): {s_est:.3f}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cce23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2: Corpus-level analysis\n",
    "\n",
    "# Tokenizer\n",
    "if keep_apostrophes:\n",
    "    TOKEN_RE = re.compile(r\"[^\\W\\d_]+(?:[''][^\\W\\d_]+)?\", flags=re.UNICODE)\n",
    "else:\n",
    "    TOKEN_RE = re.compile(r\"[^\\W\\d_]+\", flags=re.UNICODE)\n",
    "\n",
    "def iter_tokens_from_file(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            if lowercase:\n",
    "                line = line.lower()\n",
    "            for m in TOKEN_RE.finditer(line):\n",
    "                yield m.group(0)\n",
    "\n",
    "# Count tokens in corpus\n",
    "if not os.path.exists(FILE_PATH):\n",
    "    raise FileNotFoundError(f\"file not found: {FILE_PATH}\")\n",
    "\n",
    "freq = Counter()\n",
    "for tok in iter_tokens_from_file(FILE_PATH):\n",
    "    freq[tok] += 1\n",
    "\n",
    "total_tokens = sum(freq.values())\n",
    "vocab_size = len(freq)\n",
    "\n",
    "print(f\"file: {FILE_PATH}\")\n",
    "print(f\"total tokens: {total_tokens:,}\")\n",
    "print(f\"vocabulary size: {vocab_size:,}\")\n",
    "\n",
    "print(\"\\ntop 25 tokens:\")\n",
    "for i, (tok, c) in enumerate(freq.most_common(25), 1):\n",
    "    print(f\"{i:>2}. {tok}\\t{c}\")\n",
    "\n",
    "# Prepare for Zipf plot\n",
    "counts = np.array(sorted(freq.values(), reverse=True), dtype=np.int64)\n",
    "ranks = np.arange(1, len(counts) + 1, dtype=np.int64)\n",
    "\n",
    "# Zipf plot for corpus words\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.loglog(ranks, counts, marker='o', linestyle='none', markersize=3)\n",
    "plt.xlabel(\"Rank (log)\")\n",
    "plt.ylabel(\"Frequency (log)\")\n",
    "plt.title(\"Zipf plot — \" + os.path.basename(FILE_PATH))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7477156f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3: Advanced statistical modeling\n",
    "\n",
    "# Tokenizer (same as before)\n",
    "if keep_apostrophes:\n",
    "    TOKEN_RE = re.compile(r\"[^\\W\\d_]+(?:[''][^\\W\\d_]+)?\", flags=re.UNICODE)\n",
    "else:\n",
    "    TOKEN_RE = re.compile(r\"[^\\W\\d_]+\", flags=re.UNICODE)\n",
    "\n",
    "def iter_tokens(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            if lowercase:\n",
    "                line = line.lower()\n",
    "            for m in TOKEN_RE.finditer(line):\n",
    "                yield m.group(0)\n",
    "\n",
    "# Heaps' law: vocabulary growth\n",
    "def vocab_growth(path, stride=50):\n",
    "    \"\"\"Track vocabulary growth as we process tokens.\"\"\"\n",
    "    seen = set()\n",
    "    n_points, V_points = [], []\n",
    "    n = 0\n",
    "    for tok in iter_tokens(path):\n",
    "        n += 1\n",
    "        if tok not in seen:\n",
    "            seen.add(tok)\n",
    "        if (n % stride) == 0:\n",
    "            n_points.append(n)\n",
    "            V_points.append(len(seen))\n",
    "    if not n_points or n_points[-1] != n:\n",
    "        n_points.append(n)\n",
    "        V_points.append(len(seen))\n",
    "    return np.array(n_points, dtype=np.int64), np.array(V_points, dtype=np.int64)\n",
    "\n",
    "def fit_heaps_logls(n, V, min_n=1000, tail_frac=0.7):\n",
    "    \"\"\"Fit Heaps' law: V(n) = K * n^β in log-space.\"\"\"\n",
    "    mask = n >= max(1, min_n)\n",
    "    n_fit, V_fit = (n[mask], V[mask]) if mask.any() else (n, V)\n",
    "\n",
    "    if 0 < tail_frac < 1.0:\n",
    "        start = int((1 - tail_frac) * len(n_fit))\n",
    "        n_fit = n_fit[start:]\n",
    "        V_fit = V_fit[start:]\n",
    "\n",
    "    x = np.log(n_fit)\n",
    "    y = np.log(V_fit)\n",
    "    slope, intercept = np.polyfit(x, y, 1)\n",
    "    beta = slope\n",
    "    K = np.exp(intercept)\n",
    "    \n",
    "    yhat = slope * x + intercept\n",
    "    ss_res = np.sum((y - yhat)**2)\n",
    "    ss_tot = np.sum((y - np.mean(y))**2)\n",
    "    r2 = 1 - ss_res/ss_tot if ss_tot > 0 else np.nan\n",
    "    return beta, K, r2, (n_fit[0], n_fit[-1])\n",
    "\n",
    "# Zipf-Mandelbrot fitting\n",
    "def counts_from_file(path):\n",
    "    \"\"\"Count word frequencies and return sorted counts.\"\"\"\n",
    "    freq = Counter()\n",
    "    for tok in iter_tokens(path):\n",
    "        freq[tok] += 1\n",
    "    counts = np.array(sorted(freq.values(), reverse=True), dtype=np.int64)\n",
    "    return counts, freq\n",
    "\n",
    "def fit_zipf_mandelbrot(counts, rmin=1, rmax=None):\n",
    "    \"\"\"Fit Zipf-Mandelbrot model: frequency(r) = C / (r + q)^s\"\"\"\n",
    "    R = np.arange(1, len(counts) + 1, dtype=np.float64)\n",
    "    if rmax is None or rmax > len(R): rmax = len(R)\n",
    "    r_slice = slice(rmin-1, rmax)\n",
    "    r = R[r_slice]\n",
    "    c = counts[r_slice].astype(np.float64)\n",
    "\n",
    "    log_r = np.log(r)\n",
    "\n",
    "    def rmse_for(s, q):\n",
    "        rq = r + q\n",
    "        if np.any(rq <= 0):\n",
    "            return np.inf, None\n",
    "        log_rq = np.log(rq)\n",
    "        logC = np.mean(np.log(c) + s * log_rq)\n",
    "        logc_hat = logC - s * log_rq\n",
    "        rmse = np.sqrt(np.mean((np.log(c) - logc_hat)**2))\n",
    "        return rmse, logC\n",
    "\n",
    "    # Coarse grid search\n",
    "    s_grid = np.linspace(0.6, 1.6, 27)\n",
    "    q_grid = np.concatenate([np.linspace(0.0, 20.0, 21),\n",
    "                             np.linspace(25.0, 200.0, 8)])\n",
    "    best = (np.inf, None, None, None)\n",
    "    for s in s_grid:\n",
    "        for q in q_grid:\n",
    "            rmse, logC = rmse_for(s, q)\n",
    "            if rmse < best[0]:\n",
    "                best = (rmse, s, q, logC)\n",
    "\n",
    "    rmse0, s0, q0, logC0 = best\n",
    "\n",
    "    # Refined local search\n",
    "    def refine(s_c, q_c, s_step=0.05, q_step=2.0, n_iter=6):\n",
    "        best_rmse, best_s, best_q, best_logC = rmse0, s0, q0, logC0\n",
    "        for _ in range(n_iter):\n",
    "            improved = False\n",
    "            for s in np.linspace(best_s - s_step, best_s + s_step, 7):\n",
    "                for q in np.linspace(max(-0.9, best_q - q_step), best_q + q_step, 7):\n",
    "                    rmse, logC = rmse_for(s, q)\n",
    "                    if rmse < best_rmse:\n",
    "                        best_rmse, best_s, best_q, best_logC = rmse, s, q, logC\n",
    "                        improved = True\n",
    "            s_step *= 0.5\n",
    "            q_step *= 0.5\n",
    "            if not improved:\n",
    "                break\n",
    "        return best_rmse, best_s, best_q, best_logC\n",
    "\n",
    "    rmse, s, q, logC = refine(s0, q0)\n",
    "    C = float(np.exp(logC))\n",
    "    return s, q, C, rmse, (rmin, rmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f461f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute Heaps' law analysis\n",
    "if not os.path.exists(FILE_PATH):\n",
    "    raise FileNotFoundError(f\"file not found: {FILE_PATH}\")\n",
    "\n",
    "n_arr, V_arr = vocab_growth(FILE_PATH, stride=heaps_stride)\n",
    "beta, K, r2, (n0, n1) = fit_heaps_logls(n_arr, V_arr, min_n=2000, tail_frac=0.7)\n",
    "\n",
    "print(\"\\n=== heaps' law ===\")\n",
    "print(f\"β (slope): {beta:.4f}\")\n",
    "print(f\"K: {K:.4f}\")\n",
    "print(f\"R^2 (log–log fit): {r2:.4f}\")\n",
    "print(f\"fit range n in [{n0}, {n1}]   total tokens seen ~ {n_arr[-1]}   vocab ~ {V_arr[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0002d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute Zipf-Mandelbrot fitting\n",
    "counts, freq = counts_from_file(FILE_PATH)\n",
    "ranks = np.arange(1, len(counts)+1, dtype=np.int64)\n",
    "\n",
    "rmin = max(1, min_rank_for_fit)\n",
    "rmax = len(counts) if max_rank_for_fit is None else min(max_rank_for_fit, len(counts))\n",
    "s, q, C, rmse_log, (rf0, rf1) = fit_zipf_mandelbrot(counts, rmin=rmin, rmax=rmax)\n",
    "\n",
    "print(\"\\n=== zipf–mandelbrot ===\")\n",
    "print(f\"s: {s:.4f}\")\n",
    "print(f\"q: {q:.4f}\")\n",
    "print(f\"C: {C:.4e}\")\n",
    "print(f\"RMSE in log-frequency (fit ranks [{rf0}, {rf1}]): {rmse_log:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7effbe59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization plots\n",
    "\n",
    "# Heaps' law plot\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.loglog(n_arr, V_arr, marker='o', linestyle='none', markersize=2, label=\"Observed V(n)\")\n",
    "V_fit = K * (n_arr.astype(float) ** beta)\n",
    "plt.loglog(n_arr, V_fit, linewidth=1.5, label=f\"Fit: β={beta:.3f}, K={K:.2f}\")\n",
    "plt.xlabel(\"Tokens n (log)\")\n",
    "plt.ylabel(\"Vocabulary V(n) (log)\")\n",
    "plt.title(\"Heaps' Law on Quechua corpus\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Zipf curve with Zipf–Mandelbrot overlay\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.loglog(ranks, counts, marker='o', linestyle='none', markersize=2, label=\"Empirical counts\")\n",
    "r = ranks.astype(float)\n",
    "model = C / ((r + q) ** s)\n",
    "plt.loglog(r, model, linewidth=1.5, label=f\"Zipf–Mandelbrot fit (s={s:.3f}, q={q:.1f})\")\n",
    "plt.xlabel(\"Rank (log)\")\n",
    "plt.ylabel(\"Frequency (log)\")\n",
    "plt.title(\"Zipf curve with Zipf–Mandelbrot fit\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3464aa32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling Heaps exponent analysis\n",
    "win = 200\n",
    "\n",
    "x = np.log(n_arr.astype(float))\n",
    "y = np.log(V_arr.astype(float))\n",
    "\n",
    "roll_beta = np.full_like(x, np.nan, dtype=float)\n",
    "for i in range(win, len(x)):\n",
    "    xs, ys = x[i-win:i], y[i-win:i]\n",
    "    slope, _ = np.polyfit(xs, ys, 1)\n",
    "    roll_beta[i] = slope\n",
    "\n",
    "print(\"tail rolling β (last 5):\", np.round(roll_beta[-5:], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92333405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: save results\n",
    "# Uncomment to save analysis results\n",
    "\n",
    "# Save Heaps' Law parameters\n",
    "# heaps_results = {\n",
    "#     'beta': float(beta),\n",
    "#     'K': float(K),\n",
    "#     'r2': float(r2),\n",
    "#     'total_tokens': int(n_arr[-1]),\n",
    "#     'vocab_size': int(V_arr[-1])\n",
    "# }\n",
    "# import json\n",
    "# with open(os.path.join(DATA_FOLDER, 'heaps_law_results.json'), 'w') as f:\n",
    "#     json.dump(heaps_results, f, indent=2)\n",
    "\n",
    "# Save Zipf-Mandelbrot parameters\n",
    "# zipf_mandelbrot_results = {\n",
    "#     's': float(s),\n",
    "#     'q': float(q),\n",
    "#     'C': float(C),\n",
    "#     'rmse_log': float(rmse_log),\n",
    "#     'rank_range': [int(rf0), int(rf1)]\n",
    "# }\n",
    "# with open(os.path.join(DATA_FOLDER, 'zipf_mandelbrot_results.json'), 'w') as f:\n",
    "#     json.dump(zipf_mandelbrot_results, f, indent=2)\n",
    "\n",
    "# Save vocabulary growth data\n",
    "# vocab_growth_df = pd.DataFrame({\n",
    "#     'token_count': n_arr,\n",
    "#     'vocab_size': V_arr\n",
    "# })\n",
    "# vocab_growth_df.to_csv(os.path.join(DATA_FOLDER, 'vocab_growth_heaps.csv'), index=False)\n",
    "\n",
    "# Save word frequency distribution\n",
    "# freq_df = pd.DataFrame({\n",
    "#     'word': list(freq.keys()),\n",
    "#     'frequency': list(freq.values())\n",
    "# })\n",
    "# freq_df = freq_df.sort_values('frequency', ascending=False)\n",
    "# freq_df.to_csv(os.path.join(DATA_FOLDER, 'word_frequencies.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
