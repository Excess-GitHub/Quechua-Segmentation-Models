{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6521d754",
   "metadata": {},
   "source": [
    "# Transmorpher: Transformer-Based Morphology Parser\n",
    "\n",
    "Transformer encoder-decoder for Quechua morphological segmentation. Generates segmented words directly (e.g., 'pikunas' -> 'pi+kuna+s') using sequence-to-sequence architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb11de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import os\n",
    "import ast\n",
    "import json\n",
    "import hashlib\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# Data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "# Statistics\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "# ML\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce2f770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATA_FOLDER = \"data\"\n",
    "MODEL_NAME = \"transmorpher\"\n",
    "MODELS_FOLDER = f\"models_{MODEL_NAME}\"\n",
    "os.makedirs(DATA_FOLDER, exist_ok=True)\n",
    "os.makedirs(MODELS_FOLDER, exist_ok=True)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785491ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load gold standard data\n",
    "print(\"loading gold data...\")\n",
    "gold_df = pd.read_parquet(os.path.join(DATA_FOLDER, \"Sue_kalt.parquet\"))\n",
    "gold_df['Word'] = gold_df['word']\n",
    "gold_df['morph'] = gold_df['morph'].str.replace('-', ' ')\n",
    "gold_df['Morph_split_str'] = gold_df['morph']\n",
    "gold_df['Morph_split'] = gold_df['morph'].str.split(' ')\n",
    "gold_df = gold_df[['Word', 'Morph_split', 'Morph_split_str']]\n",
    "gold_df.drop_duplicates(subset='Word', keep='first', inplace=True)\n",
    "gold_df.dropna(subset=['Word'], inplace=True)\n",
    "print(f\"got {len(gold_df):,} gold examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35cc8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract basic features\n",
    "gold_df['num_morphemes'] = gold_df['Morph_split'].apply(len)\n",
    "gold_df['word_len'] = gold_df['Word'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b0cd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert morpheme splits to segmentation format with '+' separators\n",
    "gold_df['segmentation'] = gold_df['Morph_split_str'].str.replace(' ', '+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a5646c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of word length vs morpheme count\n",
    "heatmap_data = gold_df.groupby(['word_len', 'num_morphemes']).size().unstack(fill_value=0)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(heatmap_data, annot=True, fmt='d', cmap='YlGnBu')\n",
    "plt.title('Heatmap of Word Length vs. Morpheme Count')\n",
    "plt.xlabel('Number of Morphemes')\n",
    "plt.ylabel('Word Length (Characters)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78d1080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "x = gold_df['word_len']\n",
    "y = gold_df['num_morphemes']\n",
    "\n",
    "pearson_corr, pearson_p = pearsonr(x, y)\n",
    "spearman_corr, spearman_p = spearmanr(x, y)\n",
    "\n",
    "print(f\"pearson correlation: {pearson_corr:.3f} (p={pearson_p:.3e})\")\n",
    "print(f\"spearman correlation: {spearman_corr:.3f} (p={spearman_p:.3e})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7db973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression with outlier removal\n",
    "gold_df1 = gold_df.copy()\n",
    "print(f\"original size: {gold_df1.shape}\")\n",
    "\n",
    "X = gold_df1[['word_len']]\n",
    "y = gold_df1['num_morphemes']\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "gold_df1['predicted'] = model.predict(X)\n",
    "gold_df1['residual'] = gold_df1['num_morphemes'] - gold_df1['predicted']\n",
    "\n",
    "std_residual = gold_df1['residual'].std()\n",
    "filtered_df = gold_df1[np.abs(gold_df1['residual']) <= std_residual]\n",
    "print(f\"cleaned size: {filtered_df.shape}\")\n",
    "\n",
    "X_filtered = filtered_df[['word_len']]\n",
    "y_filtered = filtered_df['num_morphemes']\n",
    "\n",
    "model_filtered = LinearRegression()\n",
    "model_filtered.fit(X_filtered, y_filtered)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=filtered_df, x='word_len', y='num_morphemes', alpha=0.5, label='Filtered Data')\n",
    "plt.plot(X_filtered, model_filtered.predict(X_filtered), color='red', linewidth=2, label='Regression Line')\n",
    "plt.title('Optimized Linear Regression: Word Length vs Morpheme Count')\n",
    "plt.xlabel('Word Length (Characters)')\n",
    "plt.ylabel('Number of Morphemes')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "slope = model.coef_[0]\n",
    "intercept = model.intercept_\n",
    "print(f\"pre-refined regression: num_morphemes ≈ {slope:.2f} × word_len + {intercept:.2f}\")\n",
    "\n",
    "slope = model_filtered.coef_[0]\n",
    "intercept = model_filtered.intercept_\n",
    "print(f\"refined regression: num_morphemes ≈ {slope:.2f} × word_len + {intercept:.2f}\")\n",
    "\n",
    "r2_full = r2_score(y, gold_df1['predicted'])\n",
    "r2_filtered = r2_score(y_filtered, model_filtered.predict(X_filtered))\n",
    "print(f\"R2 (original): {r2_full:.3f}\")\n",
    "print(f\"R2 (filtered): {r2_filtered:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075637c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest regression\n",
    "gold_df2 = gold_df.copy()\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(gold_df2[['word_len']], gold_df2['num_morphemes'])\n",
    "\n",
    "gold_df2['predicted_rf'] = rf.predict(gold_df2[['word_len']])\n",
    "gold_df2['residual_rf'] = gold_df2['num_morphemes'] - gold_df2['predicted_rf']\n",
    "\n",
    "mse_full = mean_squared_error(gold_df2['num_morphemes'], gold_df2['predicted_rf'])\n",
    "mae_full = mean_absolute_error(gold_df2['num_morphemes'], gold_df2['predicted_rf'])\n",
    "r2_full = r2_score(gold_df2['num_morphemes'], gold_df2['predicted_rf'])\n",
    "\n",
    "std_residual = gold_df2['residual_rf'].std()\n",
    "filtered_df_rf = gold_df2[np.abs(gold_df2['residual_rf']) <= std_residual].copy()\n",
    "\n",
    "rf_filtered = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "X_filtered = filtered_df_rf[['word_len']]\n",
    "y_filtered = filtered_df_rf['num_morphemes']\n",
    "rf_filtered.fit(X_filtered, y_filtered)\n",
    "\n",
    "filtered_df_rf['predicted_rf'] = rf_filtered.predict(X_filtered)\n",
    "r2_filtered = r2_score(y_filtered, filtered_df_rf['predicted_rf'])\n",
    "mse_filtered = mean_squared_error(y_filtered, filtered_df_rf['predicted_rf'])\n",
    "mae_filtered = mean_absolute_error(y_filtered, filtered_df_rf['predicted_rf'])\n",
    "\n",
    "print(\"random forest (before outlier removal):\")\n",
    "print(f\"MSE: {mse_full:.3f}\")\n",
    "print(f\"MAE: {mae_full:.3f}\")\n",
    "print(f\"R²:  {r2_full:.3f}\")\n",
    "\n",
    "print(\"random forest (after outlier removal):\")\n",
    "print(f\"MSE: {mse_filtered:.3f}\")\n",
    "print(f\"MAE: {mae_filtered:.3f}\")\n",
    "print(f\"R²:  {r2_filtered:.3f}\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=filtered_df_rf['word_len'], y=filtered_df_rf['num_morphemes'], alpha=0.5, label='Filtered Data')\n",
    "sns.lineplot(x=filtered_df_rf['word_len'], y=filtered_df_rf['predicted_rf'], color='red', label='RF Prediction (Filtered)')\n",
    "plt.xlabel('Word Length (Characters)')\n",
    "plt.ylabel('Number of Morphemes')\n",
    "plt.title('Random Forest Regression (Filtered)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bca800e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial regression\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_poly = poly.fit_transform(gold_df2[['word_len']])\n",
    "model_poly = LinearRegression().fit(X_poly, gold_df2['num_morphemes'])\n",
    "preds_poly = model_poly.predict(X_poly)\n",
    "r2_before = r2_score(gold_df2['num_morphemes'], preds_poly)\n",
    "print(f\"polynomial regression R2 (before filtering): {r2_before:.3f}\")\n",
    "\n",
    "residuals_poly = gold_df2['num_morphemes'] - preds_poly\n",
    "std_resid_poly = residuals_poly.std()\n",
    "mask = np.abs(residuals_poly) <= std_resid_poly\n",
    "filtered_df_poly = gold_df2[mask].copy()\n",
    "\n",
    "X_filtered_poly = poly.fit_transform(filtered_df_poly[['word_len']])\n",
    "model_poly_filtered = LinearRegression().fit(X_filtered_poly, filtered_df_poly['num_morphemes'])\n",
    "preds_filtered = model_poly_filtered.predict(X_filtered_poly)\n",
    "r2_after = r2_score(filtered_df_poly['num_morphemes'], preds_filtered)\n",
    "print(f\"polynomial regression R2 (after filtering):  {r2_after:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2273fdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify outliers from all models\n",
    "linear_outliers = gold_df1[np.abs(gold_df1['residual']) > std_residual]\n",
    "rf_outliers = gold_df2[np.abs(gold_df2['residual_rf']) > std_residual]\n",
    "poly_outliers = gold_df2[np.abs(residuals_poly) > std_resid_poly]\n",
    "\n",
    "all_outliers = pd.concat([linear_outliers, rf_outliers, poly_outliers])\n",
    "all_outliers = all_outliers[['word_len', 'num_morphemes']].drop_duplicates()\n",
    "\n",
    "# Visualize outliers on heatmap\n",
    "heatmap_data = gold_df.groupby(['word_len', 'num_morphemes']).size().unstack(fill_value=0)\n",
    "outlier_coords = all_outliers[['word_len', 'num_morphemes']]\n",
    "outlier_coords = outlier_coords[\n",
    "    (outlier_coords['word_len'].isin(heatmap_data.index)) & \n",
    "    (outlier_coords['num_morphemes'].isin(heatmap_data.columns))\n",
    "]\n",
    "outlier_coords = outlier_coords.copy()\n",
    "outlier_coords['freq'] = outlier_coords.apply(\n",
    "    lambda row: heatmap_data.at[row['word_len'], row['num_morphemes']], axis=1\n",
    ")\n",
    "\n",
    "norm = plt.Normalize(vmin=heatmap_data.values.min(), vmax=heatmap_data.values.max())\n",
    "colors = cm.Purples_r(norm(outlier_coords['freq'].values))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.heatmap(heatmap_data, annot=True, fmt='d', cmap='Reds', cbar_kws={'label': 'Frequency'})\n",
    "plt.title('Heatmap of Word Length vs. Morpheme Count with Outliers')\n",
    "plt.xlabel('Number of Morphemes')\n",
    "plt.ylabel('Word Length (Characters)')\n",
    "\n",
    "for j, (_, row) in enumerate(outlier_coords.iterrows()):\n",
    "    plt.scatter(\n",
    "        x=row['num_morphemes'] + 0.5,\n",
    "        y=row['word_len'] + 0.5,\n",
    "        color=colors[j],\n",
    "        s=100,\n",
    "        marker='X',\n",
    "        linewidths=2,\n",
    "        label='Outlier' if j == 0 else \"\"\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db61dc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build character-level vocabularies\n",
    "special_tokens = ['<pad>', '<s>', '</s>']\n",
    "input_chars = sorted({ch for word in gold_df['Word'] for ch in word})\n",
    "output_chars = sorted({ch for seg in gold_df['segmentation'] for ch in seg})\n",
    "input_vocab = special_tokens + input_chars\n",
    "output_vocab = special_tokens + output_chars\n",
    "input2idx = {ch: idx for idx, ch in enumerate(input_vocab)}\n",
    "output2idx = {ch: idx for idx, ch in enumerate(output_vocab)}\n",
    "PAD_IN, START_IN, END_IN = input2idx['<pad>'], input2idx['<s>'], input2idx['</s>']\n",
    "PAD_OUT, START_OUT, END_OUT = output2idx['<pad>'], output2idx['<s>'], output2idx['</s>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212fd685",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuechuaSegDataset(Dataset):\n",
    "    \"\"\"Dataset for Quechua morphological segmentation.\"\"\"\n",
    "    def __init__(self, df, input2idx, output2idx, max_input_len=None, max_output_len=None):\n",
    "        self.words = df['Word'].tolist()\n",
    "        self.segs = df['segmentation'].tolist()\n",
    "        self.input2idx = input2idx\n",
    "        self.output2idx = output2idx\n",
    "        self.max_input_len = max_input_len or max(len(w) for w in self.words) + 2\n",
    "        self.max_output_len = max_output_len or max(len(s) for s in self.segs) + 2\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.words)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        word = self.words[idx]\n",
    "        seg = self.segs[idx]\n",
    "        src = [self.input2idx.get(ch, PAD_IN) for ch in word]\n",
    "        src = [START_IN] + src + [END_IN]\n",
    "        src += [PAD_IN] * (self.max_input_len - len(src))\n",
    "        tgt = [START_OUT] + [self.output2idx[ch] for ch in seg] + [END_OUT]\n",
    "        tgt += [PAD_OUT] * (self.max_output_len - len(tgt))\n",
    "        return torch.tensor(src), torch.tensor(tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd234e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/validation split\n",
    "dataset = QuechuaSegDataset(gold_df, input2idx, output2idx)\n",
    "n_train = int(0.8 * len(dataset))\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [n_train, len(dataset) - n_train])\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "print(f\"training: {len(train_dataset):,} samples\")\n",
    "print(f\"validation: {len(val_dataset):,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb24e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Sinusoidal positional encoding for Transformer models.\"\"\"\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(pos * div)\n",
    "        pe[:, 1::2] = torch.cos(pos * div)\n",
    "        pe = pe.unsqueeze(1)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0)]\n",
    "\n",
    "class MorphSegModel(nn.Module):\n",
    "    \"\"\"Transformer encoder-decoder for morphological segmentation.\"\"\"\n",
    "    def __init__(self, in_vocab, out_vocab, d_model=64, ff=128, heads=2, layers=1, drop=0.0):\n",
    "        super().__init__()\n",
    "        self.enc_embed = nn.Embedding(in_vocab, d_model, padding_idx=PAD_IN)\n",
    "        self.pos_enc = PositionalEncoding(d_model)\n",
    "        enc_layer = nn.TransformerEncoderLayer(d_model, heads, ff, drop, batch_first=False)\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, layers)\n",
    "\n",
    "        self.dec_embed = nn.Embedding(out_vocab, d_model, padding_idx=PAD_OUT)\n",
    "        self.pos_dec = PositionalEncoding(d_model)\n",
    "        dec_layer = nn.TransformerDecoderLayer(d_model, heads, ff, drop, batch_first=False)\n",
    "        self.decoder = nn.TransformerDecoder(dec_layer, layers)\n",
    "\n",
    "        self.out_proj = nn.Linear(d_model, out_vocab)\n",
    "\n",
    "    def forward(self, src, tgt, tgt_mask=None, src_key_padding_mask=None, tgt_key_padding_mask=None):\n",
    "        e_src = self.pos_enc(self.enc_embed(src))\n",
    "        memory = self.encoder(e_src, src_key_padding_mask=src_key_padding_mask)\n",
    "        \n",
    "        e_tgt = self.pos_dec(self.dec_embed(tgt))\n",
    "        out = self.decoder(e_tgt, memory, tgt_mask=tgt_mask,\n",
    "                           memory_key_padding_mask=src_key_padding_mask,\n",
    "                           tgt_key_padding_mask=tgt_key_padding_mask)\n",
    "        \n",
    "        return self.out_proj(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84048e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_id(d_model, ff, heads, layers, drop, epochs, batch_size, lr, in_vocab_size, out_vocab_size):\n",
    "    \"\"\"Hash training params to get unique model ID.\"\"\"\n",
    "    params_dict = {\n",
    "        'd_model': d_model,\n",
    "        'ff': ff,\n",
    "        'heads': heads,\n",
    "        'layers': layers,\n",
    "        'drop': drop,\n",
    "        'epochs': epochs,\n",
    "        'batch_size': batch_size,\n",
    "        'lr': lr,\n",
    "        'in_vocab_size': in_vocab_size,\n",
    "        'out_vocab_size': out_vocab_size\n",
    "    }\n",
    "    params_str = json.dumps(params_dict, sort_keys=True)\n",
    "    return hashlib.md5(params_str.encode()).hexdigest()[:16]\n",
    "\n",
    "def save_model_checkpoint(model, input2idx, output2idx, model_id, models_folder=MODELS_FOLDER):\n",
    "    \"\"\"Save model checkpoint.\"\"\"\n",
    "    model_dir = os.path.join(models_folder, model_id)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    checkpoint_path = os.path.join(model_dir, \"transformer_morph_seg.pt\")\n",
    "    torch.save({\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"input2idx\": input2idx,\n",
    "        \"output2idx\": output2idx\n",
    "    }, checkpoint_path)\n",
    "    \n",
    "    metadata_path = os.path.join(model_dir, \"metadata.json\")\n",
    "    with open(metadata_path, \"w\") as f:\n",
    "        json.dump({\n",
    "            'model_id': model_id,\n",
    "            'in_vocab_size': len(input2idx),\n",
    "            'out_vocab_size': len(output2idx),\n",
    "            'model_name': MODEL_NAME\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    print(f\"saved checkpoint to {model_dir}\")\n",
    "    return model_dir\n",
    "\n",
    "def load_model_checkpoint(model_id, models_folder=MODELS_FOLDER):\n",
    "    \"\"\"Load model checkpoint.\"\"\"\n",
    "    model_dir = os.path.join(models_folder, model_id)\n",
    "    checkpoint_path = os.path.join(model_dir, \"transformer_morph_seg.pt\")\n",
    "    \n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        return None\n",
    "    \n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    print(f\"loaded checkpoint from {model_dir}\")\n",
    "    return {\n",
    "        'model_state': checkpoint['model_state'],\n",
    "        'input2idx': checkpoint['input2idx'],\n",
    "        'output2idx': checkpoint['output2idx'],\n",
    "        'checkpoint_path': checkpoint_path,\n",
    "        'model_dir': model_dir\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5303f5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "D_MODEL = 64\n",
    "FF = 128\n",
    "HEADS = 2\n",
    "LAYERS = 1\n",
    "DROP = 0.0\n",
    "EPOCHS = 15\n",
    "BATCH_SIZE = 16\n",
    "LR = 1e-4\n",
    "\n",
    "# Generate model identifier\n",
    "model_id = generate_model_id(D_MODEL, FF, HEADS, LAYERS, DROP, EPOCHS, BATCH_SIZE, LR, \n",
    "                              len(input_vocab), len(output_vocab))\n",
    "\n",
    "# Try to load existing model\n",
    "print(f\"looking for model {model_id}...\")\n",
    "loaded = load_model_checkpoint(model_id, models_folder=MODELS_FOLDER)\n",
    "\n",
    "if loaded is not None:\n",
    "    print(f\"found it! loading from {loaded['model_dir']}\")\n",
    "    input2idx = loaded['input2idx']\n",
    "    output2idx = loaded['output2idx']\n",
    "    PAD_IN, START_IN, END_IN = input2idx['<pad>'], input2idx['<s>'], input2idx['</s>']\n",
    "    PAD_OUT, START_OUT, END_OUT = output2idx['<pad>'], output2idx['<s>'], output2idx['</s>']\n",
    "    model = MorphSegModel(len(input2idx), len(output2idx), d_model=D_MODEL, ff=FF, \n",
    "                          heads=HEADS, layers=LAYERS, drop=DROP).to(device)\n",
    "    model.load_state_dict(loaded['model_state'])\n",
    "    model.eval()\n",
    "    print(\"skipping training, model ready\")\n",
    "else:\n",
    "    print(f\"not found, training from scratch...\")\n",
    "    model = MorphSegModel(len(input_vocab), len(output_vocab), d_model=D_MODEL, ff=FF, \n",
    "                          heads=HEADS, layers=LAYERS, drop=DROP).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=PAD_OUT)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR, betas=(0.9, 0.98), eps=1e-9)\n",
    "    \n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for src_batch, tgt_batch in train_loader:\n",
    "            src_batch = src_batch.to(device)\n",
    "            tgt_batch = tgt_batch.to(device)\n",
    "\n",
    "            src_pad_mask = (src_batch == PAD_IN)\n",
    "            tgt_pad_mask = (tgt_batch == PAD_OUT)\n",
    "\n",
    "            src = src_batch.transpose(0, 1)\n",
    "            tgt = tgt_batch.transpose(0, 1)\n",
    "            tgt_input = tgt[:-1, :]\n",
    "            tgt_output = tgt[1:, :]\n",
    "\n",
    "            tgt_mask = nn.Transformer.generate_square_subsequent_mask(\n",
    "                           tgt_input.size(0)\n",
    "                       ).to(device)\n",
    "\n",
    "            logits = model(\n",
    "                src,\n",
    "                tgt_input,\n",
    "                tgt_mask=tgt_mask,\n",
    "                src_key_padding_mask=src_pad_mask,\n",
    "                tgt_key_padding_mask=tgt_pad_mask[:, :-1]\n",
    "            )\n",
    "            \n",
    "            loss = criterion(\n",
    "                logits.reshape(-1, logits.size(-1)),\n",
    "                tgt_output.reshape(-1)\n",
    "            )\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"epoch {epoch:02d} — train loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    save_model_checkpoint(model, input2idx, output2idx, model_id, models_folder=MODELS_FOLDER)\n",
    "    print(f\"\\ntraining done! model saved with ID: {model_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645749da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_word(word, model, in2idx, out2idx, idx2out, max_len=50, debug=False):\n",
    "    \"\"\"Segment a word using the trained Transformer model.\"\"\"\n",
    "    model.eval()\n",
    "    src_idx = [START_IN] + [in2idx.get(ch, PAD_IN) for ch in word] + [END_IN]\n",
    "    src = torch.tensor(src_idx).unsqueeze(1).to(device)\n",
    "    src_pad = (src.squeeze(1) == PAD_IN).unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        mem = model.pos_enc(model.enc_embed(src))\n",
    "        mem = model.encoder(mem, src_key_padding_mask=src_pad)\n",
    "    \n",
    "    out_ids = [START_OUT]\n",
    "    if debug:\n",
    "        print(f\"segmenting '{word}':\")\n",
    "        print(f\"  generated tokens so far: \", end=\"\")\n",
    "    \n",
    "    for step in range(max_len):\n",
    "        tgt = torch.tensor(out_ids).unsqueeze(1).to(device)\n",
    "        mask = nn.Transformer.generate_square_subsequent_mask(len(out_ids)).to(device)\n",
    "        with torch.no_grad():\n",
    "            dec_out = model.pos_dec(model.dec_embed(tgt))\n",
    "            dec = model.decoder(dec_out, mem, tgt_mask=mask, memory_key_padding_mask=src_pad)\n",
    "            logits = model.out_proj(dec)\n",
    "        \n",
    "        probs = torch.softmax(logits[-1, 0], dim=0)\n",
    "        top_probs, top_indices = torch.topk(probs, k=3)\n",
    "        \n",
    "        nxt = logits[-1, 0].argmax().item()\n",
    "        \n",
    "        if debug:\n",
    "            top_chars = [idx2out[idx.item()] for idx in top_indices]\n",
    "            print(f\"\\n  step {step+1}: top predictions = {list(zip(top_chars, top_probs.tolist()))}\")\n",
    "            print(f\"    -> selected: '{idx2out[nxt]}' (prob={probs[nxt].item():.4f})\")\n",
    "        \n",
    "        current_output = ''.join(idx2out[i] for i in out_ids[1:])\n",
    "        current_chars = len(current_output.replace('+', ''))\n",
    "        input_chars = len(word)\n",
    "        min_expected_chars = input_chars\n",
    "        \n",
    "        if nxt == END_OUT:\n",
    "            end_prob = probs[END_OUT].item()\n",
    "            if current_chars >= min_expected_chars or end_prob > 0.8:\n",
    "                if debug:\n",
    "                    print(f\"  stopped at END_OUT token (chars: {current_chars}/{min_expected_chars}, prob: {end_prob:.4f})\")\n",
    "                break\n",
    "            else:\n",
    "                if debug:\n",
    "                    print(f\"  END_OUT predicted but too early (chars: {current_chars}/{min_expected_chars}), forcing continuation...\")\n",
    "                for idx in top_indices:\n",
    "                    if idx.item() != END_OUT:\n",
    "                        nxt = idx.item()\n",
    "                        if debug:\n",
    "                            print(f\"    -> forced selection: '{idx2out[nxt]}' (prob={probs[nxt].item():.4f})\")\n",
    "                        break\n",
    "        \n",
    "        out_ids.append(nxt)\n",
    "        \n",
    "        if debug:\n",
    "            current_seg = ''.join(idx2out[i] for i in out_ids[1:])\n",
    "            print(f\"    current segmentation: '{current_seg}'\")\n",
    "    \n",
    "    result = ''.join(idx2out[i] for i in out_ids[1:])\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"  final result: '{result}'\")\n",
    "        print(f\"  expected length check: input '{word}' ({len(word)} chars) -> output '{result}' ({len(result.replace('+', ''))} chars)\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Build reverse vocabulary mapping\n",
    "idx2output = {idx: ch for ch, idx in output2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dbb6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_kfold_cross_validation(\n",
    "    df,\n",
    "    n_folds=5,\n",
    "    d_model=64,\n",
    "    ff=128,\n",
    "    heads=2,\n",
    "    layers=1,\n",
    "    drop=0.0,\n",
    "    epochs=15,\n",
    "    batch_size=16,\n",
    "    lr=1e-4,\n",
    "    random_state=42,\n",
    "    device=device\n",
    "):\n",
    "    \"\"\"K-fold cross-validation for more robust evaluation.\"\"\"\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"K-FOLD CV (k={n_folds}) WITH TRANSFORMER\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    \n",
    "    kfold = KFold(n_splits=n_folds, shuffle=True, random_state=random_state)\n",
    "    indices = np.arange(len(df))\n",
    "    \n",
    "    fold_results = []\n",
    "    all_metrics = {\n",
    "        'val_loss': [],\n",
    "        'exact_match': []\n",
    "    }\n",
    "    \n",
    "    for fold_idx, (train_indices, val_indices) in enumerate(kfold.split(indices), 1):\n",
    "        print(f\"\\n--- fold {fold_idx}/{n_folds} ---\")\n",
    "        print(f\"train: {len(train_indices)}, val: {len(val_indices)}\")\n",
    "        \n",
    "        train_df_fold = df.iloc[train_indices].reset_index(drop=True)\n",
    "        val_df_fold = df.iloc[val_indices].reset_index(drop=True)\n",
    "        \n",
    "        print(f\"  building vocabularies from training fold...\")\n",
    "        special_tokens = ['<pad>', '<s>', '</s>']\n",
    "        input_chars_fold = sorted({ch for word in train_df_fold['Word'] for ch in word})\n",
    "        output_chars_fold = sorted({ch for seg in train_df_fold['segmentation'] for ch in seg})\n",
    "        input_vocab_fold = special_tokens + input_chars_fold\n",
    "        output_vocab_fold = special_tokens + output_chars_fold\n",
    "        input2idx_fold = {ch: idx for idx, ch in enumerate(input_vocab_fold)}\n",
    "        output2idx_fold = {ch: idx for idx, ch in enumerate(output_vocab_fold)}\n",
    "        PAD_IN_FOLD = input2idx_fold['<pad>']\n",
    "        START_IN_FOLD = input2idx_fold['<s>']\n",
    "        END_IN_FOLD = input2idx_fold['</s>']\n",
    "        PAD_OUT_FOLD = output2idx_fold['<pad>']\n",
    "        START_OUT_FOLD = output2idx_fold['<s>']\n",
    "        END_OUT_FOLD = output2idx_fold['</s>']\n",
    "        \n",
    "        train_dataset_fold = QuechuaSegDataset(train_df_fold, input2idx_fold, output2idx_fold)\n",
    "        val_dataset_fold = QuechuaSegDataset(val_df_fold, input2idx_fold, output2idx_fold)\n",
    "        train_loader_fold = DataLoader(train_dataset_fold, batch_size=batch_size, shuffle=True)\n",
    "        val_loader_fold = DataLoader(val_dataset_fold, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        model_fold = MorphSegModel(\n",
    "            len(input_vocab_fold), \n",
    "            len(output_vocab_fold), \n",
    "            d_model=d_model, \n",
    "            ff=ff, \n",
    "            heads=heads, \n",
    "            layers=layers, \n",
    "            drop=drop\n",
    "        ).to(device)\n",
    "        \n",
    "        criterion_fold = nn.CrossEntropyLoss(ignore_index=PAD_OUT_FOLD)\n",
    "        optimizer_fold = optim.Adam(model_fold.parameters(), lr=lr, betas=(0.9, 0.98), eps=1e-9)\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        best_epoch = 0\n",
    "        best_exact_match = 0.0\n",
    "        \n",
    "        for epoch in range(1, epochs + 1):\n",
    "            model_fold.train()\n",
    "            total_loss = 0.0\n",
    "            for src_batch, tgt_batch in train_loader_fold:\n",
    "                src_batch = src_batch.to(device)\n",
    "                tgt_batch = tgt_batch.to(device)\n",
    "                \n",
    "                src_pad_mask = (src_batch == PAD_IN_FOLD)\n",
    "                tgt_pad_mask = (tgt_batch == PAD_OUT_FOLD)\n",
    "                \n",
    "                src = src_batch.transpose(0, 1)\n",
    "                tgt = tgt_batch.transpose(0, 1)\n",
    "                tgt_input = tgt[:-1, :]\n",
    "                tgt_output = tgt[1:, :]\n",
    "                \n",
    "                tgt_mask = nn.Transformer.generate_square_subsequent_mask(\n",
    "                    tgt_input.size(0)\n",
    "                ).to(device)\n",
    "                \n",
    "                logits = model_fold(\n",
    "                    src,\n",
    "                    tgt_input,\n",
    "                    tgt_mask=tgt_mask,\n",
    "                    src_key_padding_mask=src_pad_mask,\n",
    "                    tgt_key_padding_mask=tgt_pad_mask[:, :-1]\n",
    "                )\n",
    "                \n",
    "                loss = criterion_fold(\n",
    "                    logits.reshape(-1, logits.size(-1)),\n",
    "                    tgt_output.reshape(-1)\n",
    "                )\n",
    "                \n",
    "                optimizer_fold.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer_fold.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            train_loss = total_loss / len(train_loader_fold)\n",
    "            \n",
    "            model_fold.eval()\n",
    "            val_loss = 0.0\n",
    "            exact_matches = 0\n",
    "            total_val = 0\n",
    "            \n",
    "            idx2output_fold = {idx: ch for ch, idx in output2idx_fold.items()}\n",
    "            \n",
    "            def segment_word_fold(word, model, in2idx, out2idx, idx2out, max_len=50):\n",
    "                \"\"\"Fold-specific version of segment_word.\"\"\"\n",
    "                model.eval()\n",
    "                src_idx = [START_IN_FOLD] + [in2idx.get(ch, PAD_IN_FOLD) for ch in word] + [END_IN_FOLD]\n",
    "                src = torch.tensor(src_idx).unsqueeze(1).to(device)\n",
    "                src_pad = (src.squeeze(1) == PAD_IN_FOLD).unsqueeze(0)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    mem = model.pos_enc(model.enc_embed(src))\n",
    "                    mem = model.encoder(mem, src_key_padding_mask=src_pad)\n",
    "                \n",
    "                out_ids = [START_OUT_FOLD]\n",
    "                for step in range(max_len):\n",
    "                    tgt = torch.tensor(out_ids).unsqueeze(1).to(device)\n",
    "                    mask = nn.Transformer.generate_square_subsequent_mask(len(out_ids)).to(device)\n",
    "                    with torch.no_grad():\n",
    "                        dec_out = model.pos_dec(model.dec_embed(tgt))\n",
    "                        dec = model.decoder(dec_out, mem, tgt_mask=mask, memory_key_padding_mask=src_pad)\n",
    "                        logits = model.out_proj(dec)\n",
    "                    \n",
    "                    probs = torch.softmax(logits[-1, 0], dim=0)\n",
    "                    nxt = logits[-1, 0].argmax().item()\n",
    "                    \n",
    "                    current_output = ''.join(idx2out[i] for i in out_ids[1:])\n",
    "                    current_chars = len(current_output.replace('+', ''))\n",
    "                    input_chars = len(word)\n",
    "                    min_expected_chars = input_chars\n",
    "                    \n",
    "                    if nxt == END_OUT_FOLD:\n",
    "                        end_prob = probs[END_OUT_FOLD].item()\n",
    "                        if current_chars >= min_expected_chars or end_prob > 0.8:\n",
    "                            break\n",
    "                        else:\n",
    "                            top_probs, top_indices = torch.topk(probs, k=3)\n",
    "                            for idx in top_indices:\n",
    "                                if idx.item() != END_OUT_FOLD:\n",
    "                                    nxt = idx.item()\n",
    "                                    break\n",
    "                    \n",
    "                    out_ids.append(nxt)\n",
    "                \n",
    "                result = ''.join(idx2out[i] for i in out_ids[1:])\n",
    "                return result\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for src_batch, tgt_batch in val_loader_fold:\n",
    "                    src_batch = src_batch.to(device)\n",
    "                    tgt_batch = tgt_batch.to(device)\n",
    "                    \n",
    "                    src_pad_mask = (src_batch == PAD_IN_FOLD)\n",
    "                    tgt_pad_mask = (tgt_batch == PAD_OUT_FOLD)\n",
    "                    \n",
    "                    src = src_batch.transpose(0, 1)\n",
    "                    tgt = tgt_batch.transpose(0, 1)\n",
    "                    tgt_input = tgt[:-1, :]\n",
    "                    tgt_output = tgt[1:, :]\n",
    "                    \n",
    "                    tgt_mask = nn.Transformer.generate_square_subsequent_mask(\n",
    "                        tgt_input.size(0)\n",
    "                    ).to(device)\n",
    "                    \n",
    "                    logits = model_fold(\n",
    "                        src,\n",
    "                        tgt_input,\n",
    "                        tgt_mask=tgt_mask,\n",
    "                        src_key_padding_mask=src_pad_mask,\n",
    "                        tgt_key_padding_mask=tgt_pad_mask[:, :-1]\n",
    "                    )\n",
    "                    \n",
    "                    loss = criterion_fold(\n",
    "                        logits.reshape(-1, logits.size(-1)),\n",
    "                        tgt_output.reshape(-1)\n",
    "                    )\n",
    "                    val_loss += loss.item()\n",
    "                \n",
    "                for i in range(len(val_dataset_fold)):\n",
    "                    word = val_df_fold.iloc[i]['Word']\n",
    "                    gold_seg = val_df_fold.iloc[i]['segmentation']\n",
    "                    \n",
    "                    seg_str = segment_word_fold(\n",
    "                        word, model_fold, input2idx_fold, output2idx_fold, \n",
    "                        idx2output_fold, max_len=50\n",
    "                    )\n",
    "                    \n",
    "                    if seg_str == gold_seg:\n",
    "                        exact_matches += 1\n",
    "                    total_val += 1\n",
    "            \n",
    "            val_loss = val_loss / len(val_loader_fold)\n",
    "            exact_match_rate = exact_matches / total_val if total_val > 0 else 0.0\n",
    "            \n",
    "            print(f\"  ep {epoch:02d} | train_loss={train_loss:.4f}  val_loss={val_loss:.4f}  exact_match={exact_match_rate:.3f}\")\n",
    "            \n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_exact_match = exact_match_rate\n",
    "                best_epoch = epoch\n",
    "        \n",
    "        print(f\"\\n  best epoch: {best_epoch}\")\n",
    "        print(f\"  best validation: loss={best_val_loss:.4f}  exact_match={best_exact_match:.3f}\")\n",
    "        \n",
    "        fold_results.append({\n",
    "            'fold': fold_idx,\n",
    "            'val_loss': best_val_loss,\n",
    "            'exact_match': best_exact_match,\n",
    "            'best_epoch': best_epoch\n",
    "        })\n",
    "        \n",
    "        all_metrics['val_loss'].append(best_val_loss)\n",
    "        all_metrics['exact_match'].append(best_exact_match)\n",
    "    \n",
    "    mean_metrics = {k: np.mean(v) for k, v in all_metrics.items()}\n",
    "    std_metrics = {k: np.std(v) for k, v in all_metrics.items()}\n",
    "    best_fold_idx = min(range(len(fold_results)), key=lambda i: fold_results[i]['val_loss'])\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(\"CV SUMMARY\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    for r in fold_results:\n",
    "        print(f\"  fold {r['fold']}: loss={r['val_loss']:.4f}, exact_match={r['exact_match']:.3f}\")\n",
    "    \n",
    "    print(f\"\\nmean +/- std over {n_folds} folds:\")\n",
    "    print(f\"  validation loss:   {mean_metrics['val_loss']:.4f} +/- {std_metrics['val_loss']:.4f}\")\n",
    "    print(f\"  exact match rate:  {mean_metrics['exact_match']:.3f} +/- {std_metrics['exact_match']:.3f}\")\n",
    "    print(f\"\\nbest fold: {fold_results[best_fold_idx]['fold']} \"\n",
    "          f\"(loss: {fold_results[best_fold_idx]['val_loss']:.4f}, \"\n",
    "          f\"exact_match: {fold_results[best_fold_idx]['exact_match']:.3f})\")\n",
    "    print(f\"{'=' * 80}\\n\")\n",
    "    \n",
    "    return {\n",
    "        'fold_results': fold_results,\n",
    "        'mean_metrics': mean_metrics,\n",
    "        'std_metrics': std_metrics,\n",
    "        'best_fold_idx': best_fold_idx,\n",
    "        'all_metrics': all_metrics\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91cf062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run k-fold cross-validation\n",
    "kfold_results = run_kfold_cross_validation(\n",
    "    df=gold_df,\n",
    "    n_folds=5,\n",
    "    d_model=D_MODEL,\n",
    "    ff=FF,\n",
    "    heads=HEADS,\n",
    "    layers=LAYERS,\n",
    "    drop=DROP,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    lr=LR,\n",
    "    random_state=42,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"\\navg exact match rate: {kfold_results['mean_metrics']['exact_match']:.3f} +/- {kfold_results['std_metrics']['exact_match']:.3f}\")\n",
    "print(f\"avg validation loss: {kfold_results['mean_metrics']['val_loss']:.4f} +/- {kfold_results['std_metrics']['val_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2700acce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_segmentation(seg_str):\n",
    "    \"\"\"Parse segmented string into list of morphemes.\"\"\"\n",
    "    if not seg_str:\n",
    "        return []\n",
    "    return seg_str.split('+')\n",
    "\n",
    "def is_correct_prediction(predicted, gold_variants):\n",
    "    \"\"\"Check if predicted segmentation matches any gold variant.\"\"\"\n",
    "    if gold_variants is None:\n",
    "        return False\n",
    "    \n",
    "    if isinstance(gold_variants, np.ndarray):\n",
    "        gold_variants = gold_variants.tolist()\n",
    "    \n",
    "    if isinstance(gold_variants, list):\n",
    "        normalized = []\n",
    "        for variant in gold_variants:\n",
    "            if isinstance(variant, np.ndarray):\n",
    "                normalized.append(variant.tolist())\n",
    "            elif isinstance(variant, list):\n",
    "                normalized.append([item.tolist() if isinstance(item, np.ndarray) else item for item in variant])\n",
    "            else:\n",
    "                normalized.append(variant)\n",
    "        gold_variants = normalized\n",
    "    \n",
    "    return any(predicted == variant for variant in gold_variants)\n",
    "\n",
    "def split_count_metrics(predicted_segments, gold_variants):\n",
    "    \"\"\"Compute split-count accuracy variants.\"\"\"\n",
    "    pred_count = len(predicted_segments)\n",
    "    \n",
    "    if gold_variants is None:\n",
    "        return {\"Exact\": False, \"+1\": False, \"-1\": False, \"±1\": False}\n",
    "    \n",
    "    if isinstance(gold_variants, np.ndarray):\n",
    "        gold_variants = gold_variants.tolist()\n",
    "    \n",
    "    if isinstance(gold_variants, list):\n",
    "        normalized = []\n",
    "        for variant in gold_variants:\n",
    "            if isinstance(variant, np.ndarray):\n",
    "                normalized.append(variant.tolist())\n",
    "            elif isinstance(variant, list):\n",
    "                normalized.append([item.tolist() if isinstance(item, np.ndarray) else item for item in variant])\n",
    "            else:\n",
    "                normalized.append(variant)\n",
    "        gold_variants = normalized\n",
    "    \n",
    "    gold_counts = [len(gold) for gold in gold_variants]\n",
    "\n",
    "    exact = any(pred_count == g for g in gold_counts)\n",
    "    plus1 = any(pred_count == g + 1 for g in gold_counts)\n",
    "    minus1 = any(pred_count == g - 1 for g in gold_counts)\n",
    "    pm1 = any(abs(pred_count - g) <= 1 for g in gold_counts)\n",
    "\n",
    "    return {\"Exact\": exact, \"+1\": plus1, \"-1\": minus1, \"±1\": pm1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3216e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "print(\"loading test data...\")\n",
    "test_df = pd.read_parquet(os.path.join(DATA_FOLDER, \"cleaned_data_df.parquet\"))\n",
    "print(f\"loaded {len(test_df):,} test examples\")\n",
    "\n",
    "# Build reverse vocabulary mapping\n",
    "idx2output = {idx: ch for ch, idx in output2idx.items()}\n",
    "\n",
    "# Evaluate on test set\n",
    "records = []\n",
    "all_words = test_df[\"Word\"].tolist()\n",
    "\n",
    "print(\"predicting segmentations for test words...\")\n",
    "for word in all_words:\n",
    "    seg_str = segment_word(word, model, input2idx, output2idx, idx2output)\n",
    "    predicted_segments = parse_segmentation(seg_str)\n",
    "    \n",
    "    gold_variants = test_df[test_df[\"Word\"] == word][\"Gold\"].iloc[0] if len(test_df[test_df[\"Word\"] == word]) > 0 else []\n",
    "    \n",
    "    correct_exact = is_correct_prediction(predicted_segments, gold_variants)\n",
    "    split_metrics = split_count_metrics(predicted_segments, gold_variants)\n",
    "    \n",
    "    records.append({\n",
    "        \"Word\": word,\n",
    "        \"Prediction\": predicted_segments,\n",
    "        \"Gold\": gold_variants,\n",
    "        \"CorrectExactSeg\": correct_exact,\n",
    "        \"CorrectSplitCount\": split_metrics[\"Exact\"],\n",
    "        \"SplitCount+1\": split_metrics[\"+1\"],\n",
    "        \"SplitCount-1\": split_metrics[\"-1\"],\n",
    "        \"SplitCount±1\": split_metrics[\"±1\"],\n",
    "        \"OverlapExactAndSplit\": correct_exact and split_metrics[\"Exact\"]\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(records)\n",
    "\n",
    "# Compute aggregate metrics\n",
    "accuracy = results_df[\"CorrectExactSeg\"].mean()\n",
    "split_exact_acc = results_df[\"CorrectSplitCount\"].mean()\n",
    "split_plus1_acc = results_df[\"SplitCount+1\"].mean()\n",
    "split_minus1_acc = results_df[\"SplitCount-1\"].mean()\n",
    "split_pm1_acc = results_df[\"SplitCount±1\"].mean()\n",
    "overlap_accuracy = results_df[\"OverlapExactAndSplit\"].mean()\n",
    "\n",
    "print(f\"\\n=== evaluation results ===\")\n",
    "print(f\"exact segmentation accuracy: {accuracy:.4f}\")\n",
    "print(f\"\\n=== split-count metrics ===\")\n",
    "print(f\"split-count (exact):          {split_exact_acc:.4f}\")\n",
    "print(f\"split-count (+1):             {split_plus1_acc:.4f}\")\n",
    "print(f\"split-count (−1):              {split_minus1_acc:.4f}\")\n",
    "print(f\"split-count (±1):              {split_pm1_acc:.4f}\")\n",
    "print(f\"overlap (exact ∩ split):      {overlap_accuracy:.4f}\")\n",
    "\n",
    "# Save results\n",
    "results_output_path = os.path.join(DATA_FOLDER, \"transformer_eval_results.csv\")\n",
    "results_df.to_csv(results_output_path, index=False)\n",
    "print(f\"\\nevaluation results saved to {results_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aca54f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example segmentations\n",
    "test_words = [\"pikunas\", \"rikuchkani\", \"ñichkanchus\"]\n",
    "print(\"example segmentations:\")\n",
    "for word in test_words:\n",
    "    segmented = segment_word(word, model, input2idx, output2idx, idx2output)\n",
    "    print(f\"  {word} -> {segmented}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
