{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "aa24b801",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DT-LSTM-MARKOV FILTER: MORPHOLOGY PARSER WITH DECISION TREE PRIORS\n",
    "===================================================================\n",
    "\n",
    "This notebook implements a morphology parser for Quechua that combines:\n",
    "1. BiLSTM neural network for boundary prediction\n",
    "2. Decision Tree (DT) priors based on token-window features\n",
    "3. Privileged knowledge (K-teacher) for regularization\n",
    "\n",
    "The parser segments Quechua words into morphemes by predicting boundary positions\n",
    "between tokens. It uses:\n",
    "- Gold standard data (Sue Kalt dataset) as the base training data\n",
    "- Decision Tree classifier trained on token-window features to provide priors\n",
    "- K-teacher regularization to improve generalization\n",
    "\n",
    "Key Differences from Markov-LSTM-MarkovFilter:\n",
    "- Uses Decision Trees instead of HMM for prior generation\n",
    "- DT priors are based on local token-window features (left/right context)\n",
    "- HMM priors (in Markov-LSTM) use suffix patterns and forward-backward algorithm\n",
    "\n",
    "Key Features:\n",
    "- Model checkpointing: saves/loads models to avoid retraining\n",
    "- Decision Tree caching: saves/loads DT priors to avoid retraining\n",
    "- Comprehensive evaluation metrics (precision, recall, F1, exact match)\n",
    "- Token-window feature extraction for DT prior\n",
    "\n",
    "All data is read from the 'data' folder and models are saved to the 'models_DT-LSTM-MarkovFilter' folder.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cce08699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading gold standard data...\n",
      "Loaded 6,896 gold standard examples\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# DATA FOLDER CONFIGURATION\n",
    "# =========================\n",
    "# All data files should be read from and saved to the data folder\n",
    "DATA_FOLDER = \"data\"\n",
    "\n",
    "# Model folder named after this notebook\n",
    "MODEL_NAME = \"DT-LSTM-MarkovFilter\"\n",
    "MODELS_FOLDER = f\"models_{MODEL_NAME}\"\n",
    "\n",
    "# Create models folder if it doesn't exist\n",
    "os.makedirs(MODELS_FOLDER, exist_ok=True)\n",
    "\n",
    "# =========================\n",
    "# LOAD GOLD STANDARD DATA\n",
    "# =========================\n",
    "# The gold standard dataset contains high-quality morphological segmentations\n",
    "# This is the base training data\n",
    "print(\"Loading gold standard data...\")\n",
    "gold_df = pd.read_parquet(os.path.join(DATA_FOLDER, \"Sue_kalt.parquet\"))\n",
    "gold_df['Word'] = gold_df['word']\n",
    "gold_df['morph'] = gold_df['morph'].str.replace('-', ' ')  # Normalize separators\n",
    "gold_df['Morph_split_str'] = gold_df['morph']  # String version\n",
    "gold_df['Morph_split'] = gold_df['morph'].str.split(' ')  # List version\n",
    "gold_df = gold_df[['Word', 'Morph_split', 'Morph_split_str']]\n",
    "gold_df.drop_duplicates(subset='Word', keep='first', inplace=True)\n",
    "gold_df.dropna(subset=['Word'], inplace=True)\n",
    "print(f\"Loaded {len(gold_df):,} gold standard examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "69347c29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Morph_split</th>\n",
       "      <th>Morph_split_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cementerioman</td>\n",
       "      <td>[cementerio, man]</td>\n",
       "      <td>cementerio man</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kawsachkananta</td>\n",
       "      <td>[kawsa, chka, na, n, ta]</td>\n",
       "      <td>kawsa chka na n ta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mañakunpis</td>\n",
       "      <td>[maña, ku, n, pis]</td>\n",
       "      <td>maña ku n pis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>imaynapichus</td>\n",
       "      <td>[imayna, pi, chus]</td>\n",
       "      <td>imayna pi chus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>qipiyuq</td>\n",
       "      <td>[qipi, yuq]</td>\n",
       "      <td>qipi yuq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Quispepis</td>\n",
       "      <td>[Quispe, pis]</td>\n",
       "      <td>Quispe pis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ñichkanmanchá</td>\n",
       "      <td>[ñi, chka, nman, chá]</td>\n",
       "      <td>ñi chka nman chá</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>qukuni</td>\n",
       "      <td>[qu, ku, ni]</td>\n",
       "      <td>qu ku ni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dejasunpunichu</td>\n",
       "      <td>[deja, sun, puni, chu]</td>\n",
       "      <td>deja sun puni chu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>phutikunki</td>\n",
       "      <td>[phuti, ku, nki]</td>\n",
       "      <td>phuti ku nki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>kawsakunman</td>\n",
       "      <td>[kawsa, ku, nman]</td>\n",
       "      <td>kawsa ku nman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>yanapasqayki</td>\n",
       "      <td>[yanapa, sqayki]</td>\n",
       "      <td>yanapa sqayki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Pachamamaman</td>\n",
       "      <td>[Pachamama, man]</td>\n",
       "      <td>Pachamama man</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>pagawanki</td>\n",
       "      <td>[paga, wa, nki]</td>\n",
       "      <td>paga wa nki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>qhawanakun</td>\n",
       "      <td>[qhawa, naku, n]</td>\n",
       "      <td>qhawa naku n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>paypis</td>\n",
       "      <td>[pay, pis]</td>\n",
       "      <td>pay pis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>qhurasqita</td>\n",
       "      <td>[qhura, sqa, it]</td>\n",
       "      <td>qhura sqa it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>defendekusaq</td>\n",
       "      <td>[defende, ku, saq]</td>\n",
       "      <td>defende ku saq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>parlaspaqa</td>\n",
       "      <td>[parla, spa, qa]</td>\n",
       "      <td>parla spa qa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>tinqan</td>\n",
       "      <td>[tinqa, n]</td>\n",
       "      <td>tinqa n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>kutuykapun</td>\n",
       "      <td>[kutu, yka, pu, n]</td>\n",
       "      <td>kutu yka pu n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>qurikun</td>\n",
       "      <td>[qu, ri, ku, n]</td>\n",
       "      <td>qu ri ku n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>tortugapunichu</td>\n",
       "      <td>[tortuga, puni, chu]</td>\n",
       "      <td>tortuga puni chu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>qipirisqallataq</td>\n",
       "      <td>[qipi, ri, sqa, lla, taq]</td>\n",
       "      <td>qipi ri sqa lla taq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>sigue</td>\n",
       "      <td>[sigue]</td>\n",
       "      <td>sigue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>ratullachu</td>\n",
       "      <td>[ratu, lla, chu]</td>\n",
       "      <td>ratu lla chu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>tulluta</td>\n",
       "      <td>[tullu, ta]</td>\n",
       "      <td>tullu ta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>wawasninku</td>\n",
       "      <td>[wawa, s, ni, nku]</td>\n",
       "      <td>wawa s ni nku</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>ñawisitunta</td>\n",
       "      <td>[ñawi, situ, n, ta]</td>\n",
       "      <td>ñawi situ n ta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>nipuni</td>\n",
       "      <td>[ni, puni]</td>\n",
       "      <td>ni puni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>mirananpaq</td>\n",
       "      <td>[mira, na, n, paq]</td>\n",
       "      <td>mira na n paq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>yuthuwan</td>\n",
       "      <td>[yuthu, wan]</td>\n",
       "      <td>yuthu wan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>ñuqaptaqa</td>\n",
       "      <td>[ñuqa, p, ta, qa]</td>\n",
       "      <td>ñuqa p ta qa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>willachkanchá</td>\n",
       "      <td>[willa, chka, n, chá]</td>\n",
       "      <td>willa chka n chá</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>hampatitunta</td>\n",
       "      <td>[hampatu, it, n, ta]</td>\n",
       "      <td>hampatu it n ta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>paypachussina</td>\n",
       "      <td>[pay, pa, chus, sina]</td>\n",
       "      <td>pay pa chus sina</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>sufrichinankupaq</td>\n",
       "      <td>[sufri, chi, na, nku, paq]</td>\n",
       "      <td>sufri chi na nku paq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>mama</td>\n",
       "      <td>[mama]</td>\n",
       "      <td>mama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>zorritupis</td>\n",
       "      <td>[zorr, itu, pis]</td>\n",
       "      <td>zorr itu pis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>hanqarakuspa</td>\n",
       "      <td>[hanqara, ku, spa]</td>\n",
       "      <td>hanqara ku spa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>wachaytapis</td>\n",
       "      <td>[wacha, y, ta, pis]</td>\n",
       "      <td>wacha y ta pis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>waqanakunku</td>\n",
       "      <td>[waqa, naku, nku]</td>\n",
       "      <td>waqa naku nku</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>qalata</td>\n",
       "      <td>[qala, ta]</td>\n",
       "      <td>qala ta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>thatarpanchá</td>\n",
       "      <td>[thata, rpa, n, chá]</td>\n",
       "      <td>thata rpa n chá</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>saksasqita</td>\n",
       "      <td>[saksa, sqa, it]</td>\n",
       "      <td>saksa sqa it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>chayrayku</td>\n",
       "      <td>[chay, rayku]</td>\n",
       "      <td>chay rayku</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>purichkan</td>\n",
       "      <td>[puri, chka, n]</td>\n",
       "      <td>puri chka n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>maytachus</td>\n",
       "      <td>[may, ta, chus]</td>\n",
       "      <td>may ta chus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>rikhurimpuptin</td>\n",
       "      <td>[rikhuri, m, pu, pti, n]</td>\n",
       "      <td>rikhuri m pu pti n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>pachay</td>\n",
       "      <td>[pacha, y]</td>\n",
       "      <td>pacha y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Word                 Morph_split       Morph_split_str\n",
       "0      cementerioman           [cementerio, man]        cementerio man\n",
       "1     kawsachkananta    [kawsa, chka, na, n, ta]    kawsa chka na n ta\n",
       "2         mañakunpis          [maña, ku, n, pis]         maña ku n pis\n",
       "3       imaynapichus          [imayna, pi, chus]        imayna pi chus\n",
       "4            qipiyuq                 [qipi, yuq]              qipi yuq\n",
       "5          Quispepis               [Quispe, pis]            Quispe pis\n",
       "6      ñichkanmanchá       [ñi, chka, nman, chá]      ñi chka nman chá\n",
       "7             qukuni                [qu, ku, ni]              qu ku ni\n",
       "8     dejasunpunichu      [deja, sun, puni, chu]     deja sun puni chu\n",
       "9         phutikunki            [phuti, ku, nki]          phuti ku nki\n",
       "10       kawsakunman           [kawsa, ku, nman]         kawsa ku nman\n",
       "11      yanapasqayki            [yanapa, sqayki]         yanapa sqayki\n",
       "12      Pachamamaman            [Pachamama, man]         Pachamama man\n",
       "13         pagawanki             [paga, wa, nki]           paga wa nki\n",
       "14        qhawanakun            [qhawa, naku, n]          qhawa naku n\n",
       "15            paypis                  [pay, pis]               pay pis\n",
       "16        qhurasqita            [qhura, sqa, it]          qhura sqa it\n",
       "17      defendekusaq          [defende, ku, saq]        defende ku saq\n",
       "18        parlaspaqa            [parla, spa, qa]          parla spa qa\n",
       "19            tinqan                  [tinqa, n]               tinqa n\n",
       "20        kutuykapun          [kutu, yka, pu, n]         kutu yka pu n\n",
       "21           qurikun             [qu, ri, ku, n]            qu ri ku n\n",
       "22    tortugapunichu        [tortuga, puni, chu]      tortuga puni chu\n",
       "23   qipirisqallataq   [qipi, ri, sqa, lla, taq]   qipi ri sqa lla taq\n",
       "24             sigue                     [sigue]                 sigue\n",
       "25        ratullachu            [ratu, lla, chu]          ratu lla chu\n",
       "26           tulluta                 [tullu, ta]              tullu ta\n",
       "27        wawasninku          [wawa, s, ni, nku]         wawa s ni nku\n",
       "28       ñawisitunta         [ñawi, situ, n, ta]        ñawi situ n ta\n",
       "29            nipuni                  [ni, puni]               ni puni\n",
       "30        mirananpaq          [mira, na, n, paq]         mira na n paq\n",
       "31          yuthuwan                [yuthu, wan]             yuthu wan\n",
       "32         ñuqaptaqa           [ñuqa, p, ta, qa]          ñuqa p ta qa\n",
       "33     willachkanchá       [willa, chka, n, chá]      willa chka n chá\n",
       "34      hampatitunta        [hampatu, it, n, ta]       hampatu it n ta\n",
       "35     paypachussina       [pay, pa, chus, sina]      pay pa chus sina\n",
       "36  sufrichinankupaq  [sufri, chi, na, nku, paq]  sufri chi na nku paq\n",
       "37              mama                      [mama]                  mama\n",
       "38        zorritupis            [zorr, itu, pis]          zorr itu pis\n",
       "39      hanqarakuspa          [hanqara, ku, spa]        hanqara ku spa\n",
       "40       wachaytapis         [wacha, y, ta, pis]        wacha y ta pis\n",
       "41       waqanakunku           [waqa, naku, nku]         waqa naku nku\n",
       "42            qalata                  [qala, ta]               qala ta\n",
       "43      thatarpanchá        [thata, rpa, n, chá]       thata rpa n chá\n",
       "44        saksasqita            [saksa, sqa, it]          saksa sqa it\n",
       "45         chayrayku               [chay, rayku]            chay rayku\n",
       "46         purichkan             [puri, chka, n]           puri chka n\n",
       "47         maytachus             [may, ta, chus]           may ta chus\n",
       "48    rikhurimpuptin    [rikhuri, m, pu, pti, n]    rikhuri m pu pti n\n",
       "49            pachay                  [pacha, y]               pacha y"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2a700f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATASET SUMMARY\n",
      "============================================================\n",
      "Training data shape: (6896, 3)\n",
      "Test data shape: (913, 5)\n",
      "Models folder: models_DT-LSTM-MarkovFilter\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# LOAD TEST DATA\n",
    "# =========================\n",
    "# Load the test/accuracy evaluation dataset\n",
    "# This dataset is used for final evaluation of the trained model\n",
    "\n",
    "acc_df = pd.read_parquet(os.path.join(DATA_FOLDER, \"cleaned_data_df.parquet\"))\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DATASET SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training data shape: {gold_df.shape}\")\n",
    "print(f\"Test data shape: {acc_df.shape}\")\n",
    "print(f\"Models folder: {MODELS_FOLDER}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ec5eed04",
   "metadata": {},
   "outputs": [],
   "source": [
    "graphemes = [\n",
    "    \"ch\",\"ll\",\"rr\",\"tr\",\"kw\",\"ph\",  # digraphs/trigraphs\n",
    "    \"a\",\"b\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\",\"k\",\"l\",\"m\",\"n\",\"ñ\",\"o\",\"p\",\"q\",\n",
    "    \"r\",\"s\",\"t\",\"u\",\"v\",\"w\",\"x\",\"y\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e48798ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "95db6a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(\"|\".join(sorted(graphemes, key=len, reverse=True)))\n",
    "\n",
    "def tokenize_morphemes(morphs):\n",
    "    return [pattern.findall(m.lower()) for m in morphs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6b21262d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_df[\"Char_split\"] = gold_df[\"Morph_split\"].apply(tokenize_morphemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8941c70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vowels = {\"a\", \"i\", \"e\", \"o\", \"u\"}\n",
    "\n",
    "def grapheme_to_cv(grapheme):\n",
    "    return \"V\" if grapheme in vowels else \"C\"\n",
    "\n",
    "def morphs_to_cv(morphs):\n",
    "    return [[grapheme_to_cv(g) for g in morph] for morph in morphs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5da0120c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_df[\"CV_split\"] = gold_df[\"Char_split\"].apply(morphs_to_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "66284fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_to_string(cv_split):\n",
    "    \"\"\"Convert nested CV list to dash-separated string.\"\"\"\n",
    "    return \"-\".join(\"\".join(m) for m in cv_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e2e4fcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ec611f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "dd6db475",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_df[\"Full_chain\"] = gold_df[\"CV_split\"].apply(cv_to_string)\n",
    "\n",
    "# Create Trimmed_chain, but use NaN if no dash\n",
    "str_df[\"Trimmed_chain\"] = str_df[\"Full_chain\"].apply(\n",
    "    lambda x: x.split(\"-\", 1)[1] if \"-\" in x else np.nan\n",
    ")\n",
    "\n",
    "str_df[\"Word\"] = gold_df[\"Word\"]\n",
    "str_df[\"Char_split\"] = gold_df[\"Char_split\"]\n",
    "str_df[\"Morph_split\"] = gold_df[\"Morph_split\"]\n",
    "\n",
    "# Drop rows where Trimmed_chain is NaN\n",
    "str_df = str_df.dropna(subset=[\"Trimmed_chain\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c92caf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word length\n",
    "str_df[\"Word_len\"] = str_df[\"Word\"].str.len()\n",
    "\n",
    "# consonant and vowel count from Full_chain\n",
    "str_df[\"Vowel_no\"] = str_df[\"Full_chain\"].str.count(\"V\")\n",
    "str_df[\"Cons_no\"] = str_df[\"Full_chain\"].str.count(\"C\")\n",
    "\n",
    "# tail consonant and vowel counts (last segment in Full_chain after '-')\n",
    "str_df[\"Tail_cons_no\"] = str_df[\"Trimmed_chain\"].str.count(\"C\")\n",
    "str_df[\"Tail_vowel_no\"] = str_df[\"Trimmed_chain\"].str.count(\"V\")\n",
    "\n",
    "# number of splits from Morph_split\n",
    "str_df[\"No_splits\"] = str_df[\"Morph_split\"].str.len()\n",
    "\n",
    "# total y/w count in word\n",
    "str_df[\"YW_count\"] = str_df[\"Word\"].str.count(\"[yw]\")\n",
    "\n",
    "# tail y/w count (all morphs except first)\n",
    "str_df[\"Tail_YW_count\"] = str_df[\"Morph_split\"].apply(\n",
    "    lambda ms: sum(m.count(\"y\") + m.count(\"w\") for m in ms[1:])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "822afc9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Full_chain</th>\n",
       "      <th>Trimmed_chain</th>\n",
       "      <th>Word</th>\n",
       "      <th>Char_split</th>\n",
       "      <th>Morph_split</th>\n",
       "      <th>Word_len</th>\n",
       "      <th>Vowel_no</th>\n",
       "      <th>Cons_no</th>\n",
       "      <th>Tail_cons_no</th>\n",
       "      <th>Tail_vowel_no</th>\n",
       "      <th>No_splits</th>\n",
       "      <th>YW_count</th>\n",
       "      <th>Tail_YW_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VCVCCVCVV-CVC</td>\n",
       "      <td>CVC</td>\n",
       "      <td>cementerioman</td>\n",
       "      <td>[[e, m, e, n, t, e, r, i, o], [m, a, n]]</td>\n",
       "      <td>[cementerio, man]</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CVCCV-CCV-CV-C-CV</td>\n",
       "      <td>CCV-CV-C-CV</td>\n",
       "      <td>kawsachkananta</td>\n",
       "      <td>[[k, a, w, s, a], [ch, k, a], [n, a], [n], [t,...</td>\n",
       "      <td>[kawsa, chka, na, n, ta]</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CVCV-CV-C-CVC</td>\n",
       "      <td>CV-C-CVC</td>\n",
       "      <td>mañakunpis</td>\n",
       "      <td>[[m, a, ñ, a], [k, u], [n], [p, i, s]]</td>\n",
       "      <td>[maña, ku, n, pis]</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VCVCCV-CV-CVC</td>\n",
       "      <td>CV-CVC</td>\n",
       "      <td>imaynapichus</td>\n",
       "      <td>[[i, m, a, y, n, a], [p, i], [ch, u, s]]</td>\n",
       "      <td>[imayna, pi, chus]</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CVCV-CVC</td>\n",
       "      <td>CVC</td>\n",
       "      <td>qipiyuq</td>\n",
       "      <td>[[q, i, p, i], [y, u, q]]</td>\n",
       "      <td>[qipi, yuq]</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Full_chain Trimmed_chain            Word  \\\n",
       "0      VCVCCVCVV-CVC           CVC   cementerioman   \n",
       "1  CVCCV-CCV-CV-C-CV   CCV-CV-C-CV  kawsachkananta   \n",
       "2      CVCV-CV-C-CVC      CV-C-CVC      mañakunpis   \n",
       "3      VCVCCV-CV-CVC        CV-CVC    imaynapichus   \n",
       "4           CVCV-CVC           CVC         qipiyuq   \n",
       "\n",
       "                                          Char_split  \\\n",
       "0           [[e, m, e, n, t, e, r, i, o], [m, a, n]]   \n",
       "1  [[k, a, w, s, a], [ch, k, a], [n, a], [n], [t,...   \n",
       "2             [[m, a, ñ, a], [k, u], [n], [p, i, s]]   \n",
       "3           [[i, m, a, y, n, a], [p, i], [ch, u, s]]   \n",
       "4                          [[q, i, p, i], [y, u, q]]   \n",
       "\n",
       "                Morph_split  Word_len  Vowel_no  Cons_no  Tail_cons_no  \\\n",
       "0         [cementerio, man]        13         6        6             2   \n",
       "1  [kawsa, chka, na, n, ta]        14         5        8             5   \n",
       "2        [maña, ku, n, pis]        10         4        6             4   \n",
       "3        [imayna, pi, chus]        12         5        6             3   \n",
       "4               [qipi, yuq]         7         3        4             2   \n",
       "\n",
       "   Tail_vowel_no  No_splits  YW_count  Tail_YW_count  \n",
       "0              1          2         0              0  \n",
       "1              3          5         1              0  \n",
       "2              2          4         0              0  \n",
       "3              2          3         1              0  \n",
       "4              1          2         1              1  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9d8c983b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b9bd5394",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "END_LABEL = \"Ø\"\n",
    "VOWELS = set(list(\"aeiou\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f84d7e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_literal_list(obj):\n",
    "    \"\"\"Parse '[[p, i],[k, u, n, a],[s]]' → [['p','i'], ...] if string; pass through if already list.\"\"\"\n",
    "    if isinstance(obj, list):\n",
    "        return obj\n",
    "    if pd.isna(obj):\n",
    "        return None\n",
    "    s = str(obj).strip()\n",
    "    try:\n",
    "        val = ast.literal_eval(s)\n",
    "        return val\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def flatten_char_split(char_split):\n",
    "    \"\"\"Flatten list-of-lists of char tokens into a single list of tokens.\"\"\"\n",
    "    if not isinstance(char_split, list):\n",
    "        return None\n",
    "    out = []\n",
    "    for seg in char_split:\n",
    "        if isinstance(seg, list):\n",
    "            out.extend([str(x) for x in seg])\n",
    "        else:\n",
    "            out.append(str(seg))\n",
    "    return out\n",
    "\n",
    "def tokens_to_word(tokens):\n",
    "    \"\"\"Join tokens to surface word. If tokens are graphemes (like 'ch'), just concatenate.\"\"\"\n",
    "    if not tokens:\n",
    "        return \"\"\n",
    "    return \"\".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f813b7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_chain(chain: str):\n",
    "    if chain is None:\n",
    "        return []\n",
    "    s = str(chain).strip()\n",
    "    return [] if not s else s.split('-')\n",
    "\n",
    "def extract_root_and_trimmed(full_chain: str):\n",
    "    segs = split_chain(full_chain)\n",
    "    if not segs:\n",
    "        return \"\", END_LABEL\n",
    "    root = segs[0]\n",
    "    trimmed = '-'.join(segs[1:]) if len(segs) > 1 else END_LABEL\n",
    "    return root, trimmed\n",
    "\n",
    "def suffixes_from_trimmed(trimmed: str):\n",
    "    if trimmed is None or trimmed == END_LABEL or str(trimmed).strip() == \"\":\n",
    "        return []\n",
    "    return str(trimmed).split('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e083a7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_NUM_FEATS = [\n",
    "    \"Word_len\", \"Vowel_no\", \"Cons_no\",\n",
    "    \"Tail_cons_no\", \"Tail_vowel_no\",\n",
    "    \"No_splits\", \"YW_count\", \"Tail_YW_count\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6d797a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_cv_features(root_cv: str):\n",
    "    s = root_cv or \"\"\n",
    "    L = len(s)\n",
    "    feats = {\n",
    "        \"root_cv\": s,\n",
    "        \"root_len\": L,\n",
    "        \"root_end\": s[-1:] if L else \"\",\n",
    "        \"root_start\": s[:1] if L else \"\",\n",
    "        \"root_suffix2\": s[-2:] if L >= 2 else s,\n",
    "        \"root_prefix2\": s[:2] if L >= 2 else s,\n",
    "        \"num_C\": s.count('C'),\n",
    "        \"num_V\": s.count('V'),\n",
    "        \"has_CC\": int('CC' in s),\n",
    "        \"has_VV\": int('VV' in s),\n",
    "    }\n",
    "    for i in range(L-1):\n",
    "        feats[f\"bg_{s[i:i+2]}\"] = 1\n",
    "    for i in range(L-2):\n",
    "        feats[f\"tg_{s[i:i+3]}\"] = 1\n",
    "    return feats\n",
    "\n",
    "def last_char_features(word: str, k_chars=(1,2,3)):\n",
    "    feats = {}\n",
    "    if not word:\n",
    "        return feats\n",
    "    w = word\n",
    "    # raw last n characters\n",
    "    for k in k_chars:\n",
    "        s = w[-k:] if len(w) >= k else w\n",
    "        feats[f\"last{k}\"] = s\n",
    "    # last character vowel/consonant\n",
    "    last = w[-1]\n",
    "    feats[\"last_is_vowel\"] = int(last in VOWELS)\n",
    "    feats[\"last_char\"] = last\n",
    "    # last vowel identity (if any)\n",
    "    last_vowel = ''\n",
    "    for ch in reversed(w):\n",
    "        if ch in VOWELS:\n",
    "            last_vowel = ch.lower()\n",
    "            break\n",
    "    feats[\"last_vowel\"] = last_vowel\n",
    "    return feats\n",
    "\n",
    "def last_cluster_features(char_tokens: list, k_clusters=(1,2)):\n",
    "    feats = {}\n",
    "    if not char_tokens:\n",
    "        return feats\n",
    "    toks = char_tokens\n",
    "    for k in k_clusters:\n",
    "        tail = toks[-k:] if len(toks) >= k else toks\n",
    "        feats[f\"lastTok{k}\"] = \"|\".join(tail)  # keep as categorical string\n",
    "    feats[\"lastTok1\"] = toks[-1]  # ensure always present\n",
    "    return feats\n",
    "\n",
    "def cv_tail_features(word: str):\n",
    "    \"\"\"Approximate CV tail from raw word if CV_split not present (best-effort).\"\"\"\n",
    "    if not word:\n",
    "        return {}\n",
    "    def cv(c):\n",
    "        return 'V' if c in VOWELS else 'C'\n",
    "    tail_cv = ''.join(cv(ch) for ch in word[-3:])  # last 3 chars' CV\n",
    "    return {\"tail_cv_approx\": tail_cv, \"tail_last_cv\": tail_cv[-1:]}\n",
    "\n",
    "def build_features_row(row):\n",
    "    \"\"\"Unified per-row feature dict (now also pulls your numeric counters if present).\"\"\"\n",
    "    feats = {}\n",
    "    # root CV (always)\n",
    "    feats.update(root_cv_features(row.get(\"root_cv\", \"\")))\n",
    "\n",
    "    # surface word / tokens\n",
    "    word = \"\"\n",
    "    char_tokens = None\n",
    "\n",
    "    if \"Char_split\" in row and row[\"Char_split\"] is not None:\n",
    "        cs = safe_literal_list(row[\"Char_split\"])\n",
    "        toks = flatten_char_split(cs) if cs is not None else None\n",
    "        char_tokens = toks\n",
    "        word = tokens_to_word(toks) if toks else \"\"\n",
    "    elif \"Word\" in row and pd.notna(row.get(\"Word\", None)):\n",
    "        word = str(row[\"Word\"])\n",
    "    else:\n",
    "        word = \"\"\n",
    "\n",
    "    # end-of-word character features\n",
    "    feats.update(last_char_features(word, k_chars=(1,2,3)))\n",
    "\n",
    "    # if we have tokenized clusters (handles digraphs like 'ch')\n",
    "    if char_tokens:\n",
    "        feats.update(last_cluster_features(char_tokens, k_clusters=(1,2)))\n",
    "\n",
    "    # approximate CV tail from raw string (fallback)\n",
    "    feats.update(cv_tail_features(word))\n",
    "\n",
    "    # ==== NEW: attach your numeric counters if present on the row ====\n",
    "    for k in NEW_NUM_FEATS:\n",
    "        if k in row and pd.notna(row[k]):\n",
    "            # cast to float so DictVectorizer treats them as numeric\n",
    "            try:\n",
    "                feats[k] = float(row[k])\n",
    "            except Exception:\n",
    "                # if any stray non-numeric sneaks in, skip silently\n",
    "                pass\n",
    "\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3b2cc903",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(df_in: pd.DataFrame):\n",
    "    rows = []\n",
    "    for _, r in df_in.iterrows():\n",
    "        full = r['Full_chain']\n",
    "        root, trimmed_auto = extract_root_and_trimmed(full)\n",
    "        trimmed = r['Trimmed_chain'] if 'Trimmed_chain' in df_in.columns and pd.notna(r['Trimmed_chain']) else trimmed_auto\n",
    "        suffixes = suffixes_from_trimmed(trimmed)\n",
    "\n",
    "        row = {\n",
    "            \"full_chain\": full,\n",
    "            \"root_cv\": root,\n",
    "            \"trimmed\": trimmed if trimmed else END_LABEL,\n",
    "            \"suffixes\": suffixes,\n",
    "            \"suffix_len\": len(suffixes),\n",
    "        }\n",
    "        # carry optional richer columns if present\n",
    "        for opt in (\"Word\",\"Char_split\",\"CV_split\"):\n",
    "            if opt in df_in.columns:\n",
    "                row[opt] = r[opt]\n",
    "\n",
    "        # ==== NEW: carry through your precomputed numeric columns ====\n",
    "        for k in NEW_NUM_FEATS:\n",
    "            if k in df_in.columns:\n",
    "                row[k] = r[k]\n",
    "\n",
    "        rows.append(row)\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c47f9fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dicts_from_df(df: pd.DataFrame, add_prev=None):\n",
    "    \"\"\"Turn rows into feature dicts. add_prev = {'y_step1': 'CV', ...} keys present in df or to inject.\"\"\"\n",
    "    feat_dicts = []\n",
    "    for _, r in df.iterrows():\n",
    "        base = build_features_row(r)\n",
    "        if add_prev:\n",
    "            for k in add_prev:\n",
    "                if k in r and pd.notna(r[k]):\n",
    "                    base[k] = r[k]\n",
    "        feat_dicts.append(base)\n",
    "    return feat_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f90efeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_fit_transform(feat_dicts):\n",
    "    vec = DictVectorizer(sparse=True)\n",
    "    X = vec.fit_transform(feat_dicts)\n",
    "    return X, vec\n",
    "\n",
    "def vec_transform(vec, feat_dicts):\n",
    "    return vec.transform(feat_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ce7b7f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouped_split(df, train_size=0.8, seed=RANDOM_STATE):\n",
    "    gss = GroupShuffleSplit(n_splits=1, train_size=train_size, random_state=seed)\n",
    "    groups = df['root_cv'].astype(str).values\n",
    "    tr_idx, te_idx = next(gss.split(df, groups=groups))\n",
    "    return df.iloc[tr_idx].reset_index(drop=True), df.iloc[te_idx].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "10112f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topN_labels_by_freq(y, Ns=(16,25,37,57,103)):\n",
    "    ctr = Counter(y)\n",
    "    most_common = ctr.most_common()\n",
    "    return {N: set([lab for lab,_ in most_common[:N]]) for N in Ns}, ctr\n",
    "\n",
    "def eval_subsets(y_true, y_pred, labels_by_topN):\n",
    "    out = {}\n",
    "    y_true = np.array(y_true); y_pred = np.array(y_pred)\n",
    "    for N, labelset in labels_by_topN.items():\n",
    "        idx = [i for i,lab in enumerate(y_true) if lab in labelset]\n",
    "        if not idx:\n",
    "            out[N] = {\"accuracy\": np.nan, \"f1_macro\": np.nan, \"f1_weighted\": np.nan, \"support\": 0}\n",
    "            continue\n",
    "        yt, yp = y_true[idx], y_pred[idx]\n",
    "        out[N] = {\n",
    "            \"accuracy\": accuracy_score(yt, yp),\n",
    "            \"f1_macro\": f1_score(yt, yp, average='macro', zero_division=0),\n",
    "            \"f1_weighted\": f1_score(yt, yp, average='weighted', zero_division=0),\n",
    "            \"support\": len(idx),\n",
    "        }\n",
    "    return out\n",
    "\n",
    "def print_subset_metrics(name, d):\n",
    "    print(f\"\\n== {name}: Top-N subsets ==\")\n",
    "    for N in sorted(d.keys()):\n",
    "        m = d[N]\n",
    "        print(f\"Top-{N:>3} (n={m['support']:>4}): Acc={m['accuracy']:.3f} | F1_mac={m['f1_macro']:.3f} | F1_wt={m['f1_weighted']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ce9dabbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_classifier(kind=\"tree\", **kwargs):\n",
    "    if kind == \"rf\":\n",
    "        return RandomForestClassifier(\n",
    "            n_estimators=80, max_depth=10, min_samples_leaf=5,\n",
    "            random_state=RANDOM_STATE, n_jobs=-1\n",
    "        )\n",
    "    # default: simple tree\n",
    "    return DecisionTreeClassifier(\n",
    "        criterion=\"entropy\",\n",
    "        max_depth=kwargs.get(\"max_depth\", 6),\n",
    "        min_samples_leaf=kwargs.get(\"min_samples_leaf\", 10),\n",
    "        random_state=RANDOM_STATE\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "77a04700",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_shot(df_all, clf_kind=\"tree\"):\n",
    "    df_tr, df_te = grouped_split(df_all, train_size=0.8)\n",
    "    # vectorize\n",
    "    Xtr_dicts = dicts_from_df(df_tr)\n",
    "    Xtr, vec = vec_fit_transform(Xtr_dicts)\n",
    "    ytr = df_tr['trimmed'].astype(str).values\n",
    "\n",
    "    clf = make_classifier(clf_kind)\n",
    "    clf.fit(Xtr, ytr)\n",
    "\n",
    "    Xte = vec_transform(vec, dicts_from_df(df_te))\n",
    "    yte = df_te['trimmed'].astype(str).values\n",
    "    yhat = clf.predict(Xte)\n",
    "\n",
    "    acc = accuracy_score(yte, yhat)\n",
    "    f1m = f1_score(yte, yhat, average='macro', zero_division=0)\n",
    "    f1w = f1_score(yte, yhat, average='weighted', zero_division=0)\n",
    "\n",
    "    print(\"=== Single-shot classifier ===\")\n",
    "    print(f\"Test: Acc={acc:.3f} | F1_macro={f1m:.3f} | F1_weighted={f1w:.3f}\")\n",
    "\n",
    "    labels_by_topN, _ = topN_labels_by_freq(df_tr['trimmed'].astype(str).values)\n",
    "    subset = eval_subsets(yte, yhat, labels_by_topN)\n",
    "    print_subset_metrics(\"Single-shot\", subset)\n",
    "\n",
    "    # gentle warning if we had no surface info\n",
    "    if (\"Word\" not in df_all.columns) and (\"Char_split\" not in df_all.columns):\n",
    "        print(\"\\n[warn] No 'Word' or 'Char_split' columns found. Using root-only features. \"\n",
    "              \"Add surface columns for better results.\")\n",
    "\n",
    "    return {\"clf\": clf, \"vec\": vec, \"test_df\": df_te, \"test_pred\": yhat}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6dd97dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_length_classifier(df_tr, clf_kind=\"tree\"):\n",
    "    # target: suffix_len binned: 1,2,3,4+  (map 4..6 to '4+')\n",
    "    ylen = []\n",
    "    for n in df_tr['suffix_len'].values:\n",
    "        ylen.append(str(n) if n in (1,2,3) else \"4+\")\n",
    "    ylen = np.array(ylen)\n",
    "\n",
    "    X_dicts = dicts_from_df(df_tr)\n",
    "    X, vec = vec_fit_transform(X_dicts)\n",
    "    clf = make_classifier(clf_kind, max_depth=5, min_samples_leaf=10)\n",
    "    clf.fit(X, ylen)\n",
    "    return clf, vec\n",
    "\n",
    "def make_step_frame(df, step):\n",
    "    # y_step = suffix at position 'step' from end, or END if none\n",
    "    y = []\n",
    "    for sufs in df['suffixes']:\n",
    "        if len(sufs) >= step:\n",
    "            y.append(sufs[-step])\n",
    "        else:\n",
    "            y.append(END_LABEL)\n",
    "    df2 = df.copy()\n",
    "    df2[f\"y_step{step}\"] = y\n",
    "    return df2\n",
    "\n",
    "def run_sequential(df_all, clf_kind=\"tree\", max_steps_cap=5):\n",
    "    df_tr, df_te = grouped_split(df_all, train_size=0.8)\n",
    "\n",
    "    # 1) Length-first\n",
    "    len_clf, len_vec = train_length_classifier(df_tr, clf_kind=clf_kind)\n",
    "\n",
    "    # determine max steps to train (≤ cap)\n",
    "    max_steps = min(max_steps_cap, 4)  # we only need up to 4 because 4+ bucket\n",
    "    print(f\"\\n=== Length-first + Sequential ===\\nTraining up to {max_steps} steps (last→first)\")\n",
    "\n",
    "    # 2) Train step-wise trees (teacher forcing for previous predictions)\n",
    "    step_vecs, step_clfs = {}, {}\n",
    "    prev_cols = []\n",
    "    for step in range(1, max_steps+1):\n",
    "        df_step = make_step_frame(df_tr, step)\n",
    "        X_dicts = dicts_from_df(df_step, add_prev=set(prev_cols))\n",
    "        X, vec = vec_fit_transform(X_dicts)\n",
    "        y = df_step[f\"y_step{step}\"].astype(str).values\n",
    "\n",
    "        clf = make_classifier(clf_kind, max_depth=6, min_samples_leaf=8)\n",
    "        clf.fit(X, y)\n",
    "\n",
    "        step_vecs[step] = vec\n",
    "        step_clfs[step] = clf\n",
    "        prev_cols.append(f\"y_step{step}\")\n",
    "\n",
    "    # 3) Inference\n",
    "    gold_full = df_te['trimmed'].astype(str).values\n",
    "    preds_full = []\n",
    "\n",
    "    # diagnostics per-step\n",
    "    per_step_gold = defaultdict(list)\n",
    "    per_step_pred = defaultdict(list)\n",
    "\n",
    "    # Pre-compute length predictions\n",
    "    Xlen = vec_transform(len_vec, dicts_from_df(df_te))\n",
    "    ylen_pred = len_clf.predict(Xlen)\n",
    "\n",
    "    for i, r in df_te.iterrows():\n",
    "        # predicted number of suffixes to output\n",
    "        k_str = ylen_pred[i]\n",
    "        K = 4 if k_str == \"4+\" else int(k_str)\n",
    "\n",
    "        prev_preds = []\n",
    "        # Build base feat (constant for this word; we do not peel in features)\n",
    "        base_row = r.to_dict()\n",
    "\n",
    "        for step in range(1, K+1):\n",
    "            # feature dict with previous predicted labels injected\n",
    "            feat = build_features_row(base_row)\n",
    "            for j, lab in enumerate(prev_preds, start=1):\n",
    "                feat[f\"y_step{j}\"] = lab\n",
    "\n",
    "            X_one = vec_transform(step_vecs[step], [feat])\n",
    "            yhat = step_clfs[step].predict(X_one)[0]\n",
    "\n",
    "            # collect step metrics (gold at this step)\n",
    "            gold_suffixes = r['suffixes']\n",
    "            ygold = gold_suffixes[-step] if len(gold_suffixes) >= step else END_LABEL\n",
    "            per_step_gold[step].append(ygold)\n",
    "            per_step_pred[step].append(yhat)\n",
    "\n",
    "            if yhat == END_LABEL:\n",
    "                break\n",
    "            prev_preds.append(yhat)\n",
    "\n",
    "        # reconstruct chain (earliest→latest)\n",
    "        pred_chain = '-'.join(reversed(prev_preds)) if prev_preds else END_LABEL\n",
    "        preds_full.append(pred_chain)\n",
    "\n",
    "    # Evaluate exact chain\n",
    "    acc = accuracy_score(gold_full, preds_full)\n",
    "    f1m = f1_score(gold_full, preds_full, average='macro', zero_division=0)\n",
    "    f1w = f1_score(gold_full, preds_full, average='weighted', zero_division=0)\n",
    "\n",
    "    print(f\"Test: Acc={acc:.3f} | F1_macro={f1m:.3f} | F1_weighted={f1w:.3f}\")\n",
    "\n",
    "    labels_by_topN, _ = topN_labels_by_freq(df_tr['trimmed'].astype(str).values)\n",
    "    subset = eval_subsets(gold_full, preds_full, labels_by_topN)\n",
    "    print_subset_metrics(\"Sequential\", subset)\n",
    "\n",
    "    # per-step metrics (sanity)\n",
    "    for step in range(1, max_steps+1):\n",
    "        if len(per_step_gold[step]) == 0:\n",
    "            continue\n",
    "        ys = np.array(per_step_gold[step]); ps = np.array(per_step_pred[step])\n",
    "        a = accuracy_score(ys, ps)\n",
    "        fm = f1_score(ys, ps, average='macro', zero_division=0)\n",
    "        fw = f1_score(ys, ps, average='weighted', zero_division=0)\n",
    "        print(f\"Step {step}: Acc={a:.3f} | F1_macro={fm:.3f} | F1_weighted={fw:.3f}\")\n",
    "\n",
    "    # gentle warning if no surface columns\n",
    "    if (\"Word\" not in df_all.columns) and (\"Char_split\" not in df_all.columns):\n",
    "        print(\"\\n[warn] No 'Word' or 'Char_split' found. Sequential features can’t see surface endings; \"\n",
    "              \"add them to boost Step1/Step2 substantially.\")\n",
    "\n",
    "    return {\"len_clf\": len_clf, \"len_vec\": len_vec,\n",
    "            \"step_clfs\": step_clfs, \"step_vecs\": step_vecs,\n",
    "            \"test_df\": df_te, \"test_pred\": preds_full}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ac791977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# SUFFIX CLASSIFIER SAVING/LOADING FUNCTIONS\n",
    "# =========================\n",
    "# These functions handle saving and loading the single-shot and sequential\n",
    "# classifiers (RandomForest/DecisionTree) that predict suffixes\n",
    "\n",
    "def generate_suffix_classifier_id(str_df, clf_kind=\"tree\"):\n",
    "    \"\"\"\n",
    "    Generate a unique identifier for suffix classifiers based on data and classifier type.\n",
    "    \n",
    "    Args:\n",
    "        str_df: DataFrame used for training\n",
    "        clf_kind: Type of classifier (\"tree\" or \"rf\")\n",
    "    \n",
    "    Returns:\n",
    "        A string identifier (hash) for the classifiers\n",
    "    \"\"\"\n",
    "    # Create a dictionary of parameters\n",
    "    params_dict = {\n",
    "        'clf_kind': clf_kind,\n",
    "        'df_shape': str_df.shape if str_df is not None else (0, 0),\n",
    "        'df_columns': sorted(str_df.columns.tolist()) if str_df is not None else []\n",
    "    }\n",
    "    \n",
    "    # Convert to JSON string and hash it\n",
    "    params_str = json.dumps(params_dict, sort_keys=True)\n",
    "    classifier_id = hashlib.md5(params_str.encode()).hexdigest()[:16]\n",
    "    return classifier_id\n",
    "\n",
    "def save_suffix_classifiers(single, seq, classifier_id, data_folder=DATA_FOLDER):\n",
    "    \"\"\"\n",
    "    Save the single-shot and sequential suffix classifiers.\n",
    "    \n",
    "    Args:\n",
    "        single: Dictionary with single-shot classifier results\n",
    "        seq: Dictionary with sequential classifier results\n",
    "        classifier_id: Unique identifier for these classifiers\n",
    "        data_folder: Folder to save classifiers in\n",
    "    \"\"\"\n",
    "    classifier_dir = os.path.join(data_folder, f\"suffix_classifiers_{classifier_id}\")\n",
    "    os.makedirs(classifier_dir, exist_ok=True)\n",
    "    \n",
    "    # Save single-shot classifier\n",
    "    if single is not None:\n",
    "        single_path = os.path.join(classifier_dir, \"single.pkl\")\n",
    "        with open(single_path, \"wb\") as f:\n",
    "            pickle.dump(single, f)\n",
    "    \n",
    "    # Save sequential classifier\n",
    "    if seq is not None:\n",
    "        seq_path = os.path.join(classifier_dir, \"seq.pkl\")\n",
    "        with open(seq_path, \"wb\") as f:\n",
    "            pickle.dump(seq, f)\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata_path = os.path.join(classifier_dir, \"metadata.json\")\n",
    "    with open(metadata_path, \"w\") as f:\n",
    "        json.dump({\n",
    "            'classifier_id': classifier_id,\n",
    "            'clf_kind': single.get('clf').__class__.__name__ if single and 'clf' in single else 'unknown'\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    print(f\"Suffix classifiers saved to {classifier_dir}\")\n",
    "    return classifier_dir\n",
    "\n",
    "def load_suffix_classifiers(classifier_id, data_folder=DATA_FOLDER):\n",
    "    \"\"\"\n",
    "    Load the single-shot and sequential suffix classifiers.\n",
    "    \n",
    "    Args:\n",
    "        classifier_id: Unique identifier for the classifiers\n",
    "        data_folder: Folder where classifiers are saved\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (single, seq) or (None, None) if not found\n",
    "    \"\"\"\n",
    "    classifier_dir = os.path.join(data_folder, f\"suffix_classifiers_{classifier_id}\")\n",
    "    \n",
    "    if not os.path.exists(classifier_dir):\n",
    "        return None, None\n",
    "    \n",
    "    single_path = os.path.join(classifier_dir, \"single.pkl\")\n",
    "    seq_path = os.path.join(classifier_dir, \"seq.pkl\")\n",
    "    \n",
    "    single = None\n",
    "    seq = None\n",
    "    \n",
    "    if os.path.exists(single_path):\n",
    "        with open(single_path, \"rb\") as f:\n",
    "            single = pickle.load(f)\n",
    "    \n",
    "    if os.path.exists(seq_path):\n",
    "        with open(seq_path, \"rb\") as f:\n",
    "            seq = pickle.load(f)\n",
    "    \n",
    "    if single is not None or seq is not None:\n",
    "        print(f\"Suffix classifiers loaded from {classifier_dir}\")\n",
    "    \n",
    "    return single, seq\n",
    "\n",
    "def run_all(str_df, clf_kind=\"tree\"):\n",
    "    \"\"\"\n",
    "    Train or load suffix prediction classifiers (single-shot and sequential).\n",
    "    \n",
    "    This function will:\n",
    "    1. Check if classifiers with the same parameters already exist\n",
    "    2. If found, load them and return them (skipping training)\n",
    "    3. If not found, train new classifiers and save them\n",
    "    \n",
    "    Args:\n",
    "        str_df: DataFrame with morphological data\n",
    "        clf_kind: \"tree\" (default) or \"rf\" (small RandomForest for extra lift).\n",
    "                  Your `str_df` can be:\n",
    "                  - minimal: ['Full_chain'] or ['Full_chain','Trimmed_chain']\n",
    "                  - richer: add ['Word','Char_split','CV_split'] for much better results\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (single, seq) dictionaries containing classifiers and results\n",
    "    \"\"\"\n",
    "    # Generate classifier identifier\n",
    "    classifier_id = generate_suffix_classifier_id(str_df, clf_kind=clf_kind)\n",
    "    \n",
    "    # Try to load existing classifiers\n",
    "    print(f\"Checking for existing suffix classifiers with ID: {classifier_id}\")\n",
    "    single, seq = load_suffix_classifiers(classifier_id, data_folder=DATA_FOLDER)\n",
    "    \n",
    "    if single is not None and seq is not None:\n",
    "        print(f\"✅ Found existing classifiers! Loading from data folder.\")\n",
    "        return single, seq\n",
    "    \n",
    "    # Classifiers don't exist, need to train\n",
    "    print(f\"No existing classifiers found. Training new classifiers...\")\n",
    "    \n",
    "    df_all = build_dataset(str_df)\n",
    "    print(\"Total samples:\", len(df_all))\n",
    "    print(\"Unique Trimmed_chain:\", df_all['trimmed'].nunique())\n",
    "    print(\"Suffix length distribution:\", df_all['suffix_len'].value_counts().sort_index().to_dict())\n",
    "\n",
    "    single = run_single_shot(df_all, clf_kind=clf_kind)\n",
    "    seq = run_sequential(df_all, clf_kind=clf_kind, max_steps_cap=5)\n",
    "    \n",
    "    # Save the trained classifiers\n",
    "    print(f\"\\nSaving trained suffix classifiers with ID: {classifier_id}\")\n",
    "    save_suffix_classifiers(single, seq, classifier_id, data_folder=DATA_FOLDER)\n",
    "    \n",
    "    return single, seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ce3d57cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for existing suffix classifiers with ID: eeab9cd2ca1ef3f6\n",
      "Suffix classifiers loaded from data\\suffix_classifiers_eeab9cd2ca1ef3f6\n",
      "✅ Found existing classifiers! Loading from data folder.\n"
     ]
    }
   ],
   "source": [
    "single, seq = run_all(str_df, clf_kind=\"tree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "976ff0e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for existing suffix classifiers with ID: b04651ccd6358c94\n",
      "Suffix classifiers loaded from data\\suffix_classifiers_b04651ccd6358c94\n",
      "✅ Found existing classifiers! Loading from data folder.\n"
     ]
    }
   ],
   "source": [
    "single, seq = run_all(str_df, clf_kind=\"rf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "85804240",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast, re, numpy as np, pandas as pd, torch, torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b68083a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "RNG = 42\n",
    "torch.manual_seed(RNG)\n",
    "np.random.seed(RNG)\n",
    "\n",
    "NEW_NUM_FEATS = [\n",
    "    \"Word_len\", \"Vowel_no\", \"Cons_no\",\n",
    "    \"Tail_cons_no\", \"Tail_vowel_no\",\n",
    "    \"No_splits\", \"YW_count\", \"Tail_YW_count\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e5894398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# DATA PREPROCESSING HELPER FUNCTIONS\n",
    "# =========================\n",
    "# These functions convert DataFrame rows into training samples with tokens,\n",
    "# boundary labels, and privileged features\n",
    "\n",
    "def safe_list(x):\n",
    "    \"\"\"\n",
    "    Safely convert a string representation of a list to an actual list.\n",
    "    Handles various formats that pandas might use when storing lists.\n",
    "    \n",
    "    Args:\n",
    "        x: Either a list or a string representation of a list\n",
    "    \n",
    "    Returns:\n",
    "        A Python list\n",
    "    \"\"\"\n",
    "    if isinstance(x, list): \n",
    "        return x\n",
    "    s = str(x)\n",
    "    try:\n",
    "        return ast.literal_eval(s)\n",
    "    except Exception:\n",
    "        # Try alternative format conversion for nested lists\n",
    "        s2 = s.replace(\"[[\", \"[['\").replace(\"]]\", \"']]\").replace(\"], [\", \"'],['\").replace(\", \", \"','\")\n",
    "        return ast.literal_eval(s2)\n",
    "\n",
    "def flatten(list_of_lists):\n",
    "    \"\"\"\n",
    "    Flatten a nested list structure into a single list.\n",
    "    \n",
    "    Args:\n",
    "        list_of_lists: A list containing sublists (e.g., [[a,b], [c], [d,e]])\n",
    "    \n",
    "    Returns:\n",
    "        A flattened list (e.g., [a, b, c, d, e])\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for seg in list_of_lists: \n",
    "        out.extend(seg)\n",
    "    return [str(t) for t in out]\n",
    "\n",
    "def extract_priv_features_from_row(row, feat_names):\n",
    "    \"\"\"\n",
    "    Extract privileged (numeric) features from a DataFrame row.\n",
    "    \n",
    "    Privileged features are features that are available during training\n",
    "    but not during inference (e.g., word length, vowel count, etc.).\n",
    "    These are used by the K-teacher regularizer.\n",
    "    \n",
    "    Args:\n",
    "        row: A pandas Series (DataFrame row)\n",
    "        feat_names: List of feature column names to extract\n",
    "    \n",
    "    Returns:\n",
    "        List of feature values as floats\n",
    "    \"\"\"\n",
    "    vec = []\n",
    "    for k in feat_names:\n",
    "        # Get value from row, defaulting to 0.0 if missing or NaN\n",
    "        val = row[k] if (k in row and pd.notna(row[k])) else 0.0\n",
    "        try: \n",
    "            vec.append(float(val))\n",
    "        except Exception: \n",
    "            vec.append(0.0)  # Default to 0.0 if conversion fails\n",
    "    return vec\n",
    "\n",
    "def build_samples_with_priv(df, feat_names=NEW_NUM_FEATS):\n",
    "    \"\"\"\n",
    "    Convert DataFrame rows into training samples with tokens, labels, and privileged features.\n",
    "    \n",
    "    Each sample contains:\n",
    "    - tokens: List of character-level tokens (graphemes) for the word\n",
    "    - y: Binary labels indicating boundary positions (1 = boundary, 0 = no boundary)\n",
    "    - priv: Privileged numeric features (word length, vowel count, etc.)\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with columns including 'Char_split' and feature columns\n",
    "        feat_names: List of privileged feature names to extract\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries, each containing:\n",
    "          {\"tokens\": [...], \"y\": [0/1, ...], \"priv\": [f1, ..., fF]}\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for _, r in df.iterrows():\n",
    "        # Get character-level splits (list of lists of graphemes per morpheme)\n",
    "        cs = safe_list(r[\"Char_split\"])\n",
    "        \n",
    "        # Flatten to get all tokens in sequence\n",
    "        toks = flatten(cs)\n",
    "        \n",
    "        # Calculate boundary positions based on morpheme lengths\n",
    "        lens = [len(seg) for seg in cs]  # Length of each morpheme in tokens\n",
    "        cut_idxs = set(np.cumsum(lens)[:-1].tolist())  # Cumulative positions where boundaries occur\n",
    "        \n",
    "        # Create binary labels: 1 if boundary after token i, 0 otherwise\n",
    "        y = [1 if (i+1) in cut_idxs else 0 for i in range(len(toks)-1)]\n",
    "        \n",
    "        # Extract privileged features\n",
    "        priv = extract_priv_features_from_row(r, feat_names)\n",
    "        \n",
    "        rows.append({\"tokens\": toks, \"y\": y, \"priv\": priv})\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "96de39b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# DECISION TREE PRIOR FUNCTIONS\n",
    "# =========================\n",
    "# These functions create and use a Decision Tree classifier to provide boundary priors\n",
    "# The DT is trained on token-window features (left/right context around each boundary)\n",
    "\n",
    "def featurize_window(tokens, i, k_left=2, k_right=2):\n",
    "    \"\"\"\n",
    "    Extract features for a token-window around position i.\n",
    "    \n",
    "    Creates features based on the local context (left and right tokens) around\n",
    "    a potential boundary position. This includes:\n",
    "    - Token identities (L1, L2, R1, R2)\n",
    "    - CV patterns (consonant/vowel classification)\n",
    "    - Character-level features\n",
    "    \n",
    "    Args:\n",
    "        tokens: List of token strings\n",
    "        i: Position index (boundary is after token i)\n",
    "        k_left: Number of left context tokens to include\n",
    "        k_right: Number of right context tokens to include\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of feature name -> value mappings\n",
    "    \"\"\"\n",
    "    feats = {}\n",
    "    # Left context tokens (before the boundary)\n",
    "    for k in range(1, k_left+1):\n",
    "        idx = i-(k-1)\n",
    "        feats[f\"L{k}\"] = tokens[idx] if idx >= 0 else \"<BOS>\"  # Beginning of sequence\n",
    "    \n",
    "    # Right context tokens (after the boundary)\n",
    "    for k in range(1, k_right+1):\n",
    "        idx = i+k\n",
    "        feats[f\"R{k}\"] = tokens[idx] if idx < len(tokens) else \"<EOS>\"  # End of sequence\n",
    "    \n",
    "    # Helper function to classify characters as vowels or consonants\n",
    "    def is_vowel(ch): \n",
    "        return ch.lower() in \"aeiouáéíóú\"\n",
    "    \n",
    "    # Extract CV (consonant/vowel) patterns from immediate neighbors\n",
    "    L1 = feats[\"L1\"]\n",
    "    R1 = feats[\"R1\"]\n",
    "    feats[\"L1_cv\"] = 'V' if is_vowel(L1[-1]) else 'C'  # Last char of left token\n",
    "    feats[\"R1_cv\"] = 'V' if (R1 != \"<EOS>\" and is_vowel(R1[0])) else 'C'  # First char of right token\n",
    "    \n",
    "    # Character-level features\n",
    "    feats[\"L1_last\"] = L1[-1]  # Last character of left token\n",
    "    feats[\"R1_first\"] = R1[0] if R1 != \"<EOS>\" else \"<EOS>\"  # First character of right token\n",
    "    \n",
    "    return feats\n",
    "\n",
    "def train_dt_prior(samples, max_depth=6, min_leaf=8):\n",
    "    \"\"\"\n",
    "    Train a Decision Tree classifier to predict boundary probabilities.\n",
    "    \n",
    "    The DT learns patterns in token-window features that indicate where\n",
    "    morpheme boundaries are likely to occur. This prior is then used to\n",
    "    guide the LSTM model during training.\n",
    "    \n",
    "    Args:\n",
    "        samples: List of training samples with tokens and boundary labels\n",
    "        max_depth: Maximum depth of the decision tree\n",
    "        min_leaf: Minimum samples required in a leaf node\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (DecisionTreeClassifier, DictVectorizer)\n",
    "        - clf: Trained decision tree classifier\n",
    "        - vec: Fitted vectorizer for converting feature dicts to vectors\n",
    "    \"\"\"\n",
    "    # Extract features and labels for all boundary positions\n",
    "    Xdict, y = [], []\n",
    "    for s in samples:\n",
    "        T = len(s[\"tokens\"])\n",
    "        # For each potential boundary position (between tokens)\n",
    "        for i in range(T-1):\n",
    "            # Extract window features around position i\n",
    "            Xdict.append(featurize_window(s[\"tokens\"], i))\n",
    "            # Label: 1 if boundary exists, 0 otherwise\n",
    "            y.append(s[\"y\"][i])\n",
    "    \n",
    "    # Convert feature dictionaries to sparse matrix format\n",
    "    vec = DictVectorizer(sparse=True)\n",
    "    X = vec.fit_transform(Xdict)\n",
    "    \n",
    "    # Train Decision Tree classifier\n",
    "    clf = DecisionTreeClassifier(\n",
    "        criterion=\"entropy\",  # Use entropy for splitting\n",
    "        max_depth=max_depth,\n",
    "        min_samples_leaf=min_leaf,\n",
    "        random_state=RNG  # For reproducibility\n",
    "    )\n",
    "    clf.fit(X, y)\n",
    "    \n",
    "    print(f\"Trained DT prior: {clf.tree_.node_count} nodes, depth={clf.tree_.max_depth}\")\n",
    "    return clf, vec\n",
    "\n",
    "def prior_probs_for_sample(clf, vec, tokens):\n",
    "    \"\"\"\n",
    "    Get boundary probabilities from Decision Tree for a tokenized word.\n",
    "    \n",
    "    Args:\n",
    "        clf: Trained DecisionTreeClassifier\n",
    "        vec: Fitted DictVectorizer\n",
    "        tokens: List of token strings for a word\n",
    "    \n",
    "    Returns:\n",
    "        List of probabilities (one per potential boundary position)\n",
    "    \"\"\"\n",
    "    if clf is None or vec is None or len(tokens) <= 1:\n",
    "        # Default to 0.5 (uncertain) if no prior available\n",
    "        return [0.5] * (max(len(tokens)-1, 0))\n",
    "    \n",
    "    # Extract features for each boundary position\n",
    "    Xd = [featurize_window(tokens, i) for i in range(len(tokens)-1)]\n",
    "    X = vec.transform(Xd)  # Convert to feature matrix\n",
    "    \n",
    "    # Get probability predictions from DT\n",
    "    proba = clf.predict_proba(X)  # Returns [P(no_boundary), P(boundary)]\n",
    "    return proba[:, 1].tolist()  # Return probability of boundary (class 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "880ff69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_k_teacher_priv(samples, feat_dim):\n",
    "    \"\"\"\n",
    "    Train a regressor to predict K (number of cuts) from priv feature vector.\n",
    "    \"\"\"\n",
    "    X = np.array([s[\"priv\"] for s in samples], dtype=float)   # (N, F)\n",
    "    y = np.array([int(np.sum(s[\"y\"])) for s in samples], dtype=float)\n",
    "    reg = DecisionTreeRegressor(max_depth=6, min_samples_leaf=10, random_state=RNG)\n",
    "    reg.fit(X, y)\n",
    "    return reg\n",
    "\n",
    "def predict_k_hat_priv(reg, priv_batch):\n",
    "    # priv_batch: (B, F) float tensor\n",
    "    with torch.no_grad():\n",
    "        k = reg.predict(priv_batch.cpu().numpy())\n",
    "    return torch.tensor(k, dtype=torch.float32, device=priv_batch.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8b5d05f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(samples, min_freq=1):\n",
    "    from collections import Counter\n",
    "    ctr = Counter()\n",
    "    for s in samples: ctr.update(s[\"tokens\"])\n",
    "    vocab = {\"<PAD>\":0, \"<UNK>\":1}\n",
    "    for t,c in sorted(ctr.items(), key=lambda x: (-x[1], x[0])):\n",
    "        if c>=min_freq and t not in vocab:\n",
    "            vocab[t] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "class SegDataset(Dataset):\n",
    "    def __init__(self, samples, vocab, dt_clf=None, dt_vec=None, feat_dim=0):\n",
    "        self.samples = samples\n",
    "        self.vocab = vocab\n",
    "        self.dt_clf = dt_clf\n",
    "        self.dt_vec = dt_vec\n",
    "        self.feat_dim = feat_dim\n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, idx):\n",
    "        s = self.samples[idx]\n",
    "        tokens = s[\"tokens\"]\n",
    "        ids = [self.vocab.get(t, self.vocab[\"<UNK>\"]) for t in tokens]\n",
    "        y = s[\"y\"]  # length T-1\n",
    "        prior = prior_probs_for_sample(self.dt_clf, self.dt_vec, tokens)\n",
    "        priv = s[\"priv\"] if self.feat_dim>0 else []\n",
    "        return {\"ids\": ids, \"y\": y, \"prior\": prior, \"priv\": priv, \"tokens\": tokens}\n",
    "\n",
    "def collate(batch):\n",
    "    maxT = max(len(b[\"ids\"]) for b in batch)\n",
    "    maxB = maxT-1\n",
    "    B = len(batch)\n",
    "\n",
    "    ids = torch.full((B, maxT), 0, dtype=torch.long)\n",
    "    mask_tok = torch.zeros((B, maxT), dtype=torch.bool)\n",
    "    y = torch.full((B, maxB), -100, dtype=torch.long)\n",
    "    prior = torch.zeros((B, maxB), dtype=torch.float32)\n",
    "    mask_b = torch.zeros((B, maxB), dtype=torch.bool)\n",
    "\n",
    "    feat_dim = len(batch[0][\"priv\"]) if isinstance(batch[0][\"priv\"], list) else 0\n",
    "    priv = torch.zeros((B, feat_dim), dtype=torch.float32) if feat_dim>0 else None\n",
    "\n",
    "    for i, b in enumerate(batch):\n",
    "        T = len(b[\"ids\"])\n",
    "        ids[i,:T] = torch.tensor(b[\"ids\"], dtype=torch.long)\n",
    "        mask_tok[i,:T] = True\n",
    "        if T>1:\n",
    "            L = T-1\n",
    "            y[i,:L] = torch.tensor(b[\"y\"], dtype=torch.long)\n",
    "            p = b[\"prior\"] if len(b[\"prior\"])==L else [0.5]*L\n",
    "            prior[i,:L] = torch.tensor(p, dtype=torch.float32)\n",
    "            mask_b[i,:L] = True\n",
    "        if feat_dim>0:\n",
    "            priv[i] = torch.tensor(b[\"priv\"], dtype=torch.float32)\n",
    "\n",
    "    return {\n",
    "        \"ids\": ids, \"mask_tok\": mask_tok,\n",
    "        \"y\": y, \"prior\": prior, \"mask_b\": mask_b,\n",
    "        \"priv\": priv  # (B, F) or None\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "79f1c9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=16, hidden_size=64, num_layers=2,\n",
    "                 use_prior=True, dropout=0.1, freeze_emb=False, fuse_mode=\"logit_add\"):\n",
    "        super().__init__()\n",
    "        self.use_prior = use_prior\n",
    "        self.fuse_mode = fuse_mode\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        if freeze_emb:\n",
    "            for p in self.emb.parameters(): p.requires_grad = False\n",
    "        lstm_dropout = dropout if num_layers > 1 else 0.0\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=emb_dim, hidden_size=hidden_size//2,\n",
    "            num_layers=num_layers, dropout=lstm_dropout,\n",
    "            bidirectional=True, batch_first=True\n",
    "        )\n",
    "        in_mlp = hidden_size + (1 if (use_prior and fuse_mode==\"concat\") else 0)\n",
    "        self.boundary_mlp = nn.Sequential(\n",
    "            nn.Linear(in_mlp, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size // 2, 2)\n",
    "        )\n",
    "        if use_prior and fuse_mode == \"logit_add\":\n",
    "            self.alpha = nn.Parameter(torch.tensor(1.0))\n",
    "\n",
    "    def forward(self, ids, prior, mask_tok):\n",
    "        emb = self.emb(ids)\n",
    "        h, _ = self.lstm(emb)          # (B,T,H)\n",
    "        left = h[:, :-1, :]            # (B,T-1,H)\n",
    "        if self.use_prior and self.fuse_mode == \"concat\":\n",
    "            feat = torch.cat([left, prior.unsqueeze(-1)], dim=-1)\n",
    "            return self.boundary_mlp(feat)\n",
    "        logits = self.boundary_mlp(left)\n",
    "        if self.use_prior and self.fuse_mode == \"logit_add\":\n",
    "            eps = 1e-6\n",
    "            p = prior.clamp(eps, 1-eps)\n",
    "            prior_logit = torch.log(p) - torch.log(1-p)\n",
    "            logits[..., 1] = logits[..., 1] + self.alpha * prior_logit\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0619d1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boundary_metrics_from_lists(probs_list, gold_list, thr=0.5):\n",
    "    if not probs_list: return 0.0,0.0,0.0\n",
    "    p = torch.cat([t for t in probs_list if t.numel()>0], dim=0).numpy()\n",
    "    g = torch.cat([t for t in gold_list if t.numel()>0], dim=0).numpy()\n",
    "    pred = (p >= thr).astype(int)\n",
    "    P,R,F1,_ = precision_recall_fscore_support(g, pred, average='binary', zero_division=0)\n",
    "    return P,R,F1\n",
    "\n",
    "def exact_match_rate_from_lists(probs_list, gold_list, thr=0.5):\n",
    "    if not probs_list: return 0.0\n",
    "    em=[]\n",
    "    for p,g in zip(probs_list, gold_list):\n",
    "        if g.numel()==0: em.append(1.0)\n",
    "        else:\n",
    "            pred = (p.numpy() >= thr).astype(int)\n",
    "            em.append(float(np.array_equal(pred, g.numpy())))\n",
    "    return float(np.mean(em))\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict(model, loader):\n",
    "    model.eval()\n",
    "    probs_list, gold_list = [], []\n",
    "    for batch in loader:\n",
    "        logits = model(batch[\"ids\"], batch[\"prior\"], batch[\"mask_tok\"])\n",
    "        probs = torch.softmax(logits, dim=-1)[..., 1]      # (B,T-1)\n",
    "        y = batch[\"y\"]; mask = batch[\"mask_b\"]\n",
    "        B = probs.shape[0]\n",
    "        for b in range(B):\n",
    "            L = int(mask[b].sum().item())\n",
    "            if L==0:\n",
    "                probs_list.append(torch.empty(0))\n",
    "                gold_list.append(torch.empty(0, dtype=torch.long))\n",
    "            else:\n",
    "                probs_list.append(probs[b,:L].cpu())\n",
    "                gold_list.append(y[b,:L].cpu())\n",
    "    return probs_list, gold_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e2aa916e",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_ce  = nn.CrossEntropyLoss()\n",
    "criterion_bce = nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
    "mse = nn.MSELoss(reduction=\"mean\")\n",
    "\n",
    "def train_epoch(model, loader, opt, lambda_prior=0.1, lambda_k=0.1, k_reg=None):\n",
    "    model.train()\n",
    "    tot=0; n=0\n",
    "    for batch in loader:\n",
    "        ids, prior, y, mask_b = batch[\"ids\"], batch[\"prior\"], batch[\"y\"], batch[\"mask_b\"]\n",
    "        priv = batch[\"priv\"]  # (B,F) or None\n",
    "\n",
    "        logits = model(ids, prior, batch[\"mask_tok\"])    # (B,T-1,2)\n",
    "        logits_flat = logits[mask_b]                     # (N,2)\n",
    "        y_true = y[mask_b]                               # (N,)\n",
    "\n",
    "        # (1) CE on gold boundaries\n",
    "        loss = criterion_ce(logits_flat, y_true)\n",
    "\n",
    "        # (2) Optional: distill toward DT prior on cut-logit\n",
    "        if lambda_prior > 0:\n",
    "            cut_logit = logits[..., 1]                   # (B,T-1)\n",
    "            prior_flat = prior[mask_b]                   # (N,)\n",
    "            loss_pr = criterion_bce(cut_logit[mask_b], prior_flat)\n",
    "            loss = loss + lambda_prior * loss_pr\n",
    "\n",
    "        # (3) K-regularizer using privileged K-hat\n",
    "        if (lambda_k > 0) and (k_reg is not None) and (priv is not None):\n",
    "            with torch.no_grad():\n",
    "                k_hat = predict_k_hat_priv(k_reg, priv)  # (B,)\n",
    "            # expected number of cuts from model = sum(sigmoid(cut_logit))\n",
    "            cut_logit = logits[..., 1]                   # (B,T-1)\n",
    "            p_cut = torch.sigmoid(cut_logit)             # (B,T-1)\n",
    "            exp_K = p_cut.sum(dim=1)                     # (B,)\n",
    "            loss_k = mse(exp_K, k_hat)\n",
    "            loss = loss + lambda_k * loss_k\n",
    "\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "        tot += loss.item(); n += 1\n",
    "    return tot/max(n,1)\n",
    "\n",
    "def split_train_test(samples, test_ratio=0.2):\n",
    "    n = len(samples); idx = np.arange(n); np.random.shuffle(idx)\n",
    "    cut = int(n*(1-test_ratio))\n",
    "    tr = [samples[i] for i in idx[:cut]]\n",
    "    te = [samples[i] for i in idx[cut:]]\n",
    "    return tr, te\n",
    "\n",
    "def best_threshold_for_exact(probs_list, gold_list, grid=None):\n",
    "    if grid is None: grid = np.linspace(0.3, 0.9, 61)\n",
    "    best_thr, best_em, best_f1 = 0.5, -1.0, 0.0\n",
    "    p_all = np.concatenate([t.numpy() for t in probs_list if t.numel()>0], axis=0)\n",
    "    g_all = np.concatenate([t.numpy() for t in gold_list  if t.numel()>0], axis=0)\n",
    "    for thr in grid:\n",
    "        ems=[]\n",
    "        for p,g in zip(probs_list, gold_list):\n",
    "            if g.numel()==0: ems.append(1.0); continue\n",
    "            ems.append(float(np.array_equal((p.numpy()>=thr).astype(int), g.numpy())))\n",
    "        em = float(np.mean(ems))\n",
    "        pred_all = (p_all>=thr).astype(int)\n",
    "        P,R,F1,_ = precision_recall_fscore_support(g_all, pred_all, average='binary', zero_division=0)\n",
    "        if em>best_em or (np.isclose(em,best_em) and F1>best_f1):\n",
    "            best_thr, best_em, best_f1 = thr, em, F1\n",
    "    print(f\"[Exact-opt threshold] thr={best_thr:.3f} | exact={best_em:.3f} | boundaryF1={best_f1:.3f}\")\n",
    "    return best_thr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "84069f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# MODEL AND DECISION TREE SAVING/LOADING FUNCTIONS\n",
    "# =========================\n",
    "# These functions handle saving and loading trained models and Decision Trees\n",
    "# to avoid retraining. Models are saved to a folder named after the notebook.\n",
    "\n",
    "def generate_model_id(df, epochs, use_prior, fuse_mode, lambda_prior, lambda_k, \n",
    "                     batch_size, hparams, max_depth=6, min_leaf=8):\n",
    "    \"\"\"\n",
    "    Generate a unique identifier for a model based on its training parameters.\n",
    "    This ensures that models with the same parameters can be reused.\n",
    "    \n",
    "    Args:\n",
    "        All training parameters that affect the model\n",
    "        max_depth: Decision Tree max depth\n",
    "        min_leaf: Decision Tree min samples per leaf\n",
    "    \n",
    "    Returns:\n",
    "        A string identifier (hash) for the model\n",
    "    \"\"\"\n",
    "    # Create a dictionary of all parameters\n",
    "    params_dict = {\n",
    "        'epochs': epochs,\n",
    "        'use_prior': use_prior,\n",
    "        'fuse_mode': fuse_mode,\n",
    "        'lambda_prior': lambda_prior,\n",
    "        'lambda_k': lambda_k,\n",
    "        'batch_size': batch_size,\n",
    "        'hparams': hparams,\n",
    "        'max_depth': max_depth,\n",
    "        'min_leaf': min_leaf,\n",
    "        'df_shape': df.shape if df is not None else (0, 0)\n",
    "    }\n",
    "    \n",
    "    # Convert to JSON string and hash it\n",
    "    params_str = json.dumps(params_dict, sort_keys=True)\n",
    "    model_id = hashlib.md5(params_str.encode()).hexdigest()[:16]\n",
    "    return model_id\n",
    "\n",
    "def save_dt_prior(dt_clf, dt_vec, model_id, models_folder=MODELS_FOLDER):\n",
    "    \"\"\"\n",
    "    Save the Decision Tree prior (classifier and vectorizer).\n",
    "    \n",
    "    Args:\n",
    "        dt_clf: Trained DecisionTreeClassifier\n",
    "        dt_vec: Fitted DictVectorizer\n",
    "        model_id: Unique identifier for this model\n",
    "        models_folder: Folder to save models in\n",
    "    \"\"\"\n",
    "    model_dir = os.path.join(models_folder, model_id)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    # Save Decision Tree classifier\n",
    "    dt_clf_path = os.path.join(model_dir, \"dt_clf.pkl\")\n",
    "    with open(dt_clf_path, \"wb\") as f:\n",
    "        pickle.dump(dt_clf, f)\n",
    "    \n",
    "    # Save DictVectorizer\n",
    "    dt_vec_path = os.path.join(model_dir, \"dt_vec.pkl\")\n",
    "    with open(dt_vec_path, \"wb\") as f:\n",
    "        pickle.dump(dt_vec, f)\n",
    "    \n",
    "    print(f\"Decision Tree prior saved to {model_dir}\")\n",
    "\n",
    "def load_dt_prior(model_id, models_folder=MODELS_FOLDER):\n",
    "    \"\"\"\n",
    "    Load the Decision Tree prior (classifier and vectorizer).\n",
    "    \n",
    "    Args:\n",
    "        model_id: Unique identifier for the model\n",
    "        models_folder: Folder where models are saved\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (dt_clf, dt_vec) or (None, None) if not found\n",
    "    \"\"\"\n",
    "    model_dir = os.path.join(models_folder, model_id)\n",
    "    \n",
    "    dt_clf_path = os.path.join(model_dir, \"dt_clf.pkl\")\n",
    "    dt_vec_path = os.path.join(model_dir, \"dt_vec.pkl\")\n",
    "    \n",
    "    if not os.path.exists(dt_clf_path) or not os.path.exists(dt_vec_path):\n",
    "        return None, None\n",
    "    \n",
    "    with open(dt_clf_path, \"rb\") as f:\n",
    "        dt_clf = pickle.load(f)\n",
    "    \n",
    "    with open(dt_vec_path, \"rb\") as f:\n",
    "        dt_vec = pickle.load(f)\n",
    "    \n",
    "    print(f\"Decision Tree prior loaded from {model_dir}\")\n",
    "    return dt_clf, dt_vec\n",
    "\n",
    "def save_model(model, vocab, out, model_id, models_folder=MODELS_FOLDER):\n",
    "    \"\"\"\n",
    "    Save a trained model and its associated artifacts.\n",
    "    \n",
    "    Args:\n",
    "        model: The trained PyTorch model\n",
    "        vocab: Vocabulary dictionary\n",
    "        out: Dictionary containing dt_clf, dt_vec, k_teacher, best_thr, etc.\n",
    "        model_id: Unique identifier for this model\n",
    "        models_folder: Folder to save models in\n",
    "    \"\"\"\n",
    "    model_dir = os.path.join(models_folder, model_id)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    # Save model state\n",
    "    model_path = os.path.join(model_dir, \"model.pt\")\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    \n",
    "    # Save vocabulary\n",
    "    vocab_path = os.path.join(model_dir, \"vocab.pkl\")\n",
    "    with open(vocab_path, \"wb\") as f:\n",
    "        pickle.dump(vocab, f)\n",
    "    \n",
    "    # Save Decision Tree prior separately (if present)\n",
    "    if out.get(\"dt_clf\") is not None and out.get(\"dt_vec\") is not None:\n",
    "        save_dt_prior(out[\"dt_clf\"], out[\"dt_vec\"], model_id, models_folder)\n",
    "    \n",
    "    # Save other artifacts (k_teacher, best_thr, etc.) - exclude dt_clf/dt_vec as they're saved separately\n",
    "    artifacts = {k: v for k, v in out.items() if k not in [\"dt_clf\", \"dt_vec\"]}\n",
    "    artifacts_path = os.path.join(model_dir, \"artifacts.pkl\")\n",
    "    with open(artifacts_path, \"wb\") as f:\n",
    "        pickle.dump(artifacts, f)\n",
    "    \n",
    "    # Save metadata (parameters used)\n",
    "    metadata_path = os.path.join(model_dir, \"metadata.json\")\n",
    "    with open(metadata_path, \"w\") as f:\n",
    "        json.dump({\n",
    "            'model_id': model_id,\n",
    "            'vocab_size': len(vocab),\n",
    "            'model_name': MODEL_NAME\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    print(f\"Model saved to {model_dir}\")\n",
    "    return model_dir\n",
    "\n",
    "def load_model(model_id, models_folder=MODELS_FOLDER, vocab_size=None):\n",
    "    \"\"\"\n",
    "    Load a trained model and its associated artifacts.\n",
    "    \n",
    "    Args:\n",
    "        model_id: Unique identifier for the model\n",
    "        models_folder: Folder where models are saved\n",
    "        vocab_size: Vocabulary size (needed to reconstruct model architecture)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with 'vocab', 'out', 'dt_clf', 'dt_vec', 'model_state_path', 'model_dir' or None if not found\n",
    "    \"\"\"\n",
    "    model_dir = os.path.join(models_folder, model_id)\n",
    "    \n",
    "    if not os.path.exists(model_dir):\n",
    "        return None\n",
    "    \n",
    "    # Load vocabulary\n",
    "    vocab_path = os.path.join(model_dir, \"vocab.pkl\")\n",
    "    if not os.path.exists(vocab_path):\n",
    "        return None\n",
    "    \n",
    "    with open(vocab_path, \"rb\") as f:\n",
    "        vocab = pickle.load(f)\n",
    "    \n",
    "    # Load Decision Tree prior\n",
    "    dt_clf, dt_vec = load_dt_prior(model_id, models_folder)\n",
    "    \n",
    "    # Load other artifacts\n",
    "    artifacts_path = os.path.join(model_dir, \"artifacts.pkl\")\n",
    "    if not os.path.exists(artifacts_path):\n",
    "        return None\n",
    "    \n",
    "    with open(artifacts_path, \"rb\") as f:\n",
    "        artifacts = pickle.load(f)\n",
    "    \n",
    "    # Combine artifacts with DT prior\n",
    "    out = {**artifacts, \"dt_clf\": dt_clf, \"dt_vec\": dt_vec}\n",
    "    \n",
    "    # Load model state\n",
    "    model_path = os.path.join(model_dir, \"model.pt\")\n",
    "    if not os.path.exists(model_path):\n",
    "        return None\n",
    "    \n",
    "    print(f\"Model artifacts loaded from {model_dir}\")\n",
    "    return {\n",
    "        'vocab': vocab,\n",
    "        'out': out,\n",
    "        'dt_clf': dt_clf,\n",
    "        'dt_vec': dt_vec,\n",
    "        'model_state_path': model_path,\n",
    "        'model_dir': model_dir\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "49fd85cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# MAIN TRAINING FUNCTION WITH MODEL AND DECISION TREE CHECKPOINTING\n",
    "# ===================================================================\n",
    "# This function trains a morphology parser model. It checks if a model with\n",
    "# the same parameters already exists and loads it instead of retraining.\n",
    "# It also saves/loads the Decision Tree prior to avoid retraining it.\n",
    "\n",
    "def run_segmentation_with_privK(\n",
    "    df,\n",
    "    epochs=15,\n",
    "    use_prior=True,\n",
    "    fuse_mode=\"logit_add\",\n",
    "    lambda_prior=0.1,     # DT prior distillation weight\n",
    "    lambda_k=0.2,         # privileged K-regularizer weight (try 0.1~0.4)\n",
    "    batch_size=64,\n",
    "    hparams=None,\n",
    "    max_depth=6,          # Decision Tree max depth\n",
    "    min_leaf=8            # Decision Tree min samples per leaf\n",
    "):\n",
    "    \"\"\"\n",
    "    Train or load a morphology parser model with Decision Tree priors.\n",
    "    \n",
    "    This function will:\n",
    "    1. Check if a model with the same parameters already exists\n",
    "    2. If found, load it and return it (skipping training)\n",
    "    3. If not found, train a new model and save it\n",
    "    4. Also saves/loads the Decision Tree prior separately\n",
    "    \n",
    "    Args:\n",
    "        df: Training DataFrame\n",
    "        epochs: Number of training epochs\n",
    "        use_prior: Whether to use Decision Tree prior\n",
    "        fuse_mode: How to fuse prior with model predictions\n",
    "        lambda_prior: Weight for prior distillation loss\n",
    "        lambda_k: Weight for K-regularizer loss\n",
    "        batch_size: Training batch size\n",
    "        hparams: Model hyperparameters dictionary\n",
    "        max_depth: Decision Tree maximum depth\n",
    "        min_leaf: Decision Tree minimum samples per leaf\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (model, vocab, out_dict)\n",
    "    \"\"\"\n",
    "    if hparams is None:\n",
    "        hparams = dict(emb_dim=16, hidden_size=64, num_layers=2,\n",
    "                       dropout=0.25, lr=1e-3, weight_decay=1e-4, freeze_emb=False)\n",
    "    \n",
    "    # Generate model identifier based on parameters\n",
    "    model_id = generate_model_id(\n",
    "        df, epochs, use_prior, fuse_mode, lambda_prior, lambda_k, \n",
    "        batch_size, hparams, max_depth=max_depth, min_leaf=min_leaf\n",
    "    )\n",
    "    \n",
    "    # Try to load existing model\n",
    "    print(f\"Checking for existing model with ID: {model_id}\")\n",
    "    loaded = load_model(model_id, models_folder=MODELS_FOLDER)\n",
    "    \n",
    "    if loaded is not None:\n",
    "        print(f\"✅ Found existing model! Loading from {loaded['model_dir']}\")\n",
    "        # Reconstruct model architecture\n",
    "        vocab = loaded['vocab']\n",
    "        out = loaded['out']\n",
    "        dt_clf = loaded['dt_clf']\n",
    "        dt_vec = loaded['dt_vec']\n",
    "        model_state_path = loaded['model_state_path']\n",
    "        \n",
    "        model = BiLSTMTagger(\n",
    "            vocab_size=len(vocab),\n",
    "            emb_dim=hparams.get(\"emb_dim\", 16),\n",
    "            hidden_size=hparams.get(\"hidden_size\", 64),\n",
    "            num_layers=hparams.get(\"num_layers\", 2),\n",
    "            use_prior=(use_prior and fuse_mode!=\"none\"),\n",
    "            dropout=hparams.get(\"dropout\", 0.25),\n",
    "            freeze_emb=hparams.get(\"freeze_emb\", False),\n",
    "            fuse_mode=fuse_mode\n",
    "        )\n",
    "        \n",
    "        # Load model weights\n",
    "        model.load_state_dict(torch.load(model_state_path))\n",
    "        model.eval()\n",
    "        \n",
    "        print(\"Model and Decision Tree loaded successfully. Skipping training.\")\n",
    "        return model, vocab, out\n",
    "    \n",
    "    # Model doesn't exist, need to train\n",
    "    print(f\"No existing model found. Training new model...\")\n",
    "    \n",
    "    # Build samples with privileged numeric features\n",
    "    samples = build_samples_with_priv(df, feat_names=NEW_NUM_FEATS)\n",
    "    train_s, test_s = split_train_test(samples, 0.2)\n",
    "\n",
    "    # DT prior (token-window) trained on TRAIN ONLY\n",
    "    dt_clf, dt_vec = (None, None)\n",
    "    if use_prior:\n",
    "        # Check if DT prior exists separately (for cases where only DT needs to be reused)\n",
    "        dt_clf, dt_vec = load_dt_prior(model_id, models_folder=MODELS_FOLDER)\n",
    "        if dt_clf is None or dt_vec is None:\n",
    "            print(\"Training new Decision Tree prior...\")\n",
    "            dt_clf, dt_vec = train_dt_prior(train_s, max_depth=max_depth, min_leaf=min_leaf)\n",
    "        else:\n",
    "            print(\"Using existing Decision Tree prior.\")\n",
    "\n",
    "    # K-teacher (privileged) on TRAIN ONLY\n",
    "    feat_dim = len(NEW_NUM_FEATS)\n",
    "    k_reg = train_k_teacher_priv(train_s, feat_dim=feat_dim)\n",
    "\n",
    "    # Build vocabulary from training data\n",
    "    vocab = build_vocab(train_s, min_freq=1)\n",
    "\n",
    "    # Create datasets and data loaders\n",
    "    train_ds = SegDataset(train_s, vocab, dt_clf, dt_vec, feat_dim=feat_dim)\n",
    "    test_ds  = SegDataset(test_s,  vocab, dt_clf, dt_vec, feat_dim=feat_dim)\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  collate_fn=collate)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, collate_fn=collate)\n",
    "\n",
    "    # Initialize BiLSTM model\n",
    "    model = BiLSTMTagger(\n",
    "        vocab_size=len(vocab),\n",
    "        emb_dim=hparams.get(\"emb_dim\", 16),\n",
    "        hidden_size=hparams.get(\"hidden_size\", 64),\n",
    "        num_layers=hparams.get(\"num_layers\", 2),\n",
    "        use_prior=(use_prior and fuse_mode!=\"none\"),\n",
    "        dropout=hparams.get(\"dropout\", 0.25),\n",
    "        freeze_emb=hparams.get(\"freeze_emb\", False),\n",
    "        fuse_mode=fuse_mode\n",
    "    )\n",
    "\n",
    "    # Initialize optimizer\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=hparams.get(\"lr\", 1e-3), weight_decay=hparams.get(\"weight_decay\", 1e-4))\n",
    "\n",
    "    # Training loop\n",
    "    final_probs_list, final_gold_list = None, None\n",
    "    for ep in range(1, epochs+1):\n",
    "        loss = train_epoch(model, train_loader, opt, lambda_prior=lambda_prior, lambda_k=lambda_k, k_reg=k_reg)\n",
    "        probs_list, gold_list = predict(model, test_loader)\n",
    "        P,R,F1 = boundary_metrics_from_lists(probs_list, gold_list, thr=0.5)\n",
    "        EM = exact_match_rate_from_lists(probs_list, gold_list, thr=0.5)\n",
    "        print(f\"Epoch {ep:02d} | loss={loss:.4f} | boundary P/R/F1={P:.3f}/{R:.3f}/{F1:.3f} | exact={EM:.3f}\")\n",
    "        final_probs_list, final_gold_list = probs_list, gold_list\n",
    "\n",
    "    # Find best threshold for exact match rate\n",
    "    best_thr = best_threshold_for_exact(final_probs_list, final_gold_list)\n",
    "\n",
    "    # Prepare output dictionary\n",
    "    out = {\n",
    "        \"probs_list\": final_probs_list,\n",
    "        \"gold_list\": final_gold_list,\n",
    "        \"dt_clf\": dt_clf, \n",
    "        \"dt_vec\": dt_vec,\n",
    "        \"k_teacher\": k_reg,\n",
    "        \"best_thr\": best_thr\n",
    "    }\n",
    "    \n",
    "    # Save the trained model and Decision Tree\n",
    "    print(f\"\\nSaving trained model with ID: {model_id}\")\n",
    "    save_model(model, vocab, out, model_id, models_folder=MODELS_FOLDER)\n",
    "\n",
    "    return model, vocab, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3712be86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_with_vocab(word: str, vocab: dict, max_token_len: int = 4):\n",
    "    i, toks = 0, []\n",
    "    while i < len(word):\n",
    "        matched = None\n",
    "        Lmax = min(max_token_len, len(word)-i)\n",
    "        for L in range(Lmax, 0, -1):\n",
    "            seg = word[i:i+L]\n",
    "            if seg in vocab:\n",
    "                matched = seg; break\n",
    "        toks.append(matched if matched else word[i])\n",
    "        i += len(toks[-1])\n",
    "    return toks\n",
    "\n",
    "@torch.no_grad()\n",
    "def segment_tokens(model, vocab, tokens, dt_clf=None, dt_vec=None, thr=0.5):\n",
    "    ids = torch.tensor([[vocab.get(t, vocab[\"<UNK>\"]) for t in tokens]], dtype=torch.long)\n",
    "    mask_tok = torch.ones_like(ids, dtype=torch.bool)\n",
    "    T = len(tokens)\n",
    "    if T<=1: return \"\".join(tokens), np.array([])\n",
    "    prior_list = prior_probs_for_sample(dt_clf, dt_vec, tokens)\n",
    "    prior = torch.tensor([prior_list], dtype=torch.float32)\n",
    "    logits = model(ids, prior, mask_tok)\n",
    "    probs = torch.softmax(logits, dim=-1)[0, :, 1].cpu().numpy()\n",
    "    cuts = (probs >= thr).astype(int)\n",
    "    out=[]\n",
    "    for i, tok in enumerate(tokens):\n",
    "        out.append(tok)\n",
    "        if i < T-1 and cuts[i]==1: out.append(\"-\")\n",
    "    return \"\".join(out), probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "95dd9027",
   "metadata": {},
   "outputs": [],
   "source": [
    "best = {\n",
    "  \"emb_dim\": 16, \"hidden_size\": 64, \"num_layers\": 2,\n",
    "  \"dropout\": 0.25, \"lr\": 0.001, \"weight_decay\": 0.0001, \"freeze_emb\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "644fe445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for existing model with ID: b4157b221fb77816\n",
      "Decision Tree prior loaded from models_DT-LSTM-MarkovFilter\\b4157b221fb77816\n",
      "Model artifacts loaded from models_DT-LSTM-MarkovFilter\\b4157b221fb77816\n",
      "✅ Found existing model! Loading from models_DT-LSTM-MarkovFilter\\b4157b221fb77816\n",
      "Model and Decision Tree loaded successfully. Skipping training.\n"
     ]
    }
   ],
   "source": [
    "model, vocab, out = run_segmentation_with_privK(\n",
    "    str_df,\n",
    "    epochs=50,\n",
    "    use_prior=True,          # keep DT window prior\n",
    "    fuse_mode=\"logit_add\",   # or \"concat\" / \"none\" / \"logit_add\"\n",
    "    lambda_prior=0.1,        # DT prior distillation\n",
    "    lambda_k=0.2,            # privileged K-regularizer (uses NEW_NUM_FEATS)\n",
    "    batch_size=64,\n",
    "    hparams=best\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "942c49a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['p', 'i', 'k', 'u', 'n', 'a', 's']\n",
      "Boundary probs: [0.0, 0.6949999928474426, 0.0, 0.9259999990463257, 0.0, 0.9929999709129333]\n",
      "Segmentation (thr=0.430): pi-ku-na-s\n"
     ]
    }
   ],
   "source": [
    "word = \"pikunas\"\n",
    "tokens = tokenize_with_vocab(word, vocab, max_token_len=4)\n",
    "thr = out.get(\"best_thr\", 0.5)\n",
    "seg_string, boundary_probs = segment_tokens(model, vocab, tokens, dt_clf=out[\"dt_clf\"], dt_vec=out[\"dt_vec\"], thr=thr)\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Boundary probs:\", np.round(boundary_probs, 3).tolist())\n",
    "print(f\"Segmentation (thr={thr:.3f}):\", seg_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "010699b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Set, Tuple\n",
    "\n",
    "# ---------- helpers to turn segs into boundary sets (char offsets) ----------\n",
    "def offsets_from_morphemes(morphs: List[str]) -> Set[int]:\n",
    "    # boundaries after each morph except the last\n",
    "    offs = []\n",
    "    s = 0\n",
    "    for i, m in enumerate(morphs):\n",
    "        s += len(m)\n",
    "        if i < len(morphs) - 1:\n",
    "            offs.append(s)\n",
    "    return set(offs)\n",
    "\n",
    "def offsets_from_tokens_and_mask(tokens: List[str], mask01: np.ndarray) -> Set[int]:\n",
    "    # boundaries after token i where mask01[i]==1, measured in character offsets\n",
    "    offs = set()\n",
    "    cum = 0\n",
    "    for i, t in enumerate(tokens):\n",
    "        cum += len(t)\n",
    "        if i < len(tokens) - 1 and mask01[i] == 1:\n",
    "            offs.add(cum)\n",
    "    return offs\n",
    "\n",
    "def f1_from_sets(pred: Set[int], gold: Set[int]) -> Tuple[float, float, float, int, int, int]:\n",
    "    tp = len(pred & gold)\n",
    "    fp = len(pred - gold)\n",
    "    fn = len(gold - pred)\n",
    "    P = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    R = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    F1 = 2 * P * R / (P + R) if (P + R) > 0 else 0.0\n",
    "    return P, R, F1, tp, fp, fn\n",
    "\n",
    "def normalize_gold_variants(gold_variants):\n",
    "    \"\"\"\n",
    "    Convert gold_variants to a list format, handling numpy arrays and nested structures.\n",
    "    \"\"\"\n",
    "    if gold_variants is None:\n",
    "        return []\n",
    "    \n",
    "    # If it's a numpy array, convert to list\n",
    "    if isinstance(gold_variants, np.ndarray):\n",
    "        gold_variants = gold_variants.tolist()\n",
    "    \n",
    "    # If it's already a list, ensure nested elements are also lists (not numpy arrays)\n",
    "    if isinstance(gold_variants, list):\n",
    "        normalized = []\n",
    "        for variant in gold_variants:\n",
    "            if isinstance(variant, np.ndarray):\n",
    "                normalized.append(variant.tolist())\n",
    "            elif isinstance(variant, list):\n",
    "                # Recursively normalize nested lists\n",
    "                normalized.append([item.tolist() if isinstance(item, np.ndarray) else item for item in variant])\n",
    "            else:\n",
    "                normalized.append(variant)\n",
    "        return normalized\n",
    "    \n",
    "    return []\n",
    "\n",
    "# ===================================================================\n",
    "# NEW CODE: Suffix Validator Function\n",
    "# ===================================================================\n",
    "\n",
    "def is_segmentation_valid(\n",
    "    segmentation: list[str],\n",
    "    allowed_suffixes: set[str]\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Checks if a segmentation is valid based on a list of allowed suffixes.\n",
    "\n",
    "    The first morpheme is assumed to be the root and is ignored. All subsequent\n",
    "    morphemes must be in the `allowed_suffixes` set.\n",
    "\n",
    "    Args:\n",
    "        segmentation (list[str]): The predicted segmentation, e.g., ['pay', 'kunaq'].\n",
    "        allowed_suffixes (set[str]): A set of valid suffix strings.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the segmentation is valid, False otherwise.\n",
    "    \"\"\"\n",
    "    if len(segmentation) <= 1:\n",
    "        # A word with no splits is always valid.\n",
    "        return True\n",
    "\n",
    "    # Check every morpheme starting from the second one.\n",
    "    for morpheme in segmentation[1:]:\n",
    "        if morpheme not in allowed_suffixes:\n",
    "            return False  # Found a suffix that is not in the allowed list.\n",
    "\n",
    "    return True\n",
    "\n",
    "# ---------- main evaluation ----------\n",
    "# ===================================================================\n",
    "# MODIFIED CODE: Evaluation function with a rejection step\n",
    "# ===================================================================\n",
    "def evaluate_with_rejection(\n",
    "    df, model, vocab, out,\n",
    "    allowed_suffixes: list[str], # <-- New required argument\n",
    "    max_token_len=4,\n",
    "    use_tuned_thr=True,\n",
    "    show_sample=5\n",
    "):\n",
    "    dt_clf, dt_vec = out[\"dt_clf\"], out[\"dt_vec\"]\n",
    "    thr = float(out.get(\"best_thr\", 0.5)) if use_tuned_thr else 0.5\n",
    "\n",
    "    # Convert the list to a set for fast lookups\n",
    "    allowed_suffixes_set = set(allowed_suffixes)\n",
    "\n",
    "    total_tp = total_fp = total_fn = 0\n",
    "    exact_hits = 0\n",
    "    n_eval = 0\n",
    "    rejection_count = 0  # <-- Counter for rejected predictions\n",
    "    examples = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        word = str(row[\"Word\"])\n",
    "        gold_variants = row[\"Gold\"]\n",
    "\n",
    "        # Normalize gold_variants (convert numpy arrays to lists)\n",
    "        gold_variants = normalize_gold_variants(gold_variants)\n",
    "\n",
    "        if not isinstance(gold_variants, list) or len(gold_variants) == 0:\n",
    "            continue\n",
    "\n",
    "        # 1. Get the model's prediction\n",
    "        toks = tokenize_with_vocab(word, vocab, max_token_len=max_token_len)\n",
    "        seg_string, probs = segment_tokens(model, vocab, toks, dt_clf=dt_clf, dt_vec=dt_vec, thr=thr)\n",
    "        predicted_morphs = seg_string.split('-')\n",
    "\n",
    "        # 2. Validate the prediction using the suffix list\n",
    "        if is_segmentation_valid(predicted_morphs, allowed_suffixes_set):\n",
    "            # VALID: Score it normally\n",
    "            mask01 = (probs >= thr).astype(int)\n",
    "            pred_set = offsets_from_tokens_and_mask(toks, mask01)\n",
    "        else:\n",
    "            # REJECTED: Treat as a complete failure (no boundaries found)\n",
    "            rejection_count += 1\n",
    "            pred_set = set() # An empty set means 0 true positives and 0 false positives.\n",
    "\n",
    "        # 3. Compare with gold standard (this part is the same)\n",
    "        gold_sets = [offsets_from_morphemes(gv) for gv in gold_variants]\n",
    "        if any(pred_set == gs for gs in gold_sets):\n",
    "            exact_hits += 1\n",
    "\n",
    "        best = max((f1_from_sets(pred_set, gs) + (gs,) for gs in gold_sets), key=lambda z: z[2])\n",
    "        P, R, F1, tp, fp, fn, best_gs = best\n",
    "\n",
    "        total_tp += tp\n",
    "        total_fp += fp\n",
    "        total_fn += fn\n",
    "        n_eval += 1\n",
    "\n",
    "        if len(examples) < show_sample:\n",
    "            # reconstruct a nice gold string for the best variant\n",
    "            best_morphs = None\n",
    "            for gv in gold_variants:\n",
    "                if offsets_from_morphemes(gv) == best_gs:\n",
    "                    best_morphs = gv; break\n",
    "            gold_str = \"-\".join(best_morphs) if best_morphs else \"(ambig)\"\n",
    "            examples.append({\n",
    "                \"word\": word,\n",
    "                \"tokens\": toks,\n",
    "                \"pred_seg\": seg_string,\n",
    "                \"gold_best\": gold_str,\n",
    "                \"P\": round(P,3), \"R\": round(R,3), \"F1\": round(F1,3)\n",
    "            })\n",
    "\n",
    "    # --- Final Metrics ---\n",
    "    micro_P = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0.0\n",
    "    micro_R = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0.0\n",
    "    micro_F1 = 2 * micro_P * micro_R / (micro_P + micro_R) if (micro_P + micro_R) > 0 else 0.0\n",
    "    exact_rate = exact_hits / n_eval if n_eval > 0 else 0.0\n",
    "\n",
    "    print(f\"Evaluated {n_eval} words\")\n",
    "    print(f\"Predictions Rejected by Suffix Validator: {rejection_count} ({rejection_count/n_eval:.2%})\")\n",
    "    print(\"--- Final Scores (with rejections counted as failures) ---\")\n",
    "    print(f\"Boundary (micro)  P/R/F1 = {micro_P:.3f}/{micro_R:.3f}/{micro_F1:.3f}\")\n",
    "    print(f\"Exact-match rate  = {exact_rate:.3f}\")\n",
    "    if examples:\n",
    "        print(\"\\nSample predictions:\")\n",
    "        for ex in examples:\n",
    "            print(f\"- {ex['word']}\\n  tokens: {ex['tokens']}\\n  pred  : {ex['pred_seg']}\\n  gold  : {ex['gold_best']}\\n  P/R/F1: {ex['P']}/{ex['R']}/{ex['F1']}\\n\")\n",
    "\n",
    "    # ... (code for printing examples remains the same) ...\n",
    "    return { \"micro_f1\": micro_F1, \"exact_match_rate\": exact_rate, \"rejection_count\": rejection_count }\n",
    "\n",
    "# ===================================================================\n",
    "# REVISED CODE: Evaluation function that ignores rejected predictions\n",
    "# ===================================================================\n",
    "\n",
    "def evaluate_and_ignore_rejected(\n",
    "    df, model, vocab, out,\n",
    "    allowed_suffixes: list[str], # Required argument for the validator\n",
    "    max_token_len=4,\n",
    "    use_tuned_thr=True,\n",
    "    show_sample=5\n",
    "):\n",
    "    dt_clf, dt_vec = out[\"dt_clf\"], out[\"dt_vec\"]\n",
    "    thr = float(out.get(\"best_thr\", 0.5)) if use_tuned_thr else 0.5\n",
    "    allowed_suffixes_set = set(allowed_suffixes)\n",
    "\n",
    "    total_tp = total_fp = total_fn = 0\n",
    "    exact_hits = 0\n",
    "    \n",
    "    n_total_words = 0      # Counts all words we attempt to evaluate\n",
    "    n_evaluated_words = 0  # Counts only words with valid, scored predictions\n",
    "    rejection_count = 0\n",
    "    examples = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        word = str(row[\"Word\"])\n",
    "        gold_variants = row[\"Gold\"]\n",
    "\n",
    "        # Normalize gold_variants (convert numpy arrays to lists)\n",
    "        gold_variants = normalize_gold_variants(gold_variants)\n",
    "\n",
    "        if not isinstance(gold_variants, list) or len(gold_variants) == 0:\n",
    "            continue\n",
    "        \n",
    "        n_total_words += 1\n",
    "\n",
    "        # 1. Get the model's prediction\n",
    "        toks = tokenize_with_vocab(word, vocab, max_token_len=max_token_len)\n",
    "        seg_string, probs = segment_tokens(model, vocab, toks, dt_clf=dt_clf, dt_vec=dt_vec, thr=thr)\n",
    "        predicted_morphs = seg_string.split('-')\n",
    "\n",
    "        # 2. Validate the prediction. If invalid, ignore this row and continue.\n",
    "        if not is_segmentation_valid(predicted_morphs, allowed_suffixes_set):\n",
    "            rejection_count += 1\n",
    "            continue  # <-- KEY CHANGE: Skip the rest of the loop for this word\n",
    "\n",
    "        # --- If we reach this point, the prediction is valid and will be scored ---\n",
    "        n_evaluated_words += 1\n",
    "        \n",
    "        # 3. Score the valid prediction\n",
    "        mask01 = (probs >= thr).astype(int)\n",
    "        pred_set = offsets_from_tokens_and_mask(toks, mask01)\n",
    "        gold_sets = [offsets_from_morphemes(gv) for gv in gold_variants]\n",
    "\n",
    "        if any(pred_set == gs for gs in gold_sets):\n",
    "            exact_hits += 1\n",
    "\n",
    "        best = max((f1_from_sets(pred_set, gs) + (gs,) for gs in gold_sets), key=lambda z: z[2])\n",
    "        P, R, F1, tp, fp, fn, best_gs = best\n",
    "\n",
    "        total_tp += tp\n",
    "        total_fp += fp\n",
    "        total_fn += fn\n",
    "\n",
    "        if len(examples) < show_sample:\n",
    "            # reconstruct a nice gold string for the best variant\n",
    "            best_morphs = None\n",
    "            for gv in gold_variants:\n",
    "                if offsets_from_morphemes(gv) == best_gs:\n",
    "                    best_morphs = gv; break\n",
    "            gold_str = \"-\".join(best_morphs) if best_morphs else \"(ambig)\"\n",
    "            examples.append({\n",
    "                \"word\": word,\n",
    "                \"tokens\": toks,\n",
    "                \"pred_seg\": seg_string,\n",
    "                \"gold_best\": gold_str,\n",
    "                \"P\": round(P,3), \"R\": round(R,3), \"F1\": round(F1,3)\n",
    "            })\n",
    "\n",
    "    # --- Final Metrics ---\n",
    "    # Note: Denominators now use n_evaluated_words, which is smaller than n_total_words\n",
    "    micro_P = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0.0\n",
    "    micro_R = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0.0\n",
    "    micro_F1 = 2 * micro_P * micro_R / (micro_P + micro_R) if (micro_P + micro_R) > 0 else 0.0\n",
    "    exact_rate = exact_hits / n_evaluated_words if n_evaluated_words > 0 else 0.0\n",
    "\n",
    "    print(f\"Attempted to evaluate {n_total_words} words\")\n",
    "    print(f\"Predictions Rejected by Suffix Validator: {rejection_count} ({rejection_count/n_total_words:.2%})\")\n",
    "    print(f\"Final scores are based on the remaining {n_evaluated_words} valid predictions.\")\n",
    "    print(\"--- Final Scores (on non-rejected predictions only) ---\")\n",
    "    print(f\"Boundary (micro)  P/R/F1 = {micro_P:.3f}/{micro_R:.3f}/{micro_F1:.3f}\")\n",
    "    print(f\"Exact-match rate  = {exact_rate:.3f}\")\n",
    "\n",
    "    if examples:\n",
    "        print(\"\\nSample predictions:\")\n",
    "        for ex in examples:\n",
    "            print(f\"- {ex['word']}\\n  tokens: {ex['tokens']}\\n  pred  : {ex['pred_seg']}\\n  gold  : {ex['gold_best']}\\n  P/R/F1: {ex['P']}/{ex['R']}/{ex['F1']}\\n\")\n",
    "    return { \"micro_f1\": micro_F1, \"exact_match_rate\": exact_rate, \"rejection_count\": rejection_count }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "439ae857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove words with length > 16\n",
    "# acc_df = acc_df[acc_df['Word'].str.len() <= 14].reset_index(drop=True)\n",
    "\n",
    "# Remove rows where all gold variants have only one morpheme\n",
    "# acc_df = acc_df[acc_df['Gold'].apply(lambda variants: any(len(variant) > 1 for variant in variants))].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "641ca619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Gold</th>\n",
       "      <th>Morph_split</th>\n",
       "      <th>Morph_split_str</th>\n",
       "      <th>Filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>unupas</td>\n",
       "      <td>[[unu, pas]]</td>\n",
       "      <td>[unu, pas]</td>\n",
       "      <td>unu pas</td>\n",
       "      <td>For_Annotation_1_LS.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>umankus</td>\n",
       "      <td>[[uma, nku, s]]</td>\n",
       "      <td>[uma, nku, s]</td>\n",
       "      <td>uma nku s</td>\n",
       "      <td>For_Annotation_1_LS.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hikurin</td>\n",
       "      <td>[[hikuri, n]]</td>\n",
       "      <td>[hikuri, n]</td>\n",
       "      <td>hikuri n</td>\n",
       "      <td>For_Annotation_1_LS.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sutipi</td>\n",
       "      <td>[[suti, pi]]</td>\n",
       "      <td>[suti, pi]</td>\n",
       "      <td>suti pi</td>\n",
       "      <td>For_Annotation_1_LS.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pikunas</td>\n",
       "      <td>[[pi, kuna, s]]</td>\n",
       "      <td>[pi, kuna, s]</td>\n",
       "      <td>pi kuna s</td>\n",
       "      <td>For_Annotation_1_LS.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Word             Gold    Morph_split Morph_split_str  \\\n",
       "0   unupas     [[unu, pas]]     [unu, pas]         unu pas   \n",
       "1  umankus  [[uma, nku, s]]  [uma, nku, s]       uma nku s   \n",
       "2  hikurin    [[hikuri, n]]    [hikuri, n]        hikuri n   \n",
       "3   sutipi     [[suti, pi]]     [suti, pi]         suti pi   \n",
       "4  pikunas  [[pi, kuna, s]]  [pi, kuna, s]       pi kuna s   \n",
       "\n",
       "                  Filename  \n",
       "0  For_Annotation_1_LS.csv  \n",
       "1  For_Annotation_1_LS.csv  \n",
       "2  For_Annotation_1_LS.csv  \n",
       "3  For_Annotation_1_LS.csv  \n",
       "4  For_Annotation_1_LS.csv  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "7cce8edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_suffixes(filename):\n",
    "    suffixes = []\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            # Split into number and suffix part\n",
    "            parts = line.split(maxsplit=1)\n",
    "            if len(parts) == 2:\n",
    "                _, suffix = parts\n",
    "                suffixes.append(suffix[1:])\n",
    "    return suffixes\n",
    "\n",
    "\n",
    "filename = \"data\\suffixesCQ-Anettte-Rios_LS.txt\"  # <-- replace with your file name\n",
    "suffix_list = read_suffixes(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "8c82d09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating with Post-Processing Rejection Filter ---\n",
      "Evaluated 913 words\n",
      "Predictions Rejected by Suffix Validator: 265 (29.03%)\n",
      "--- Final Scores (with rejections counted as failures) ---\n",
      "Boundary (micro)  P/R/F1 = 0.838/0.603/0.702\n",
      "Exact-match rate  = 0.470\n",
      "\n",
      "Sample predictions:\n",
      "- unupas\n",
      "  tokens: ['u', 'n', 'u', 'p', 'a', 's']\n",
      "  pred  : unupa-s\n",
      "  gold  : unu-pas\n",
      "  P/R/F1: 0.0/0.0/0.0\n",
      "\n",
      "- umankus\n",
      "  tokens: ['u', 'm', 'a', 'n', 'k', 'u', 's']\n",
      "  pred  : uma-nku-s\n",
      "  gold  : uma-nku-s\n",
      "  P/R/F1: 1.0/1.0/1.0\n",
      "\n",
      "- hikurin\n",
      "  tokens: ['h', 'i', 'k', 'u', 'r', 'i', 'n']\n",
      "  pred  : hiku-ri-n\n",
      "  gold  : hikuri-n\n",
      "  P/R/F1: 0.5/1.0/0.667\n",
      "\n",
      "- sutipi\n",
      "  tokens: ['s', 'u', 't', 'i', 'p', 'i']\n",
      "  pred  : suti-pi\n",
      "  gold  : suti-pi\n",
      "  P/R/F1: 1.0/1.0/1.0\n",
      "\n",
      "- pikunas\n",
      "  tokens: ['p', 'i', 'k', 'u', 'n', 'a', 's']\n",
      "  pred  : pi-ku-na-s\n",
      "  gold  : pi-kuna-s\n",
      "  P/R/F1: 0.667/1.0/0.8\n",
      "\n",
      "- atipaq\n",
      "  tokens: ['a', 't', 'i', 'p', 'a', 'q']\n",
      "  pred  : atipaq\n",
      "  gold  : ati-paq\n",
      "  P/R/F1: 0.0/0.0/0.0\n",
      "\n",
      "- tomani\n",
      "  tokens: ['t', 'o', 'm', 'a', 'n', 'i']\n",
      "  pred  : toma-ni\n",
      "  gold  : toma-ni\n",
      "  P/R/F1: 1.0/1.0/1.0\n",
      "\n",
      "- rantiq\n",
      "  tokens: ['r', 'a', 'n', 't', 'i', 'q']\n",
      "  pred  : rantiq\n",
      "  gold  : ranti-q\n",
      "  P/R/F1: 0.0/0.0/0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. Call the NEW evaluation function with your suffix list\n",
    "print(\"\\n--- Evaluating with Post-Processing Rejection Filter ---\")\n",
    "results_with_rejection = evaluate_with_rejection(\n",
    "    acc_df,              # The test dataframe\n",
    "    model, vocab, out,   # The trained model and its artifacts\n",
    "    allowed_suffixes=suffix_list, # Your list of rules!\n",
    "    show_sample=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f08cbbfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating with Post-Processing Rejection Filter ---\n",
      "Attempted to evaluate 913 words\n",
      "Predictions Rejected by Suffix Validator: 265 (29.03%)\n",
      "Final scores are based on the remaining 648 valid predictions.\n",
      "--- Final Scores (on non-rejected predictions only) ---\n",
      "Boundary (micro)  P/R/F1 = 0.838/0.899/0.867\n",
      "Exact-match rate  = 0.648\n",
      "\n",
      "Sample predictions:\n",
      "- unupas\n",
      "  tokens: ['u', 'n', 'u', 'p', 'a', 's']\n",
      "  pred  : unupa-s\n",
      "  gold  : unu-pas\n",
      "  P/R/F1: 0.0/0.0/0.0\n",
      "\n",
      "- umankus\n",
      "  tokens: ['u', 'm', 'a', 'n', 'k', 'u', 's']\n",
      "  pred  : uma-nku-s\n",
      "  gold  : uma-nku-s\n",
      "  P/R/F1: 1.0/1.0/1.0\n",
      "\n",
      "- hikurin\n",
      "  tokens: ['h', 'i', 'k', 'u', 'r', 'i', 'n']\n",
      "  pred  : hiku-ri-n\n",
      "  gold  : hikuri-n\n",
      "  P/R/F1: 0.5/1.0/0.667\n",
      "\n",
      "- sutipi\n",
      "  tokens: ['s', 'u', 't', 'i', 'p', 'i']\n",
      "  pred  : suti-pi\n",
      "  gold  : suti-pi\n",
      "  P/R/F1: 1.0/1.0/1.0\n",
      "\n",
      "- pikunas\n",
      "  tokens: ['p', 'i', 'k', 'u', 'n', 'a', 's']\n",
      "  pred  : pi-ku-na-s\n",
      "  gold  : pi-kuna-s\n",
      "  P/R/F1: 0.667/1.0/0.8\n",
      "\n",
      "- atipaq\n",
      "  tokens: ['a', 't', 'i', 'p', 'a', 'q']\n",
      "  pred  : atipaq\n",
      "  gold  : ati-paq\n",
      "  P/R/F1: 0.0/0.0/0.0\n",
      "\n",
      "- tomani\n",
      "  tokens: ['t', 'o', 'm', 'a', 'n', 'i']\n",
      "  pred  : toma-ni\n",
      "  gold  : toma-ni\n",
      "  P/R/F1: 1.0/1.0/1.0\n",
      "\n",
      "- rantiq\n",
      "  tokens: ['r', 'a', 'n', 't', 'i', 'q']\n",
      "  pred  : rantiq\n",
      "  gold  : ranti-q\n",
      "  P/R/F1: 0.0/0.0/0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. Call the NEW evaluation function with your suffix list\n",
    "print(\"\\n--- Evaluating with Post-Processing Rejection Filter ---\")\n",
    "results_with_rejection = evaluate_and_ignore_rejected(\n",
    "    acc_df,              # The test dataframe\n",
    "    model, vocab, out,   # The trained model and its artifacts\n",
    "    allowed_suffixes=suffix_list, # Your list of rules!\n",
    "    show_sample=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "20558653",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Set, Tuple\n",
    "\n",
    "# ---------- helpers to turn segs into boundary sets (char offsets) ----------\n",
    "def offsets_from_morphemes(morphs: List[str]) -> Set[int]:\n",
    "    # boundaries after each morph except the last\n",
    "    offs = []\n",
    "    s = 0\n",
    "    for i, m in enumerate(morphs):\n",
    "        s += len(m)\n",
    "        if i < len(morphs) - 1:\n",
    "            offs.append(s)\n",
    "    return set(offs)\n",
    "\n",
    "def offsets_from_tokens_and_mask(tokens: List[str], mask01: np.ndarray) -> Set[int]:\n",
    "    # boundaries after token i where mask01[i]==1, measured in character offsets\n",
    "    offs = set()\n",
    "    cum = 0\n",
    "    for i, t in enumerate(tokens):\n",
    "        cum += len(t)\n",
    "        if i < len(tokens) - 1 and mask01[i] == 1:\n",
    "            offs.add(cum)\n",
    "    return offs\n",
    "\n",
    "def f1_from_sets(pred: Set[int], gold: Set[int]) -> Tuple[float, float, float, int, int, int]:\n",
    "    tp = len(pred & gold)\n",
    "    fp = len(pred - gold)\n",
    "    fn = len(gold - pred)\n",
    "    P = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    R = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    F1 = 2 * P * R / (P + R) if (P + R) > 0 else 0.0\n",
    "    return P, R, F1, tp, fp, fn\n",
    "\n",
    "def normalize_gold_variants(gold_variants):\n",
    "    \"\"\"\n",
    "    Convert gold_variants to a list format, handling numpy arrays and nested structures.\n",
    "    \"\"\"\n",
    "    if gold_variants is None:\n",
    "        return []\n",
    "    \n",
    "    # If it's a numpy array, convert to list\n",
    "    if isinstance(gold_variants, np.ndarray):\n",
    "        gold_variants = gold_variants.tolist()\n",
    "    \n",
    "    # If it's already a list, ensure nested elements are also lists (not numpy arrays)\n",
    "    if isinstance(gold_variants, list):\n",
    "        normalized = []\n",
    "        for variant in gold_variants:\n",
    "            if isinstance(variant, np.ndarray):\n",
    "                normalized.append(variant.tolist())\n",
    "            elif isinstance(variant, list):\n",
    "                # Recursively normalize nested lists\n",
    "                normalized.append([item.tolist() if isinstance(item, np.ndarray) else item for item in variant])\n",
    "            else:\n",
    "                normalized.append(variant)\n",
    "        return normalized\n",
    "    \n",
    "    return []\n",
    "\n",
    "# ---------- main evaluation ----------\n",
    "def evaluate_on_gold_df(df, model, vocab, out, max_token_len=4, use_tuned_thr=True, show_sample=5):\n",
    "    dt_clf, dt_vec = out[\"dt_clf\"], out[\"dt_vec\"]\n",
    "    thr = float(out.get(\"best_thr\", 0.5)) if use_tuned_thr else 0.5\n",
    "\n",
    "    total_tp = total_fp = total_fn = 0\n",
    "    exact_hits = 0\n",
    "    n_eval = 0\n",
    "    examples = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        word = str(row[\"Word\"])\n",
    "        gold_variants = row[\"Gold\"]  # e.g., [['pi','kuna','s'], ['pi','ku','nas']]\n",
    "\n",
    "        # Normalize gold_variants (convert numpy arrays to lists)\n",
    "        gold_variants = normalize_gold_variants(gold_variants)\n",
    "\n",
    "        # skip if no gold\n",
    "        if not isinstance(gold_variants, list) or len(gold_variants) == 0:\n",
    "            continue\n",
    "\n",
    "        # tokenize & predict\n",
    "        toks = tokenize_with_vocab(word, vocab, max_token_len=max_token_len)\n",
    "        seg_string, probs = segment_tokens(model, vocab, toks, dt_clf=dt_clf, dt_vec=dt_vec, thr=thr)\n",
    "        mask01 = (probs >= thr).astype(int)\n",
    "        pred_set = offsets_from_tokens_and_mask(toks, mask01)\n",
    "\n",
    "        # build gold sets for all variants\n",
    "        gold_sets = [offsets_from_morphemes(gv) for gv in gold_variants]\n",
    "\n",
    "        # exact match if we match ANY gold variant\n",
    "        if any(pred_set == gs for gs in gold_sets):\n",
    "            exact_hits += 1\n",
    "\n",
    "        # choose the gold variant that gives best F1 for this word\n",
    "        best = max((f1_from_sets(pred_set, gs) + (gs,) for gs in gold_sets), key=lambda z: z[2])\n",
    "        P, R, F1, tp, fp, fn, best_gs = best\n",
    "\n",
    "        total_tp += tp; total_fp += fp; total_fn += fn\n",
    "        n_eval += 1\n",
    "\n",
    "        if len(examples) < show_sample:\n",
    "            # reconstruct a nice gold string for the best variant\n",
    "            best_morphs = None\n",
    "            for gv in gold_variants:\n",
    "                if offsets_from_morphemes(gv) == best_gs:\n",
    "                    best_morphs = gv; break\n",
    "            gold_str = \"-\".join(best_morphs) if best_morphs else \"(ambig)\"\n",
    "            examples.append({\n",
    "                \"word\": word,\n",
    "                \"tokens\": toks,\n",
    "                \"pred_seg\": seg_string,\n",
    "                \"gold_best\": gold_str,\n",
    "                \"P\": round(P,3), \"R\": round(R,3), \"F1\": round(F1,3)\n",
    "            })\n",
    "\n",
    "    # micro metrics\n",
    "    micro_P = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0.0\n",
    "    micro_R = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0.0\n",
    "    micro_F1 = 2 * micro_P * micro_R / (micro_P + micro_R) if (micro_P + micro_R) > 0 else 0.0\n",
    "    exact_rate = exact_hits / n_eval if n_eval > 0 else 0.0\n",
    "\n",
    "    print(f\"Evaluated {n_eval} words\")\n",
    "    print(f\"Boundary (micro)  P/R/F1 = {micro_P:.3f}/{micro_R:.3f}/{micro_F1:.3f}\")\n",
    "    print(f\"Exact-match rate  = {exact_rate:.3f}\")\n",
    "    if examples:\n",
    "        print(\"\\nSample predictions:\")\n",
    "        for ex in examples:\n",
    "            print(f\"- {ex['word']}\\n  tokens: {ex['tokens']}\\n  pred  : {ex['pred_seg']}\\n  gold  : {ex['gold_best']}\\n  P/R/F1: {ex['P']}/{ex['R']}/{ex['F1']}\\n\")\n",
    "\n",
    "    return {\n",
    "        \"n_eval\": n_eval,\n",
    "        \"micro_precision\": micro_P,\n",
    "        \"micro_recall\": micro_R,\n",
    "        \"micro_f1\": micro_F1,\n",
    "        \"exact_match_rate\": exact_rate,\n",
    "        \"examples\": examples\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "32b061f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated 913 words\n",
      "Boundary (micro)  P/R/F1 = 0.787/0.845/0.815\n",
      "Exact-match rate  = 0.541\n",
      "\n",
      "Sample predictions:\n",
      "- unupas\n",
      "  tokens: ['u', 'n', 'u', 'p', 'a', 's']\n",
      "  pred  : unupa-s\n",
      "  gold  : unu-pas\n",
      "  P/R/F1: 0.0/0.0/0.0\n",
      "\n",
      "- umankus\n",
      "  tokens: ['u', 'm', 'a', 'n', 'k', 'u', 's']\n",
      "  pred  : uma-nku-s\n",
      "  gold  : uma-nku-s\n",
      "  P/R/F1: 1.0/1.0/1.0\n",
      "\n",
      "- hikurin\n",
      "  tokens: ['h', 'i', 'k', 'u', 'r', 'i', 'n']\n",
      "  pred  : hiku-ri-n\n",
      "  gold  : hikuri-n\n",
      "  P/R/F1: 0.5/1.0/0.667\n",
      "\n",
      "- sutipi\n",
      "  tokens: ['s', 'u', 't', 'i', 'p', 'i']\n",
      "  pred  : suti-pi\n",
      "  gold  : suti-pi\n",
      "  P/R/F1: 1.0/1.0/1.0\n",
      "\n",
      "- pikunas\n",
      "  tokens: ['p', 'i', 'k', 'u', 'n', 'a', 's']\n",
      "  pred  : pi-ku-na-s\n",
      "  gold  : pi-kuna-s\n",
      "  P/R/F1: 0.667/1.0/0.8\n",
      "\n",
      "- atipaq\n",
      "  tokens: ['a', 't', 'i', 'p', 'a', 'q']\n",
      "  pred  : atipaq\n",
      "  gold  : ati-paq\n",
      "  P/R/F1: 0.0/0.0/0.0\n",
      "\n",
      "- tomani\n",
      "  tokens: ['t', 'o', 'm', 'a', 'n', 'i']\n",
      "  pred  : toma-ni\n",
      "  gold  : toma-ni\n",
      "  P/R/F1: 1.0/1.0/1.0\n",
      "\n",
      "- rantiq\n",
      "  tokens: ['r', 'a', 'n', 't', 'i', 'q']\n",
      "  pred  : rantiq\n",
      "  gold  : ranti-q\n",
      "  P/R/F1: 0.0/0.0/0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_on_gold_df(\n",
    "    acc_df,                     # your concatenated DataFrame with Word + Gold (list of variants)\n",
    "    model, vocab, out,      # from training\n",
    "    max_token_len=4,        # must match your tokenize scheme\n",
    "    use_tuned_thr=True,     # use the best threshold found on dev\n",
    "    show_sample=8           # print a few qualitative examples\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "1a5ec1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using existing Decision Tree prior from loaded model\n",
      "🔍 Searching through 7713 words for correct segmentations...\n",
      "\n",
      "======================================================================\n",
      "DT PRIOR PROCESSING: 'umankus' (tokens: ['u', 'm', 'a', 'n', 'k', 'u', 's'])\n",
      "======================================================================\n",
      "\n",
      "Decision Tree Configuration:\n",
      "  Number of nodes: 45\n",
      "  Max depth: 6\n",
      "  Number of features: 170\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "FEATURE EXTRACTION - For each boundary position\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "\n",
      "  Position 0 (boundary after token 'u'):\n",
      "    Left context: L1='u', L2='<BOS>'\n",
      "    Right context: R1='m', R2='a'\n",
      "    CV patterns: L1_cv='V', R1_cv='C'\n",
      "    Characters: L1_last='u', R1_first='m'\n",
      "\n",
      "  Position 1 (boundary after token 'm'):\n",
      "    Left context: L1='m', L2='u'\n",
      "    Right context: R1='a', R2='n'\n",
      "    CV patterns: L1_cv='C', R1_cv='V'\n",
      "    Characters: L1_last='m', R1_first='a'\n",
      "\n",
      "  Position 2 (boundary after token 'a'):\n",
      "    Left context: L1='a', L2='m'\n",
      "    Right context: R1='n', R2='k'\n",
      "    CV patterns: L1_cv='V', R1_cv='C'\n",
      "    Characters: L1_last='a', R1_first='n'\n",
      "\n",
      "  Position 3 (boundary after token 'n'):\n",
      "    Left context: L1='n', L2='a'\n",
      "    Right context: R1='k', R2='u'\n",
      "    CV patterns: L1_cv='C', R1_cv='C'\n",
      "    Characters: L1_last='n', R1_first='k'\n",
      "\n",
      "  Position 4 (boundary after token 'k'):\n",
      "    Left context: L1='k', L2='n'\n",
      "    Right context: R1='u', R2='s'\n",
      "    CV patterns: L1_cv='C', R1_cv='V'\n",
      "    Characters: L1_last='k', R1_first='u'\n",
      "\n",
      "  Position 5 (boundary after token 'u'):\n",
      "    Left context: L1='u', L2='k'\n",
      "    Right context: R1='s', R2='<EOS>'\n",
      "    CV patterns: L1_cv='V', R1_cv='C'\n",
      "    Characters: L1_last='u', R1_first='s'\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "DECISION TREE PREDICTION\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "\n",
      "  For each boundary position, DT outputs:\n",
      "    [P(no_boundary), P(boundary)]\n",
      "\n",
      "  Position 0 (after 'u'):\n",
      "    P(no_boundary) = 1.0000\n",
      "    P(boundary) = 0.0000\n",
      "    Prediction: NO BOUNDARY\n",
      "\n",
      "  Position 1 (after 'm'):\n",
      "    P(no_boundary) = 1.0000\n",
      "    P(boundary) = 0.0000\n",
      "    Prediction: NO BOUNDARY\n",
      "\n",
      "  Position 2 (after 'a'):\n",
      "    P(no_boundary) = 0.5400\n",
      "    P(boundary) = 0.4600\n",
      "    Prediction: NO BOUNDARY\n",
      "\n",
      "  Position 3 (after 'n'):\n",
      "    P(no_boundary) = 0.5400\n",
      "    P(boundary) = 0.4600\n",
      "    Prediction: NO BOUNDARY\n",
      "\n",
      "  Position 4 (after 'k'):\n",
      "    P(no_boundary) = 1.0000\n",
      "    P(boundary) = 0.0000\n",
      "    Prediction: NO BOUNDARY\n",
      "\n",
      "  Position 5 (after 'u'):\n",
      "    P(no_boundary) = 0.0000\n",
      "    P(boundary) = 1.0000\n",
      "    Prediction: BOUNDARY\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "FINAL OUTPUT\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "Word: 'umankus'\n",
      "Tokens: ['u', 'm', 'a', 'n', 'k', 'u', 's']\n",
      "\n",
      "Boundary probabilities:\n",
      "  0.000 0.000 0.460 0.460 0.000 1.000\n",
      "\n",
      "Visualization:\n",
      "  u m a n k u s\n",
      "      | |   ||\n",
      "  0.00 0.00 0.46 0.46 0.00 1.00\n",
      "\n",
      "✅ CORRECT SEGMENTATION: 'umankus'\n",
      "   Predicted: uma-nku-s\n",
      "   Gold:      uma-nku-s\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "DT PRIOR PROCESSING: 'sutipi' (tokens: ['s', 'u', 't', 'i', 'p', 'i'])\n",
      "======================================================================\n",
      "\n",
      "Decision Tree Configuration:\n",
      "  Number of nodes: 45\n",
      "  Max depth: 6\n",
      "  Number of features: 170\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "FEATURE EXTRACTION - For each boundary position\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "\n",
      "  Position 0 (boundary after token 's'):\n",
      "    Left context: L1='s', L2='<BOS>'\n",
      "    Right context: R1='u', R2='t'\n",
      "    CV patterns: L1_cv='C', R1_cv='V'\n",
      "    Characters: L1_last='s', R1_first='u'\n",
      "\n",
      "  Position 1 (boundary after token 'u'):\n",
      "    Left context: L1='u', L2='s'\n",
      "    Right context: R1='t', R2='i'\n",
      "    CV patterns: L1_cv='V', R1_cv='C'\n",
      "    Characters: L1_last='u', R1_first='t'\n",
      "\n",
      "  Position 2 (boundary after token 't'):\n",
      "    Left context: L1='t', L2='u'\n",
      "    Right context: R1='i', R2='p'\n",
      "    CV patterns: L1_cv='C', R1_cv='V'\n",
      "    Characters: L1_last='t', R1_first='i'\n",
      "\n",
      "  Position 3 (boundary after token 'i'):\n",
      "    Left context: L1='i', L2='t'\n",
      "    Right context: R1='p', R2='i'\n",
      "    CV patterns: L1_cv='V', R1_cv='C'\n",
      "    Characters: L1_last='i', R1_first='p'\n",
      "\n",
      "  Position 4 (boundary after token 'p'):\n",
      "    Left context: L1='p', L2='i'\n",
      "    Right context: R1='i', R2='<EOS>'\n",
      "    CV patterns: L1_cv='C', R1_cv='V'\n",
      "    Characters: L1_last='p', R1_first='i'\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "DECISION TREE PREDICTION\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "\n",
      "  For each boundary position, DT outputs:\n",
      "    [P(no_boundary), P(boundary)]\n",
      "\n",
      "  Position 0 (after 's'):\n",
      "    P(no_boundary) = 1.0000\n",
      "    P(boundary) = 0.0000\n",
      "    Prediction: NO BOUNDARY\n",
      "\n",
      "  Position 1 (after 'u'):\n",
      "    P(no_boundary) = 0.5400\n",
      "    P(boundary) = 0.4600\n",
      "    Prediction: NO BOUNDARY\n",
      "\n",
      "  Position 2 (after 't'):\n",
      "    P(no_boundary) = 1.0000\n",
      "    P(boundary) = 0.0000\n",
      "    Prediction: NO BOUNDARY\n",
      "\n",
      "  Position 3 (after 'i'):\n",
      "    P(no_boundary) = 0.2558\n",
      "    P(boundary) = 0.7442\n",
      "    Prediction: BOUNDARY\n",
      "\n",
      "  Position 4 (after 'p'):\n",
      "    P(no_boundary) = 1.0000\n",
      "    P(boundary) = 0.0000\n",
      "    Prediction: NO BOUNDARY\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "FINAL OUTPUT\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "Word: 'sutipi'\n",
      "Tokens: ['s', 'u', 't', 'i', 'p', 'i']\n",
      "\n",
      "Boundary probabilities:\n",
      "  0.000 0.460 0.000 0.744 0.000\n",
      "\n",
      "Visualization:\n",
      "  s u t i p i\n",
      "    |   ||  \n",
      "  0.00 0.46 0.00 0.74 0.00\n",
      "\n",
      "✅ CORRECT SEGMENTATION: 'sutipi'\n",
      "   Predicted: suti-pi\n",
      "   Gold:      suti-pi\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "DT PRIOR PROCESSING: 'tomani' (tokens: ['t', 'o', 'm', 'a', 'n', 'i'])\n",
      "======================================================================\n",
      "\n",
      "Decision Tree Configuration:\n",
      "  Number of nodes: 45\n",
      "  Max depth: 6\n",
      "  Number of features: 170\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "FEATURE EXTRACTION - For each boundary position\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "\n",
      "  Position 0 (boundary after token 't'):\n",
      "    Left context: L1='t', L2='<BOS>'\n",
      "    Right context: R1='o', R2='m'\n",
      "    CV patterns: L1_cv='C', R1_cv='V'\n",
      "    Characters: L1_last='t', R1_first='o'\n",
      "\n",
      "  Position 1 (boundary after token 'o'):\n",
      "    Left context: L1='o', L2='t'\n",
      "    Right context: R1='m', R2='a'\n",
      "    CV patterns: L1_cv='V', R1_cv='C'\n",
      "    Characters: L1_last='o', R1_first='m'\n",
      "\n",
      "  Position 2 (boundary after token 'm'):\n",
      "    Left context: L1='m', L2='o'\n",
      "    Right context: R1='a', R2='n'\n",
      "    CV patterns: L1_cv='C', R1_cv='V'\n",
      "    Characters: L1_last='m', R1_first='a'\n",
      "\n",
      "  Position 3 (boundary after token 'a'):\n",
      "    Left context: L1='a', L2='m'\n",
      "    Right context: R1='n', R2='i'\n",
      "    CV patterns: L1_cv='V', R1_cv='C'\n",
      "    Characters: L1_last='a', R1_first='n'\n",
      "\n",
      "  Position 4 (boundary after token 'n'):\n",
      "    Left context: L1='n', L2='a'\n",
      "    Right context: R1='i', R2='<EOS>'\n",
      "    CV patterns: L1_cv='C', R1_cv='V'\n",
      "    Characters: L1_last='n', R1_first='i'\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "DECISION TREE PREDICTION\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "\n",
      "  For each boundary position, DT outputs:\n",
      "    [P(no_boundary), P(boundary)]\n",
      "\n",
      "  Position 0 (after 't'):\n",
      "    P(no_boundary) = 1.0000\n",
      "    P(boundary) = 0.0000\n",
      "    Prediction: NO BOUNDARY\n",
      "\n",
      "  Position 1 (after 'o'):\n",
      "    P(no_boundary) = 0.2558\n",
      "    P(boundary) = 0.7442\n",
      "    Prediction: BOUNDARY\n",
      "\n",
      "  Position 2 (after 'm'):\n",
      "    P(no_boundary) = 1.0000\n",
      "    P(boundary) = 0.0000\n",
      "    Prediction: NO BOUNDARY\n",
      "\n",
      "  Position 3 (after 'a'):\n",
      "    P(no_boundary) = 0.5400\n",
      "    P(boundary) = 0.4600\n",
      "    Prediction: NO BOUNDARY\n",
      "\n",
      "  Position 4 (after 'n'):\n",
      "    P(no_boundary) = 1.0000\n",
      "    P(boundary) = 0.0000\n",
      "    Prediction: NO BOUNDARY\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "FINAL OUTPUT\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "Word: 'tomani'\n",
      "Tokens: ['t', 'o', 'm', 'a', 'n', 'i']\n",
      "\n",
      "Boundary probabilities:\n",
      "  0.000 0.744 0.000 0.460 0.000\n",
      "\n",
      "Visualization:\n",
      "  t o m a n i\n",
      "    ||   |  \n",
      "  0.00 0.74 0.00 0.46 0.00\n",
      "\n",
      "✅ CORRECT SEGMENTATION: 'tomani'\n",
      "   Predicted: toma-ni\n",
      "   Gold:      toma-ni\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "📊 Summary: Found 3 correct segmentation(s) out of 7 words checked.\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# DEMONSTRATION: Decision Tree Prior Processing with Actual Model\n",
    "# ===================================================================\n",
    "import numpy as np\n",
    "\n",
    "def prior_probs_for_sample_verbose(clf, vec, tokens):\n",
    "    \"\"\"\n",
    "    Get boundary probabilities from Decision Tree with detailed verbose output\n",
    "    showing feature extraction and prediction process.\n",
    "    \"\"\"\n",
    "    if clf is None or vec is None or len(tokens) <= 1:\n",
    "        return [0.5] * (max(len(tokens)-1, 0))\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"DT PRIOR PROCESSING: '{''.join(tokens)}' (tokens: {tokens})\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Show DT configuration\n",
    "    print(f\"\\nDecision Tree Configuration:\")\n",
    "    print(f\"  Number of nodes: {clf.tree_.node_count}\")\n",
    "    print(f\"  Max depth: {clf.tree_.max_depth}\")\n",
    "    print(f\"  Number of features: {clf.n_features_in_}\")\n",
    "    \n",
    "    # Extract features for each boundary position\n",
    "    print(f\"\\n{'─'*70}\")\n",
    "    print(\"FEATURE EXTRACTION - For each boundary position\")\n",
    "    print(f\"{'─'*70}\")\n",
    "    \n",
    "    Xd = []\n",
    "    for i in range(len(tokens)-1):\n",
    "        feats = featurize_window(tokens, i)\n",
    "        Xd.append(feats)\n",
    "        \n",
    "        print(f\"\\n  Position {i} (boundary after token '{tokens[i]}'):\")\n",
    "        print(f\"    Left context: L1='{feats.get('L1', 'N/A')}', L2='{feats.get('L2', 'N/A')}'\")\n",
    "        print(f\"    Right context: R1='{feats.get('R1', 'N/A')}', R2='{feats.get('R2', 'N/A')}'\")\n",
    "        print(f\"    CV patterns: L1_cv='{feats.get('L1_cv', 'N/A')}', R1_cv='{feats.get('R1_cv', 'N/A')}'\")\n",
    "        print(f\"    Characters: L1_last='{feats.get('L1_last', 'N/A')}', R1_first='{feats.get('R1_first', 'N/A')}'\")\n",
    "    \n",
    "    # Convert to feature matrix\n",
    "    X = vec.transform(Xd)\n",
    "    \n",
    "    print(f\"\\n{'─'*70}\")\n",
    "    print(\"DECISION TREE PREDICTION\")\n",
    "    print(f\"{'─'*70}\")\n",
    "    \n",
    "    # Get probability predictions\n",
    "    proba = clf.predict_proba(X)  # Returns [P(no_boundary), P(boundary)]\n",
    "    \n",
    "    print(f\"\\n  For each boundary position, DT outputs:\")\n",
    "    print(f\"    [P(no_boundary), P(boundary)]\")\n",
    "    \n",
    "    priors = []\n",
    "    for i in range(len(tokens)-1):\n",
    "        p_no_boundary = proba[i, 0]\n",
    "        p_boundary = proba[i, 1]\n",
    "        priors.append(p_boundary)\n",
    "        \n",
    "        print(f\"\\n  Position {i} (after '{tokens[i]}'):\")\n",
    "        print(f\"    P(no_boundary) = {p_no_boundary:.4f}\")\n",
    "        print(f\"    P(boundary) = {p_boundary:.4f}\")\n",
    "        \n",
    "        # Show which path through the tree (simplified)\n",
    "        # We can't easily show the full path, but we can show the prediction\n",
    "        prediction = clf.predict(X[i:i+1])[0]\n",
    "        print(f\"    Prediction: {'BOUNDARY' if prediction == 1 else 'NO BOUNDARY'}\")\n",
    "    \n",
    "    # Final summary\n",
    "    print(f\"\\n{'─'*70}\")\n",
    "    print(\"FINAL OUTPUT\")\n",
    "    print(f\"{'─'*70}\")\n",
    "    print(f\"Word: '{''.join(tokens)}'\")\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(f\"\\nBoundary probabilities:\")\n",
    "    print(f\"  {' '.join([f'{p:.3f}' for p in priors])}\")\n",
    "    print(f\"\\nVisualization:\")\n",
    "    print(f\"  {' '.join(tokens)}\")\n",
    "    print(f\"  {' '.join([' ' if p < 0.3 else '|' if p < 0.7 else '||' for p in priors])}\")\n",
    "    print(f\"  {' '.join([f'{p:.2f}' for p in priors])}\")\n",
    "    \n",
    "    return priors\n",
    "\n",
    "# Try to use existing model if available\n",
    "try:\n",
    "    if 'out' in globals() and 'dt_clf' in out and 'dt_vec' in out and 'model' in globals() and 'vocab' in globals():\n",
    "        dt_clf = out['dt_clf']\n",
    "        dt_vec = out['dt_vec']\n",
    "        thr = out.get(\"best_thr\", 0.5)\n",
    "        print(\"✅ Using existing Decision Tree prior from loaded model\")\n",
    "        \n",
    "        # Process words from dataset - only output for correct segmentations\n",
    "        # Try words from acc_df first, then gold_df if needed\n",
    "        max_words_to_show = 3  # Maximum number of correct segmentations to display\n",
    "        correct_count = 0\n",
    "        total_count = 0\n",
    "        \n",
    "        # Combine words from both dataframes (acc_df first, then gold_df)\n",
    "        words_to_try = []\n",
    "        if 'acc_df' in globals() and len(acc_df) > 0:\n",
    "            words_to_try.extend(acc_df['Word'].tolist())\n",
    "        if 'gold_df' in globals() and len(gold_df) > 0:\n",
    "            # Add words from gold_df that aren't already in the list\n",
    "            gold_words = gold_df['Word'].tolist()\n",
    "            words_to_try.extend([w for w in gold_words if w not in words_to_try])\n",
    "        \n",
    "        if len(words_to_try) == 0:\n",
    "            print(\"⚠️  No words found in acc_df or gold_df\")\n",
    "        else:\n",
    "            print(f\"🔍 Searching through {len(words_to_try)} words for correct segmentations...\")\n",
    "            \n",
    "            for word in words_to_try:\n",
    "                try:\n",
    "                    total_count += 1\n",
    "                    \n",
    "                    # Get predicted segmentation\n",
    "                    tokens = tokenize_with_vocab(word, vocab, max_token_len=4)\n",
    "                    seg_string, probs = segment_tokens(model, vocab, tokens, dt_clf=dt_clf, dt_vec=dt_vec, thr=thr)\n",
    "                    predicted_morphs = seg_string.split('-')\n",
    "                    \n",
    "                    # Normalize predicted morphs to lowercase\n",
    "                    pred_normalized = [m.lower().strip() for m in predicted_morphs if m.strip()]\n",
    "                    \n",
    "                    # Get gold segmentation from acc_df (test data)\n",
    "                    gold_row = acc_df[acc_df['Word'] == word] if 'acc_df' in globals() else pd.DataFrame()\n",
    "                    if len(gold_row) == 0:\n",
    "                        # Try gold_df as fallback\n",
    "                        gold_row = gold_df[gold_df['Word'] == word] if 'gold_df' in globals() else pd.DataFrame()\n",
    "                        if len(gold_row) == 0:\n",
    "                            continue  # Skip silently if word not found\n",
    "                        # Use Morph_split from gold_df\n",
    "                        gold_morphs = gold_row['Morph_split'].iloc[0]\n",
    "                        if not isinstance(gold_morphs, list):\n",
    "                            gold_morphs = list(gold_morphs) if hasattr(gold_morphs, '__iter__') else [str(gold_morphs)]\n",
    "                        gold_variants = [gold_morphs]\n",
    "                    else:\n",
    "                        # Use Gold column from acc_df (list of variants)\n",
    "                        gold_variants_raw = gold_row['Gold'].iloc[0]\n",
    "                        # Normalize gold_variants (handle numpy arrays, nested structures)\n",
    "                        gold_variants = normalize_gold_variants(gold_variants_raw)\n",
    "                        if not isinstance(gold_variants, list) or len(gold_variants) == 0:\n",
    "                            continue  # Skip silently if no valid gold variants\n",
    "                    \n",
    "                    # Check if prediction matches any gold variant exactly\n",
    "                    is_correct = False\n",
    "                    matched_gold = None\n",
    "                    for gold_variant in gold_variants:\n",
    "                        if not isinstance(gold_variant, list):\n",
    "                            gold_variant = list(gold_variant) if hasattr(gold_variant, '__iter__') else [str(gold_variant)]\n",
    "                        gold_normalized = [m.lower().strip() for m in gold_variant if m.strip()]\n",
    "                        if pred_normalized == gold_normalized:\n",
    "                            is_correct = True\n",
    "                            matched_gold = gold_variant\n",
    "                            break\n",
    "                    \n",
    "                    if is_correct:\n",
    "                        correct_count += 1\n",
    "                        # Only output verbose information for correct segmentations\n",
    "                        priors = prior_probs_for_sample_verbose(dt_clf, dt_vec, tokens)\n",
    "                        print(f\"\\n✅ CORRECT SEGMENTATION: '{word}'\")\n",
    "                        print(f\"   Predicted: {seg_string}\")\n",
    "                        print(f\"   Gold:      {'-'.join(matched_gold)}\")\n",
    "                        print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "                        \n",
    "                        # Stop after finding max_words_to_show correct segmentations\n",
    "                        if correct_count >= max_words_to_show:\n",
    "                            break\n",
    "                    # Silently skip incorrect segmentations\n",
    "                            \n",
    "                except Exception as e:\n",
    "                    # Silently skip errors, continue to next word\n",
    "                    continue\n",
    "            \n",
    "            print(f\"\\n📊 Summary: Found {correct_count} correct segmentation(s) out of {total_count} words checked.\")\n",
    "            if correct_count == 0:\n",
    "                print(\"   No correct segmentations found. Try checking more words or adjusting the threshold.\")\n",
    "        \n",
    "    else:\n",
    "        print(\"⚠️  No model found in memory.\")\n",
    "        print(\"\\nTo use this demonstration:\")\n",
    "        print(\"1. First run your model training/loading cell\")\n",
    "        print(\"2. Then run this cell again\")\n",
    "        print(\"\\nAlternatively, you can manually specify:\")\n",
    "        print(\"  dt_clf = out['dt_clf']\")\n",
    "        print(\"  dt_vec = out['dt_vec']\")\n",
    "        print(\"  tokens = ['p', 'i', 'k', 'u', 'n', 'a', 's']\")\n",
    "        print(\"  priors = prior_probs_for_sample_verbose(dt_clf, dt_vec, tokens)\")\n",
    "except NameError as e:\n",
    "    print(f\"❌ {e}\")\n",
    "    print(\"\\nTo use this demonstration:\")\n",
    "    print(\"1. First run your model training/loading cell\")\n",
    "    print(\"2. Then run this cell again\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b891aaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
