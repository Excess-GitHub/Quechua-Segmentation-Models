{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3e0a341",
   "metadata": {},
   "source": [
    "# Segmenter-Morfessor: BiLSTM with Morfessor Ensemble Priors\n",
    "\n",
    "Grapheme-level BiLSTM for Quechua morphological segmentation using Morfessor ensemble boundary probabilities as an additional feature channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644aa6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import ast\n",
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "import pickle\n",
    "import random\n",
    "import unicodedata\n",
    "import string\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# Data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "\n",
    "# ML & DL\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Morfessor\n",
    "import morfessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79df4383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATA_FOLDER = \"data\"\n",
    "MODEL_NAME = \"segmenter-morfessor\"\n",
    "MODELS_FOLDER = f\"models_{MODEL_NAME}\"\n",
    "os.makedirs(DATA_FOLDER, exist_ok=True)\n",
    "os.makedirs(MODELS_FOLDER, exist_ok=True)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "# Special tokens\n",
    "PAD, UNK = \"<PAD>\", \"<UNK>\"\n",
    "\n",
    "# Text normalization constants\n",
    "APOSTROPHE_CHARS = {\"'\", \"'\", \"ʼ\", \"‛\", \"`\"}\n",
    "STD_APOS = \"\\u02BC\"\n",
    "_EXTRA_PUNCT = \"±，\"'\"'\"\n",
    "_DELETE = str.maketrans(\"\", \"\", string.punctuation + _EXTRA_PUNCT)\n",
    "\n",
    "# Quechua multigraphs\n",
    "QUECHUA_MULTIGRAPHS = [\n",
    "    \"ch\" + STD_APOS, \"k\" + STD_APOS, \"p\" + STD_APOS, \"q\" + STD_APOS, \"t\" + STD_APOS,\n",
    "    \"ch\", \"ph\", \"qh\", \"kh\", \"ll\", \"rr\", \"sh\",\n",
    "]\n",
    "MG_SET = set(QUECHUA_MULTIGRAPHS)\n",
    "MAX_MG = max((len(mg) for mg in QUECHUA_MULTIGRAPHS), default=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff131f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(s: str) -> str:\n",
    "    \"\"\"Normalize text: NFC compose, lowercase, unify apostrophes, strip punctuation.\"\"\"\n",
    "    s = unicodedata.normalize(\"NFC\", str(s)).lower()\n",
    "    s = \"\".join(STD_APOS if ch in APOSTROPHE_CHARS else ch for ch in s)\n",
    "    s = s.translate(_DELETE).strip()\n",
    "    return s\n",
    "\n",
    "def to_graphemes_quechua(s: str) -> list[str]:\n",
    "    \"\"\"Greedy longest-match multigraph fusion; fallback to Unicode grapheme clusters.\"\"\"\n",
    "    s = normalize_text(s)\n",
    "    tokens, i, n = [], 0, len(s)\n",
    "    while i < n:\n",
    "        match = None\n",
    "        for L in range(MAX_MG, 1, -1):\n",
    "            if i + L <= n:\n",
    "                cand = s[i:i+L]\n",
    "                if cand in MG_SET:\n",
    "                    match = cand\n",
    "                    break\n",
    "        if match:\n",
    "            tokens.append(match)\n",
    "            i += len(match)\n",
    "        else:\n",
    "            m = re.match(r\"\\X\", s[i:])\n",
    "            g = m.group(0)\n",
    "            tokens.append(g)\n",
    "            i += len(g)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399a4e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load gold standard data\n",
    "print(\"loading gold data...\")\n",
    "gold_df = pd.read_parquet(os.path.join(DATA_FOLDER, \"Sue_kalt.parquet\"))\n",
    "gold_df['Word'] = gold_df['word']\n",
    "gold_df['morph'] = gold_df['morph'].str.replace('-', ' ')\n",
    "gold_df['Morph_split_str'] = gold_df['morph']\n",
    "gold_df['Morph_split'] = gold_df['morph'].str.split(' ')\n",
    "gold_df = gold_df[['Word', 'Morph_split', 'Morph_split_str']]\n",
    "gold_df = gold_df.drop_duplicates(subset=['Word']).reset_index(drop=True)\n",
    "gold_df = gold_df.dropna(subset=['Word'])\n",
    "print(f\"got {len(gold_df):,} gold examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620a08e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize words and morphemes to graphemes\n",
    "gold_df['token_seq'] = gold_df['Word'].apply(lambda w: to_graphemes_quechua(w))\n",
    "gold_df['morph_token_splits'] = gold_df['Morph_split'].apply(\n",
    "    lambda var: [to_graphemes_quechua(m) for m in var]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3168ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_boundary_labels_tokens(tokens: list[str], morph_tokens: list[list[str]]) -> list[int]:\n",
    "    \"\"\"Generate binary boundary labels for a word given its morpheme token splits.\"\"\"\n",
    "    labels = [0] * len(tokens)\n",
    "    idx = 0\n",
    "    for mt in morph_tokens[:-1]:\n",
    "        idx += len(mt)\n",
    "        if 0 < idx <= len(tokens):\n",
    "            labels[idx-1] = 1\n",
    "    return labels\n",
    "\n",
    "gold_df['boundary_labels'] = gold_df.apply(\n",
    "    lambda row: get_boundary_labels_tokens(row['token_seq'], row['morph_token_splits']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "gold_df['num_morphemes'] = gold_df['Morph_split'].apply(len)\n",
    "gold_df['word_len_tokens'] = gold_df['token_seq'].apply(len)\n",
    "gold_df['char_seq'] = gold_df['token_seq']  # compatibility alias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6129e467",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MorfessorConfig:\n",
    "    n_models: int = 5\n",
    "    seed_base: int = 123\n",
    "    corpus_min_count: int = 1\n",
    "    lowercase: bool = True\n",
    "\n",
    "class MorfessorBoundaryFeaturizer:\n",
    "    \"\"\"Morfessor ensemble that provides boundary probabilities as features.\"\"\"\n",
    "    def __init__(self, cfg: MorfessorConfig):\n",
    "        self.cfg = cfg\n",
    "        self.models = []\n",
    "        self._fitted = False\n",
    "\n",
    "    def _build_model(self, seed: int):\n",
    "        \"\"\"Build a single Morfessor Baseline model.\"\"\"\n",
    "        io = morfessor.MorfessorIO()\n",
    "        model = morfessor.BaselineModel()\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        return model\n",
    "\n",
    "    def fit(self, words: List[str]):\n",
    "        \"\"\"Train N models with different random shuffles/seeds.\"\"\"\n",
    "        words = [w.lower() if self.cfg.lowercase else w for w in words]\n",
    "        uniq = list(set(words))\n",
    "        rng = np.random.default_rng(self.cfg.seed_base)\n",
    "        self.models = []\n",
    "\n",
    "        for i in range(self.cfg.n_models):\n",
    "            seed = self.cfg.seed_base + i\n",
    "            model = self._build_model(seed)\n",
    "            shuffled = uniq.copy()\n",
    "            rng.shuffle(shuffled)\n",
    "            train_data = [(1, w) for w in shuffled]\n",
    "            model.load_data(train_data)\n",
    "            model.train_batch()\n",
    "            self.models.append(model)\n",
    "\n",
    "        self._fitted = True\n",
    "\n",
    "    @staticmethod\n",
    "    def _boundaries_from_segments(word: str, segments: List[str]) -> List[int]:\n",
    "        \"\"\"Return hard boundary vector over raw characters.\"\"\"\n",
    "        b = [0] * len(word)\n",
    "        pos = 0\n",
    "        for seg_i, seg in enumerate(segments[:-1]):\n",
    "            pos += len(seg)\n",
    "            if 0 <= pos - 1 < len(word):\n",
    "                b[pos - 1] = 1\n",
    "        return b\n",
    "\n",
    "    @staticmethod\n",
    "    def _char_to_token_boundaries(word: str, tokens: List[str], char_bound: List[int]) -> List[float]:\n",
    "        \"\"\"Map char-level boundaries to tokenization.\"\"\"\n",
    "        rebuilt = \"\".join(tokens)\n",
    "        if len(rebuilt) != len(char_bound):\n",
    "            char_bound = (char_bound[:len(rebuilt)] + [0]*(len(rebuilt) - len(char_bound)))[:len(rebuilt)]\n",
    "\n",
    "        token_ends = []\n",
    "        p = 0\n",
    "        for tok in tokens:\n",
    "            p += len(tok)\n",
    "            token_ends.append(p - 1)\n",
    "\n",
    "        probs = [0.0] * len(tokens)\n",
    "        for t_i, char_end in enumerate(token_ends[:-1]):\n",
    "            probs[t_i] = float(char_bound[char_end])\n",
    "        return probs\n",
    "\n",
    "    def boundary_probs_for_word(self, word: str, tokens: List[str]) -> List[float]:\n",
    "        \"\"\"Get boundary probabilities for a single word from ensemble.\"\"\"\n",
    "        assert self._fitted, \"call fit() first\"\n",
    "        word_ = word.lower() if self.cfg.lowercase else word\n",
    "\n",
    "        per_model_probs = []\n",
    "        for m in self.models:\n",
    "            segs, _ = m.viterbi_segment(word_)\n",
    "            char_b = self._boundaries_from_segments(word_, segs)\n",
    "            per_model_probs.append(self._char_to_token_boundaries(word_, tokens, char_b))\n",
    "\n",
    "        if not per_model_probs:\n",
    "            return [0.0] * len(tokens)\n",
    "        probs = np.mean(np.array(per_model_probs), axis=0).tolist()\n",
    "        return probs\n",
    "\n",
    "    def boundary_probs_for_words(self, words: List[str], tok_lists: List[List[str]]) -> List[List[float]]:\n",
    "        \"\"\"Get boundary probabilities for multiple words.\"\"\"\n",
    "        return [self.boundary_probs_for_word(w, toks) for w, toks in zip(words, tok_lists)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbdb6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(seqs: List[List[str]]):\n",
    "    \"\"\"Build vocabulary from grapheme token sequences.\"\"\"\n",
    "    toks = {t for seq in seqs for t in seq}\n",
    "    itos = [PAD, UNK] + sorted(toks)\n",
    "    stoi = {t: i for i, t in enumerate(itos)}\n",
    "    return stoi, itos\n",
    "\n",
    "stoi, itos = build_vocab(gold_df[\"char_seq\"].tolist())\n",
    "print(f\"vocab size: {len(itos)} graphemes\")\n",
    "\n",
    "def encode(seq: List[str]) -> List[int]:\n",
    "    \"\"\"Convert grapheme sequence to integer IDs.\"\"\"\n",
    "    return [stoi.get(t, stoi[UNK]) for t in seq]\n",
    "\n",
    "def encode_labels(labels: List[int]) -> List[int]:\n",
    "    \"\"\"Labels are already 0/1.\"\"\"\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a790f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_morfessor_id(n_models, seed_base, lowercase):\n",
    "    \"\"\"Hash Morfessor config to get unique ID.\"\"\"\n",
    "    params_dict = {\n",
    "        'n_models': n_models,\n",
    "        'seed_base': seed_base,\n",
    "        'lowercase': lowercase\n",
    "    }\n",
    "    params_str = json.dumps(params_dict, sort_keys=True)\n",
    "    return hashlib.md5(params_str.encode()).hexdigest()[:16]\n",
    "\n",
    "def save_morfessor_ensemble(morf_featurizer, morfessor_id, models_folder=MODELS_FOLDER):\n",
    "    \"\"\"Save Morfessor ensemble models.\"\"\"\n",
    "    morfessor_dir = os.path.join(models_folder, f\"morfessor_{morfessor_id}\")\n",
    "    os.makedirs(morfessor_dir, exist_ok=True)\n",
    "    \n",
    "    for i, model in enumerate(morf_featurizer.models):\n",
    "        model_path = os.path.join(morfessor_dir, f\"morfessor_model_{i}.pkl}\")\n",
    "        with open(model_path, 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "    \n",
    "    config_path = os.path.join(morfessor_dir, \"morfessor_config.json\")\n",
    "    with open(config_path, \"w\") as f:\n",
    "        json.dump({\n",
    "            'n_models': morf_featurizer.cfg.n_models,\n",
    "            'seed_base': morf_featurizer.cfg.seed_base,\n",
    "            'lowercase': morf_featurizer.cfg.lowercase,\n",
    "            'morfessor_id': morfessor_id\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    print(f\"saved morfessor ensemble to {morfessor_dir}\")\n",
    "    return morfessor_dir\n",
    "\n",
    "def load_morfessor_ensemble(morfessor_id, models_folder=MODELS_FOLDER):\n",
    "    \"\"\"Load Morfessor ensemble models.\"\"\"\n",
    "    morfessor_dir = os.path.join(models_folder, f\"morfessor_{morfessor_id}\")\n",
    "    config_path = os.path.join(morfessor_dir, \"morfessor_config.json\")\n",
    "    \n",
    "    if not os.path.exists(config_path):\n",
    "        return None\n",
    "    \n",
    "    with open(config_path, \"r\") as f:\n",
    "        config_data = json.load(f)\n",
    "    \n",
    "    m_cfg = MorfessorConfig(\n",
    "        n_models=config_data['n_models'],\n",
    "        seed_base=config_data['seed_base'],\n",
    "        lowercase=config_data['lowercase']\n",
    "    )\n",
    "    morf = MorfessorBoundaryFeaturizer(m_cfg)\n",
    "    \n",
    "    morf.models = []\n",
    "    for i in range(m_cfg.n_models):\n",
    "        model_path = os.path.join(morfessor_dir, f\"morfessor_model_{i}.pkl\")\n",
    "        if os.path.exists(model_path):\n",
    "            with open(model_path, 'rb') as f:\n",
    "                morf.models.append(pickle.load(f))\n",
    "        else:\n",
    "            print(f\"warning: morfessor model {i} not found at {model_path}\")\n",
    "            return None\n",
    "    \n",
    "    morf._fitted = True\n",
    "    print(f\"loaded morfessor ensemble from {morfessor_dir}\")\n",
    "    return morf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bc0013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train or load Morfessor ensemble\n",
    "m_cfg = MorfessorConfig(n_models=5, seed_base=123, lowercase=True)\n",
    "morfessor_id = generate_morfessor_id(m_cfg.n_models, m_cfg.seed_base, m_cfg.lowercase)\n",
    "\n",
    "print(f\"looking for morfessor ensemble {morfessor_id}...\")\n",
    "morf = load_morfessor_ensemble(morfessor_id, models_folder=MODELS_FOLDER)\n",
    "\n",
    "if morf is None:\n",
    "    print(f\"not found, training...\")\n",
    "    morf = MorfessorBoundaryFeaturizer(m_cfg)\n",
    "    morf.fit(gold_df[\"Word\"].tolist())\n",
    "    save_morfessor_ensemble(morf, morfessor_id, models_folder=MODELS_FOLDER)\n",
    "    print(f\"morfessor ensemble training done! saved with ID: {morfessor_id}\")\n",
    "else:\n",
    "    print(f\"using existing morfessor ensemble (ID: {morfessor_id})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546849dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharBoundaryDatasetMorf(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset with precomputed Morfessor boundary probabilities.\"\"\"\n",
    "    def __init__(self, df, morf_featurizer: MorfessorBoundaryFeaturizer, stoi):\n",
    "        self.words = df[\"Word\"].tolist()\n",
    "        self.x = df[\"char_seq\"].tolist()\n",
    "        self.y = df[\"boundary_labels\"].tolist()\n",
    "        self.morf = morf_featurizer.boundary_probs_for_words(self.words, self.x)\n",
    "        self.stoi = stoi\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx], self.morf[idx]\n",
    "\n",
    "def pad_batch_with_morf(batch, pad_id=0):\n",
    "    \"\"\"Collate function: pads sequences and Morfessor features.\"\"\"\n",
    "    seqs, labels, morf = zip(*batch)\n",
    "\n",
    "    x_ids = [[stoi.get(t, stoi[\"<UNK>\"]) for t in s] for s in seqs]\n",
    "    y_ids = [lab for lab in labels]\n",
    "    m_probs = [mp for mp in morf]\n",
    "\n",
    "    lengths = [len(x) for x in x_ids]\n",
    "    maxlen = max(lengths)\n",
    "\n",
    "    x_pad = [xi + [pad_id] * (maxlen - len(xi)) for xi in x_ids]\n",
    "    y_pad = [yi + [0] * (maxlen - len(yi)) for yi in y_ids]\n",
    "    m_pad = [mi + [0.0] * (maxlen - len(mi)) for mi in m_probs]\n",
    "    mask = [[1] * len(xi) + [0] * (maxlen - len(xi)) for xi in x_ids]\n",
    "\n",
    "    return (\n",
    "        torch.LongTensor(x_pad),\n",
    "        torch.FloatTensor(y_pad),\n",
    "        torch.BoolTensor(mask),\n",
    "        torch.LongTensor(lengths),\n",
    "        torch.FloatTensor(m_pad).unsqueeze(-1),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87c824f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/validation split\n",
    "rng = np.random.default_rng(42)\n",
    "indices = np.arange(len(gold_df))\n",
    "rng.shuffle(indices)\n",
    "split = int(0.9 * len(indices))\n",
    "train_idx, val_idx = indices[:split], indices[split:]\n",
    "\n",
    "train_df = gold_df.iloc[train_idx].reset_index(drop=True)\n",
    "val_df = gold_df.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "print(f\"training: {len(train_df):,} samples\")\n",
    "print(f\"validation: {len(val_df):,} samples\")\n",
    "\n",
    "train_ds = CharBoundaryDatasetMorf(train_df, morf, stoi)\n",
    "val_ds = CharBoundaryDatasetMorf(val_df, morf, stoi)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=pad_batch_with_morf)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=pad_batch_with_morf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec31131",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMBoundaryWithMorf(nn.Module):\n",
    "    \"\"\"Bidirectional LSTM with Morfessor feature channel.\"\"\"\n",
    "    def __init__(self, vocab_size: int, emb_dim: int = 16, hidden_size: int = 16,\n",
    "                 num_layers: int = 1, dropout: float = 0.1, freeze_emb: bool = False,\n",
    "                 extra_feat_dim: int = 1):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        if freeze_emb:\n",
    "            for p in self.emb.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        self.in_dim = emb_dim + extra_feat_dim\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.in_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(hidden_size * 2, 1)\n",
    "\n",
    "    def forward(self, x_ids, lengths, extra_feats):\n",
    "        \"\"\"Forward pass: concatenate embeddings with Morfessor features.\"\"\"\n",
    "        emb = self.emb(x_ids)\n",
    "        x_in = torch.cat([emb, extra_feats], -1)\n",
    "\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            x_in, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        packed_out, _ = self.lstm(packed)\n",
    "        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)\n",
    "        out = self.dropout(out)\n",
    "        logits = self.out(out).squeeze(-1)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f6cd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_pos_neg(df_):\n",
    "    \"\"\"Count positive and negative examples for class weighting.\"\"\"\n",
    "    pos = sum(sum(lbls) for lbls in df_['boundary_labels'])\n",
    "    total = sum(len(seq) for seq in df_['char_seq'])\n",
    "    neg = total - pos\n",
    "    return pos, neg\n",
    "\n",
    "pos, neg = count_pos_neg(gold_df)\n",
    "pos_weight_value = float(neg) / max(float(pos), 1.0)\n",
    "\n",
    "def masked_bce_loss(logits, targets, mask):\n",
    "    \"\"\"Compute masked binary cross-entropy loss.\"\"\"\n",
    "    loss_fn = nn.BCEWithLogitsLoss(reduction=\"none\",\n",
    "                                   pos_weight=torch.tensor(pos_weight_value, device=logits.device))\n",
    "    loss_per_token = loss_fn(logits, targets) * mask.float()\n",
    "    denom = mask.float().sum().clamp_min(1.0)\n",
    "    return loss_per_token.sum() / denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3217f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boundary_f1(logits, targets, mask, threshold=0.5):\n",
    "    \"\"\"Compute precision, recall, and F1 for boundary prediction.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        probs = torch.sigmoid(logits)\n",
    "        preds = (probs >= threshold).long()\n",
    "        t = targets.long()\n",
    "        m = mask.long()\n",
    "\n",
    "        tp = ((preds == 1) & (t == 1) & (m == 1)).sum().item()\n",
    "        fp = ((preds == 1) & (t == 0) & (m == 1)).sum().item()\n",
    "        fn = ((preds == 0) & (t == 1) & (m == 1)).sum().item()\n",
    "\n",
    "        prec = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        rec = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1 = 2 * prec * rec / (prec + rec) if (prec + rec) > 0 else 0.0\n",
    "        return prec, rec, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31440a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_boundaries_with_morf(words: List[str], model, stoi, morf_featurizer,\n",
    "                                 threshold=0.5, device=device):\n",
    "    \"\"\"Predict boundary labels for a list of words using Morfessor features.\"\"\"\n",
    "    model.eval()\n",
    "    token_lists = [to_graphemes_quechua(w) for w in words]\n",
    "\n",
    "    x_ids = [[stoi.get(t, stoi[\"<UNK>\"]) for t in toks] for toks in token_lists]\n",
    "    lengths = [len(x) for x in x_ids]\n",
    "    maxlen = max(lengths) if lengths else 0\n",
    "    pad_id = stoi[\"<PAD>\"]\n",
    "    x_pad = [xi + [pad_id] * (maxlen - len(xi)) for xi in x_ids]\n",
    "    mask = [[1] * len(xi) + [0] * (maxlen - len(xi)) for xi in x_ids]\n",
    "\n",
    "    morf_probs = morf_featurizer.boundary_probs_for_words(words, token_lists)\n",
    "    morf_pad = [mp + [0.0] * (maxlen - len(mp)) for mp in morf_probs]\n",
    "    morf_feat = torch.FloatTensor(morf_pad).unsqueeze(-1)\n",
    "\n",
    "    x = torch.LongTensor(x_pad).to(device)\n",
    "    lengths_t = torch.LongTensor(lengths).to(device)\n",
    "    mask_t = torch.BoolTensor(mask).to(device)\n",
    "    morf_feat = morf_feat.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(x, lengths_t, morf_feat)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        preds = (probs >= threshold) & mask_t\n",
    "\n",
    "    out = []\n",
    "    for i, L in enumerate(lengths):\n",
    "        out.append(preds[i, :L].int().tolist())\n",
    "    return out\n",
    "\n",
    "def apply_boundaries_tokens(tokens: list[str], boundary_labels: List[int]) -> List[str]:\n",
    "    \"\"\"Reconstruct morphemes from token list and boundary labels.\"\"\"\n",
    "    segs, start = [], 0\n",
    "    for i, b in enumerate(boundary_labels):\n",
    "        if b == 1:\n",
    "            segs.append(\"\".join(tokens[start:i+1]))\n",
    "            start = i + 1\n",
    "    if start < len(tokens):\n",
    "        segs.append(\"\".join(tokens[start:]))\n",
    "    return segs\n",
    "\n",
    "def normalize_gold_variants(gold_variants):\n",
    "    \"\"\"Convert gold_variants to proper list format.\"\"\"\n",
    "    if gold_variants is None:\n",
    "        return []\n",
    "    if isinstance(gold_variants, np.ndarray):\n",
    "        gold_variants = gold_variants.tolist()\n",
    "    if isinstance(gold_variants, list):\n",
    "        normalized = []\n",
    "        for variant in gold_variants:\n",
    "            if isinstance(variant, np.ndarray):\n",
    "                normalized.append(variant.tolist())\n",
    "            elif isinstance(variant, list):\n",
    "                normalized.append([item.tolist() if isinstance(item, np.ndarray) else item for item in variant])\n",
    "            else:\n",
    "                normalized.append(variant)\n",
    "        return normalized\n",
    "    return []\n",
    "\n",
    "def evaluate_accuracy_morf(df, model, stoi, morf_featurizer, device=\"cpu\", threshold=0.5):\n",
    "    \"\"\"Accuracy = proportion of words where predicted segmentation == any gold variant.\"\"\"\n",
    "    all_words = df[\"Word\"].tolist()\n",
    "    all_gold = df[\"Gold\"].tolist()\n",
    "\n",
    "    all_boundaries = predict_boundaries_with_morf(\n",
    "        all_words, model, stoi, morf_featurizer, threshold=threshold, device=device\n",
    "    )\n",
    "\n",
    "    correct = 0\n",
    "    for word, gold_variants, boundary_labels in zip(all_words, all_gold, all_boundaries):\n",
    "        gold_variants = normalize_gold_variants(gold_variants)\n",
    "        toks = to_graphemes_quechua(word)\n",
    "        predicted = apply_boundaries_tokens(toks, boundary_labels)\n",
    "        if any(predicted == variant for variant in gold_variants):\n",
    "            correct += 1\n",
    "    return correct / len(all_words) if all_words else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656234ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_id(vocab_size, emb_dim, hidden_size, num_layers, dropout,\n",
    "                      freeze_emb, extra_feat_dim, lr, weight_decay, morfessor_id, epochs):\n",
    "    \"\"\"Hash training params to get unique model ID.\"\"\"\n",
    "    params_dict = {\n",
    "        'vocab_size': vocab_size,\n",
    "        'emb_dim': emb_dim,\n",
    "        'hidden_size': hidden_size,\n",
    "        'num_layers': num_layers,\n",
    "        'dropout': dropout,\n",
    "        'freeze_emb': freeze_emb,\n",
    "        'extra_feat_dim': extra_feat_dim,\n",
    "        'lr': lr,\n",
    "        'weight_decay': weight_decay,\n",
    "        'morfessor_id': morfessor_id,\n",
    "        'epochs': epochs\n",
    "    }\n",
    "    params_str = json.dumps(params_dict, sort_keys=True)\n",
    "    return hashlib.md5(params_str.encode()).hexdigest()[:16]\n",
    "\n",
    "def save_model_checkpoint(model, stoi, itos, model_id, models_folder=MODELS_FOLDER,\n",
    "                         suffix=\"\", best_metric_value=None):\n",
    "    \"\"\"Save BiLSTM model checkpoint.\"\"\"\n",
    "    model_dir = os.path.join(models_folder, f\"model_{model_id}\")\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    checkpoint_path = os.path.join(model_dir, f\"bilstm_morfessor{suffix}.pt\")\n",
    "    checkpoint = {\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"stoi\": stoi,\n",
    "        \"itos\": itos,\n",
    "        \"model_id\": model_id,\n",
    "        \"morfessor_id\": morfessor_id\n",
    "    }\n",
    "    if best_metric_value is not None:\n",
    "        checkpoint[\"best_metric_value\"] = best_metric_value\n",
    "    \n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    print(f\"saved checkpoint to {checkpoint_path}\")\n",
    "    return checkpoint_path\n",
    "\n",
    "def load_model_checkpoint(model_id, models_folder=MODELS_FOLDER, suffix=\"\"):\n",
    "    \"\"\"Load BiLSTM model checkpoint.\"\"\"\n",
    "    model_dir = os.path.join(models_folder, f\"model_{model_id}\")\n",
    "    checkpoint_path = os.path.join(model_dir, f\"bilstm_morfessor{suffix}.pt\")\n",
    "    \n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        return None\n",
    "    \n",
    "    checkpoint = torch.load(checkpoint_path, map_location=\"cpu\", weights_only=False)\n",
    "    print(f\"loaded checkpoint from {checkpoint_path}\")\n",
    "    return checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1281126b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "VOCAB_SIZE = len(itos)\n",
    "EMB_DIM = 64\n",
    "HIDDEN_SIZE = 128\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.3\n",
    "FREEZE_EMB = True\n",
    "EXTRA_FEAT_DIM = 1\n",
    "\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "EPOCHS = 20\n",
    "\n",
    "model_id = generate_model_id(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    emb_dim=EMB_DIM,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT,\n",
    "    freeze_emb=FREEZE_EMB,\n",
    "    extra_feat_dim=EXTRA_FEAT_DIM,\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    morfessor_id=morfessor_id,\n",
    "    epochs=EPOCHS\n",
    ")\n",
    "\n",
    "print(f\"model ID: {model_id}\")\n",
    "print(f\"using morfessor ensemble ID: {morfessor_id}\")\n",
    "\n",
    "# Check if model already exists\n",
    "print(f\"looking for model {model_id}...\")\n",
    "checkpoint = load_model_checkpoint(model_id, models_folder=MODELS_FOLDER, suffix=\"_best_f1\")\n",
    "\n",
    "if checkpoint is not None:\n",
    "    print(f\"found it! loading checkpoint...\")\n",
    "    model_loaded = True\n",
    "    saved_stoi = checkpoint[\"stoi\"]\n",
    "    saved_itos = checkpoint[\"itos\"]\n",
    "else:\n",
    "    print(f\"not found, will train new model\")\n",
    "    model_loaded = False\n",
    "    saved_stoi = None\n",
    "    saved_itos = None\n",
    "\n",
    "# Create model\n",
    "model = BiLSTMBoundaryWithMorf(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    emb_dim=EMB_DIM,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT,\n",
    "    freeze_emb=FREEZE_EMB,\n",
    "    extra_feat_dim=EXTRA_FEAT_DIM\n",
    ").to(device)\n",
    "\n",
    "# If model was loaded, restore its state\n",
    "if model_loaded:\n",
    "    model.load_state_dict(checkpoint[\"model_state\"])\n",
    "    print(\"model state restored from checkpoint\")\n",
    "    if saved_stoi == stoi and saved_itos == itos:\n",
    "        print(\"vocabulary matches saved checkpoint\")\n",
    "    else:\n",
    "        print(\"warning: vocabulary mismatch with saved checkpoint\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d9488e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "if model_loaded:\n",
    "    print(\"model already trained and loaded. skipping training.\")\n",
    "    print(\"to retrain, delete the checkpoint or change hyperparameters.\")\n",
    "    best_val_f1 = checkpoint.get(\"best_metric_value\", 0.0)\n",
    "    best_val_acc = checkpoint.get(\"best_metric_value\", 0.0)\n",
    "    print(f\"best F1 from checkpoint: {best_val_f1:.4f}\")\n",
    "    print(f\"best accuracy from checkpoint: {best_val_acc:.4f}\")\n",
    "else:\n",
    "    print(\"starting training...\")\n",
    "    best_val_f1 = 0.0\n",
    "    best_val_acc = 0.0\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        total_tokens = 0\n",
    "        for x, y, mask, lengths, morf_feat in train_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            mask = mask.to(device)\n",
    "            lengths = lengths.to(device)\n",
    "            morf_feat = morf_feat.to(device)\n",
    "\n",
    "            logits = model(x, lengths, morf_feat)\n",
    "            loss = masked_bce_loss(logits, y, mask)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * mask.sum().item()\n",
    "            total_tokens += mask.sum().item()\n",
    "\n",
    "        train_loss = total_loss / max(total_tokens, 1)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, val_tokens = 0.0, 0\n",
    "        all_prec, all_rec, all_f1 = [], [], []\n",
    "        with torch.no_grad():\n",
    "            for x, y, mask, lengths, morf_feat in val_loader:\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                mask = mask.to(device)\n",
    "                lengths = lengths.to(device)\n",
    "                morf_feat = morf_feat.to(device)\n",
    "\n",
    "                logits = model(x, lengths, morf_feat)\n",
    "                loss = masked_bce_loss(logits, y, mask)\n",
    "\n",
    "                val_loss += loss.item() * mask.sum().item()\n",
    "                val_tokens += mask.sum().item()\n",
    "\n",
    "                p, r, f = boundary_f1(logits, y, mask, threshold=0.5)\n",
    "                all_prec.append(p)\n",
    "                all_rec.append(r)\n",
    "                all_f1.append(f)\n",
    "\n",
    "        val_loss = val_loss / max(val_tokens, 1)\n",
    "        prec = np.mean(all_prec) if all_prec else 0.0\n",
    "        rec = np.mean(all_rec) if all_rec else 0.0\n",
    "        f1 = np.mean(all_f1) if all_f1 else 0.0\n",
    "\n",
    "        acc = evaluate_accuracy_morf(acc_df, model, stoi, morf, device=device, threshold=0.5)\n",
    "\n",
    "        print(f\"epoch {epoch:02d} | train_loss={train_loss:.4f}  val_loss={val_loss:.4f}  \"\n",
    "              f\"P={prec:.3f} R={rec:.3f} F1={f1:.3f}  Acc={acc:.3f}\")\n",
    "\n",
    "        if f1 > best_val_f1:\n",
    "            best_val_f1 = f1\n",
    "            save_model_checkpoint(\n",
    "                model, stoi, itos, model_id,\n",
    "                models_folder=MODELS_FOLDER,\n",
    "                suffix=\"_best_f1\",\n",
    "                best_metric_value=best_val_f1\n",
    "            )\n",
    "            print(\"  ↳ saved checkpoint by F1 (best so far)\")\n",
    "\n",
    "        if acc > best_val_acc:\n",
    "            best_val_acc = acc\n",
    "            save_model_checkpoint(\n",
    "                model, stoi, itos, model_id,\n",
    "                models_folder=MODELS_FOLDER,\n",
    "                suffix=\"_best_acc\",\n",
    "                best_metric_value=best_val_acc\n",
    "            )\n",
    "            print(\"  ↳ saved checkpoint by accuracy (best so far)\")\n",
    "\n",
    "    print(f\"\\ntraining done!\")\n",
    "    print(f\"best validation F1: {best_val_f1:.4f}\")\n",
    "    print(f\"best validation accuracy: {best_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcfc9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_kfold_cross_validation(\n",
    "    df,\n",
    "    n_folds=5,\n",
    "    emb_dim=64,\n",
    "    hidden_size=128,\n",
    "    num_layers=2,\n",
    "    dropout=0.3,\n",
    "    freeze_emb=True,\n",
    "    extra_feat_dim=1,\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-4,\n",
    "    morfessor_n_models=5,\n",
    "    morfessor_seed_base=123,\n",
    "    morfessor_lowercase=True,\n",
    "    random_state=42,\n",
    "    device=device\n",
    "):\n",
    "    \"\"\"K-fold cross-validation with Morfessor ensemble features.\"\"\"\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"K-FOLD CV (k={n_folds}) WITH MORFESSOR ENSEMBLE\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    \n",
    "    kfold = KFold(n_splits=n_folds, shuffle=True, random_state=random_state)\n",
    "    indices = np.arange(len(df))\n",
    "    \n",
    "    fold_results = []\n",
    "    all_metrics = {\n",
    "        'boundary_precision': [],\n",
    "        'boundary_recall': [],\n",
    "        'boundary_f1': [],\n",
    "        'val_loss': []\n",
    "    }\n",
    "    \n",
    "    for fold_idx, (train_indices, val_indices) in enumerate(kfold.split(indices), 1):\n",
    "        print(f\"\\n--- fold {fold_idx}/{n_folds} ---\")\n",
    "        print(f\"train: {len(train_indices)}, val: {len(val_indices)}\")\n",
    "        \n",
    "        train_df_fold = df.iloc[train_indices].reset_index(drop=True)\n",
    "        val_df_fold = df.iloc[val_indices].reset_index(drop=True)\n",
    "        \n",
    "        stoi_fold, itos_fold = build_vocab(train_df_fold[\"char_seq\"].tolist())\n",
    "        vocab_size = len(itos_fold)\n",
    "        \n",
    "        print(f\"  training morfessor ensemble on fold {fold_idx}...\")\n",
    "        m_cfg_fold = MorfessorConfig(\n",
    "            n_models=morfessor_n_models,\n",
    "            seed_base=morfessor_seed_base + fold_idx,\n",
    "            lowercase=morfessor_lowercase\n",
    "        )\n",
    "        morf_fold = MorfessorBoundaryFeaturizer(m_cfg_fold)\n",
    "        morf_fold.fit(train_df_fold[\"Word\"].tolist())\n",
    "        print(f\"  morfessor ensemble trained (n_models={morfessor_n_models})\")\n",
    "        \n",
    "        pos_fold, neg_fold = count_pos_neg(train_df_fold)\n",
    "        pos_weight_value_fold = float(neg_fold) / max(float(pos_fold), 1.0)\n",
    "        \n",
    "        def pad_batch_with_morf_fold(batch, pad_id=0):\n",
    "            seqs, labels, morf = zip(*batch)\n",
    "            x_ids = [[stoi_fold.get(t, stoi_fold[\"<UNK>\"]) for t in s] for s in seqs]\n",
    "            y_ids = [lab for lab in labels]\n",
    "            m_probs = [mp for mp in morf]\n",
    "            lengths = [len(x) for x in x_ids]\n",
    "            maxlen = max(lengths)\n",
    "            x_pad = [xi + [pad_id] * (maxlen - len(xi)) for xi in x_ids]\n",
    "            y_pad = [yi + [0] * (maxlen - len(yi)) for yi in y_ids]\n",
    "            m_pad = [mi + [0.0] * (maxlen - len(mi)) for mi in m_probs]\n",
    "            mask = [[1] * len(xi) + [0] * (maxlen - len(xi)) for xi in x_ids]\n",
    "            return (\n",
    "                torch.LongTensor(x_pad),\n",
    "                torch.FloatTensor(y_pad),\n",
    "                torch.BoolTensor(mask),\n",
    "                torch.LongTensor(lengths),\n",
    "                torch.FloatTensor(m_pad).unsqueeze(-1),\n",
    "            )\n",
    "        \n",
    "        train_ds_fold = CharBoundaryDatasetMorf(train_df_fold, morf_fold, stoi_fold)\n",
    "        val_ds_fold = CharBoundaryDatasetMorf(val_df_fold, morf_fold, stoi_fold)\n",
    "        train_loader_fold = DataLoader(\n",
    "            train_ds_fold, batch_size=batch_size, shuffle=True,\n",
    "            collate_fn=pad_batch_with_morf_fold\n",
    "        )\n",
    "        val_loader_fold = DataLoader(\n",
    "            val_ds_fold, batch_size=batch_size, shuffle=False,\n",
    "            collate_fn=pad_batch_with_morf_fold\n",
    "        )\n",
    "        \n",
    "        model_fold = BiLSTMBoundaryWithMorf(\n",
    "            vocab_size=vocab_size,\n",
    "            emb_dim=emb_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            freeze_emb=freeze_emb,\n",
    "            extra_feat_dim=extra_feat_dim\n",
    "        ).to(device)\n",
    "        \n",
    "        optimizer_fold = torch.optim.AdamW(model_fold.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        \n",
    "        def masked_bce_loss_fold(logits, targets, mask):\n",
    "            loss_fn = nn.BCEWithLogitsLoss(\n",
    "                reduction=\"none\",\n",
    "                pos_weight=torch.tensor(pos_weight_value_fold, device=logits.device)\n",
    "            )\n",
    "            loss_per_token = loss_fn(logits, targets) * mask.float()\n",
    "            denom = mask.float().sum().clamp_min(1.0)\n",
    "            return loss_per_token.sum() / denom\n",
    "        \n",
    "        best_val_f1 = 0.0\n",
    "        best_val_prec = 0.0\n",
    "        best_val_rec = 0.0\n",
    "        best_val_loss = float('inf')\n",
    "        best_epoch = 0\n",
    "        \n",
    "        for epoch in range(1, epochs + 1):\n",
    "            model_fold.train()\n",
    "            total_loss = 0.0\n",
    "            total_tokens = 0\n",
    "            for x, y, mask, lengths, morf_feat in train_loader_fold:\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                mask = mask.to(device)\n",
    "                lengths = lengths.to(device)\n",
    "                morf_feat = morf_feat.to(device)\n",
    "                \n",
    "                logits = model_fold(x, lengths, morf_feat)\n",
    "                loss = masked_bce_loss_fold(logits, y, mask)\n",
    "                \n",
    "                optimizer_fold.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(model_fold.parameters(), 1.0)\n",
    "                optimizer_fold.step()\n",
    "                \n",
    "                total_loss += loss.item() * mask.sum().item()\n",
    "                total_tokens += mask.sum().item()\n",
    "            \n",
    "            train_loss = total_loss / max(total_tokens, 1)\n",
    "            \n",
    "            model_fold.eval()\n",
    "            val_loss, val_tokens = 0.0, 0\n",
    "            all_prec, all_rec, all_f1 = [], [], []\n",
    "            with torch.no_grad():\n",
    "                for x, y, mask, lengths, morf_feat in val_loader_fold:\n",
    "                    x = x.to(device)\n",
    "                    y = y.to(device)\n",
    "                    mask = mask.to(device)\n",
    "                    lengths = lengths.to(device)\n",
    "                    morf_feat = morf_feat.to(device)\n",
    "                    \n",
    "                    logits = model_fold(x, lengths, morf_feat)\n",
    "                    loss = masked_bce_loss_fold(logits, y, mask)\n",
    "                    val_loss += loss.item() * mask.sum().item()\n",
    "                    val_tokens += mask.sum().item()\n",
    "                    \n",
    "                    p, r, f = boundary_f1(logits, y, mask, threshold=0.5)\n",
    "                    all_prec.append(p)\n",
    "                    all_rec.append(r)\n",
    "                    all_f1.append(f)\n",
    "            \n",
    "            val_loss = val_loss / max(val_tokens, 1)\n",
    "            prec = np.mean(all_prec) if all_prec else 0.0\n",
    "            rec = np.mean(all_rec) if all_rec else 0.0\n",
    "            f1 = np.mean(all_f1) if all_f1 else 0.0\n",
    "            \n",
    "            print(f\"  ep {epoch:02d} | train_loss={train_loss:.4f}  val_loss={val_loss:.4f}  P={prec:.3f} R={rec:.3f} F1={f1:.3f}\")\n",
    "            \n",
    "            if f1 > best_val_f1 or (np.isclose(f1, best_val_f1) and val_loss < best_val_loss):\n",
    "                best_val_f1 = f1\n",
    "                best_val_prec = prec\n",
    "                best_val_rec = rec\n",
    "                best_val_loss = val_loss\n",
    "                best_epoch = epoch\n",
    "        \n",
    "        print(f\"\\n  best epoch: {best_epoch}\")\n",
    "        print(f\"  best validation: P={best_val_prec:.3f} R={best_val_rec:.3f} F1={best_val_f1:.3f} Loss={best_val_loss:.4f}\")\n",
    "        \n",
    "        fold_results.append({\n",
    "            'fold': fold_idx,\n",
    "            'boundary_precision': best_val_prec,\n",
    "            'boundary_recall': best_val_rec,\n",
    "            'boundary_f1': best_val_f1,\n",
    "            'val_loss': best_val_loss,\n",
    "            'best_epoch': best_epoch\n",
    "        })\n",
    "        \n",
    "        all_metrics['boundary_precision'].append(best_val_prec)\n",
    "        all_metrics['boundary_recall'].append(best_val_rec)\n",
    "        all_metrics['boundary_f1'].append(best_val_f1)\n",
    "        all_metrics['val_loss'].append(best_val_loss)\n",
    "    \n",
    "    mean_metrics = {k: np.mean(v) for k, v in all_metrics.items()}\n",
    "    std_metrics = {k: np.std(v) for k, v in all_metrics.items()}\n",
    "    best_fold_idx = max(range(len(fold_results)), key=lambda i: fold_results[i]['boundary_f1'])\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(\"CV SUMMARY\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    for r in fold_results:\n",
    "        print(f\"  fold {r['fold']}: P={r['boundary_precision']:.3f}, R={r['boundary_recall']:.3f}, \"\n",
    "              f\"F1={r['boundary_f1']:.3f}, Loss={r['val_loss']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nmean +/- std over {n_folds} folds:\")\n",
    "    print(f\"  precision: {mean_metrics['boundary_precision']:.3f} +/- {std_metrics['boundary_precision']:.3f}\")\n",
    "    print(f\"  recall:    {mean_metrics['boundary_recall']:.3f} +/- {std_metrics['boundary_recall']:.3f}\")\n",
    "    print(f\"  F1:        {mean_metrics['boundary_f1']:.3f} +/- {std_metrics['boundary_f1']:.3f}\")\n",
    "    print(f\"  loss:      {mean_metrics['val_loss']:.4f} +/- {std_metrics['val_loss']:.4f}\")\n",
    "    print(f\"\\nbest fold: {fold_results[best_fold_idx]['fold']} (F1={fold_results[best_fold_idx]['boundary_f1']:.3f})\")\n",
    "    print(f\"{'=' * 80}\\n\")\n",
    "    \n",
    "    return {\n",
    "        'fold_results': fold_results,\n",
    "        'mean_metrics': mean_metrics,\n",
    "        'std_metrics': std_metrics,\n",
    "        'best_fold_idx': best_fold_idx,\n",
    "        'all_metrics': all_metrics\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa74364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run k-fold cross-validation\n",
    "kfold_results = run_kfold_cross_validation(\n",
    "    df=gold_df,\n",
    "    n_folds=5,\n",
    "    emb_dim=EMB_DIM,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT,\n",
    "    freeze_emb=FREEZE_EMB,\n",
    "    extra_feat_dim=EXTRA_FEAT_DIM,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    morfessor_n_models=m_cfg.n_models,\n",
    "    morfessor_seed_base=m_cfg.seed_base,\n",
    "    morfessor_lowercase=m_cfg.lowercase,\n",
    "    random_state=42,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"\\navg boundary F1: {kfold_results['mean_metrics']['boundary_f1']:.3f} +/- {kfold_results['std_metrics']['boundary_f1']:.3f}\")\n",
    "print(f\"avg precision: {kfold_results['mean_metrics']['boundary_precision']:.3f} +/- {kfold_results['std_metrics']['boundary_precision']:.3f}\")\n",
    "print(f\"avg recall: {kfold_results['mean_metrics']['boundary_recall']:.3f} +/- {kfold_results['std_metrics']['boundary_recall']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f35d615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example predictions\n",
    "test_words = [\"rikuchkani\", \"pikunas\", \"ñichkanchus\"]\n",
    "pred_b = predict_boundaries_with_morf(test_words, model, stoi, morf, threshold=0.5)\n",
    "for w, b in zip(test_words, pred_b):\n",
    "    toks = to_graphemes_quechua(w)\n",
    "    print(f\"{w} {b} -> {apply_boundaries_tokens(toks, b)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f7cb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "print(\"loading test data...\")\n",
    "acc_df = pd.read_parquet(os.path.join(DATA_FOLDER, \"cleaned_data_df.parquet\"))\n",
    "print(f\"loaded {len(acc_df):,} test examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67d6507",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boundary_positions_from_labels(labels, L=None):\n",
    "    \"\"\"Convert per-token boundary labels to boundary positions.\"\"\"\n",
    "    if not labels:\n",
    "        return set()\n",
    "    if L is None:\n",
    "        L = len(labels)\n",
    "    upto = min(L - 1, len(labels))\n",
    "    return {i for i in range(upto) if labels[i] == 1}\n",
    "\n",
    "def boundary_positions_from_morpheme_tokens(morpheme_token_lists):\n",
    "    \"\"\"Given morpheme token lists, return boundary positions.\"\"\"\n",
    "    pos = set()\n",
    "    acc = 0\n",
    "    for k, toks in enumerate(morpheme_token_lists):\n",
    "        acc += len(toks)\n",
    "        if k < len(morpheme_token_lists) - 1:\n",
    "            pos.add(acc - 1)\n",
    "    return pos\n",
    "\n",
    "def prf_from_sets(pred_set, gold_set):\n",
    "    \"\"\"Compute precision, recall, F1 from boundary sets.\"\"\"\n",
    "    tp = len(pred_set & gold_set)\n",
    "    fp = len(pred_set - gold_set)\n",
    "    fn = len(gold_set - pred_set)\n",
    "\n",
    "    if tp + fp == 0:\n",
    "        precision = 1.0 if (tp + fn == 0) else 0.0\n",
    "    else:\n",
    "        precision = tp / (tp + fp)\n",
    "\n",
    "    if tp + fn == 0:\n",
    "        recall = 1.0 if (tp + fp == 0) else 0.0\n",
    "    else:\n",
    "        recall = tp / (tp + fn)\n",
    "\n",
    "    if precision + recall == 0:\n",
    "        f1 = 1.0 if (tp + fp + fn) == 0 else 0.0\n",
    "    else:\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    return tp, fp, fn, precision, recall, f1\n",
    "\n",
    "def best_variant_metrics_token_space(word_tokens, pred_boundary_labels, gold_variants):\n",
    "    \"\"\"Compare predicted boundaries to gold variants, pick best F1.\"\"\"\n",
    "    pred_b = boundary_positions_from_labels(pred_boundary_labels, L=len(word_tokens))\n",
    "\n",
    "    best = None\n",
    "    for variant in gold_variants:\n",
    "        variant_token_lists = [to_graphemes_quechua(m) for m in variant]\n",
    "        gold_b = boundary_positions_from_morpheme_tokens(variant_token_lists)\n",
    "        tp, fp, fn, P, R, F1 = prf_from_sets(pred_b, gold_b)\n",
    "        key = (F1, tp, -fn, -fp)\n",
    "        if (best is None) or (key > best[0]):\n",
    "            best = (key, gold_b, tp, fp, fn, P, R, F1)\n",
    "\n",
    "    if best is None:\n",
    "        gold_b = set()\n",
    "        tp, fp, fn, P, R, F1 = prf_from_sets(pred_b, gold_b)\n",
    "        return pred_b, gold_b, tp, fp, fn, P, R, F1\n",
    "\n",
    "    _, gold_b, tp, fp, fn, P, R, F1 = best\n",
    "    return pred_b, gold_b, tp, fp, fn, P, R, F1\n",
    "\n",
    "def is_correct_prediction(predicted, gold_variants):\n",
    "    \"\"\"Check if predicted segmentation matches any gold variant.\"\"\"\n",
    "    return any(predicted == variant for variant in gold_variants)\n",
    "\n",
    "def split_count_metrics(predicted_segments, gold_variants):\n",
    "    \"\"\"Compute split-count accuracy variants.\"\"\"\n",
    "    pred_count = len(predicted_segments)\n",
    "    gold_counts = [len(gold) for gold in gold_variants]\n",
    "\n",
    "    exact = any(pred_count == g for g in gold_counts)\n",
    "    plus1 = any(pred_count == g + 1 for g in gold_counts)\n",
    "    minus1 = any(pred_count == g - 1 for g in gold_counts)\n",
    "    plusminus1 = any(abs(pred_count - g) <= 1 for g in gold_counts)\n",
    "\n",
    "    return {\"Exact\": exact, \"+1\": plus1, \"-1\": minus1, \"±1\": plusminus1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c7f88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model checkpoint\n",
    "print(f\"loading model {model_id}...\")\n",
    "ckpt = load_model_checkpoint(model_id, models_folder=MODELS_FOLDER, suffix=\"_best_acc\")\n",
    "\n",
    "if ckpt is None:\n",
    "    raise FileNotFoundError(f\"model checkpoint not found for model_id: {model_id}\")\n",
    "\n",
    "stoi, itos = ckpt[\"stoi\"], ckpt[\"itos\"]\n",
    "model.load_state_dict(ckpt[\"model_state\"])\n",
    "model.eval()\n",
    "print(\"model loaded successfully!\")\n",
    "\n",
    "# Evaluate on test set\n",
    "all_words = acc_df[\"Word\"].tolist()\n",
    "all_gold = acc_df[\"Gold\"]\n",
    "\n",
    "all_boundaries = predict_boundaries_with_morf(\n",
    "    all_words, model, stoi, morf, threshold=0.5, device=device\n",
    ")\n",
    "\n",
    "records = []\n",
    "micro_tp = micro_fp = micro_fn = 0\n",
    "macro_Ps, macro_Rs, macro_F1s = [], [], []\n",
    "exact_flags = []\n",
    "split_exact_flags = []\n",
    "split_plus1_flags = []\n",
    "split_minus1_flags = []\n",
    "split_pm1_flags = []\n",
    "overlap_flags = []\n",
    "\n",
    "for word, gold_variants, boundary_labels in zip(all_words, all_gold, all_boundaries):\n",
    "    gold_variants = normalize_gold_variants(gold_variants)\n",
    "    \n",
    "    toks = to_graphemes_quechua(word)\n",
    "    predicted_segments = apply_boundaries_tokens(toks, boundary_labels)\n",
    "\n",
    "    correct_exact = is_correct_prediction(predicted_segments, gold_variants)\n",
    "    split_metrics = split_count_metrics(predicted_segments, gold_variants)\n",
    "    overlap = correct_exact and split_metrics[\"Exact\"]\n",
    "\n",
    "    pred_b, gold_b_chosen, tp, fp, fn, P, R, F1 = best_variant_metrics_token_space(\n",
    "        toks, boundary_labels, gold_variants\n",
    "    )\n",
    "\n",
    "    records.append({\n",
    "        \"Word\": word,\n",
    "        \"Prediction\": predicted_segments,\n",
    "        \"Gold\": gold_variants,\n",
    "        \"PredBoundaries(tok_idx)\": sorted(pred_b),\n",
    "        \"GoldBoundaries(Chosen tok_idx)\": sorted(gold_b_chosen),\n",
    "        \"TP\": tp, \"FP\": fp, \"FN\": fn,\n",
    "        \"P_word\": P, \"R_word\": R, \"F1_word\": F1,\n",
    "        \"CorrectExactSeg\": correct_exact,\n",
    "        \"CorrectSplitCount\": split_metrics[\"Exact\"],\n",
    "        \"SplitCount+1\": split_metrics[\"+1\"],\n",
    "        \"SplitCount-1\": split_metrics[\"-1\"],\n",
    "        \"SplitCount±1\": split_metrics[\"±1\"],\n",
    "        \"OverlapExactAndSplit\": overlap\n",
    "    })\n",
    "\n",
    "    micro_tp += tp\n",
    "    micro_fp += fp\n",
    "    micro_fn += fn\n",
    "    macro_Ps.append(P)\n",
    "    macro_Rs.append(R)\n",
    "    macro_F1s.append(F1)\n",
    "    exact_flags.append(correct_exact)\n",
    "    split_exact_flags.append(split_metrics[\"Exact\"])\n",
    "    split_plus1_flags.append(split_metrics[\"+1\"])\n",
    "    split_minus1_flags.append(split_metrics[\"-1\"])\n",
    "    split_pm1_flags.append(split_metrics[\"±1\"])\n",
    "    overlap_flags.append(overlap)\n",
    "\n",
    "results_df = pd.DataFrame(records)\n",
    "\n",
    "accuracy = results_df[\"CorrectExactSeg\"].mean()\n",
    "\n",
    "if micro_tp + micro_fp == 0:\n",
    "    P_micro = 1.0 if micro_tp + micro_fn == 0 else 0.0\n",
    "else:\n",
    "    P_micro = micro_tp / (micro_tp + micro_fp)\n",
    "\n",
    "if micro_tp + micro_fn == 0:\n",
    "    R_micro = 1.0 if micro_tp + micro_fp == 0 else 0.0\n",
    "else:\n",
    "    R_micro = micro_tp / (micro_tp + micro_fn)\n",
    "\n",
    "if P_micro + R_micro == 0:\n",
    "    F1_micro = 1.0 if (micro_tp + micro_fp + micro_fn) == 0 else 0.0\n",
    "else:\n",
    "    F1_micro = 2 * P_micro * R_micro / (P_micro + R_micro)\n",
    "\n",
    "P_macro = float(pd.Series(macro_Ps).mean()) if macro_Ps else 0.0\n",
    "R_macro = float(pd.Series(macro_Rs).mean()) if macro_Rs else 0.0\n",
    "F1_macro = float(pd.Series(macro_F1s).mean()) if macro_F1s else 0.0\n",
    "\n",
    "exact_accuracy = np.mean(exact_flags)\n",
    "split_exact_acc = np.mean(split_exact_flags)\n",
    "split_plus1_acc = np.mean(split_plus1_flags)\n",
    "split_minus1_acc = np.mean(split_minus1_flags)\n",
    "split_pm1_acc = np.mean(split_pm1_flags)\n",
    "overlap_accuracy = np.mean(overlap_flags)\n",
    "\n",
    "print(\"=== segmentation and split count metrics ===\")\n",
    "print(f\"exact segmentation accuracy:  {exact_accuracy:.4f}\")\n",
    "print(f\"split-count (exact):          {split_exact_acc:.4f}\")\n",
    "print(f\"split-count (+1):             {split_plus1_acc:.4f}\")\n",
    "print(f\"split-count (−1):             {split_minus1_acc:.4f}\")\n",
    "print(f\"split-count (±1):             {split_pm1_acc:.4f}\")\n",
    "print(f\"overlap (exact ∩ split):      {overlap_accuracy:.4f}\")\n",
    "\n",
    "print(\"boundary metrics (token space):\")\n",
    "print(f\"  micro  - P: {P_micro:.4f}  R: {R_micro:.4f}  F1: {F1_micro:.4f}\")\n",
    "print(f\"  macro  - P: {P_macro:.4f}  R: {R_macro:.4f}  F1: {F1_macro:.4f}\")\n",
    "\n",
    "# Save results\n",
    "results_output_path = os.path.join(DATA_FOLDER, \"bilstm_morfessor_eval_results.csv\")\n",
    "results_df.to_csv(results_output_path, index=False)\n",
    "print(f\"\\nevaluation results saved to {results_output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
