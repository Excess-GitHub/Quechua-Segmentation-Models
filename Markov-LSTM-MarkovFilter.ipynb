{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "aa24b801",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MARKOV-LSTM-MARKOV FILTER: MORPHOLOGY PARSER WITH PRIORS\n",
    "========================================================\n",
    "\n",
    "This notebook implements a morphology parser for Quechua that combines:\n",
    "1. BiLSTM neural network for boundary prediction\n",
    "2. HMM (Hidden Markov Model) priors based on suffix patterns\n",
    "3. Privileged knowledge (K-teacher) for regularization\n",
    "\n",
    "The parser segments Quechua words into morphemes by predicting boundary positions\n",
    "between tokens. It uses:\n",
    "- Gold standard data (Sue Kalt dataset) as the base training data\n",
    "- Optional synthetic data augmentation from GPT models (gpt4o or gpt5mini)\n",
    "- HMM priors trained on suffix patterns to guide segmentation\n",
    "- K-teacher regularization to improve generalization\n",
    "\n",
    "Key Features:\n",
    "- Configurable synthetic data augmentation (none, gpt4o, gpt5mini)\n",
    "- Model checkpointing: saves/loads models to avoid retraining\n",
    "- Comprehensive evaluation metrics (precision, recall, F1, exact match)\n",
    "- Suffix validation to filter invalid segmentations\n",
    "\n",
    "All data is read from the 'data' folder and models are saved to the 'models' folder.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "82b96291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading gold standard data...\n",
      "Loaded 6,896 gold standard examples\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# DATA FOLDER CONFIGURATION\n",
    "# =========================\n",
    "# All data files should be read from and saved to the data folder\n",
    "DATA_FOLDER = \"data\"\n",
    "MODEL_NAME = \"Markov-LSTM-MarkovFilter\"\n",
    "MODELS_FOLDER = f\"models_{MODEL_NAME}\"\n",
    "\n",
    "# Create models folder if it doesn't exist\n",
    "os.makedirs(MODELS_FOLDER, exist_ok=True)\n",
    "\n",
    "# =========================\n",
    "# CONFIGURATION: SYNTHETIC DATA AUGMENTATION\n",
    "# =========================\n",
    "# Choose which synthetic data to use for augmentation:\n",
    "#   \"none\"     - Use only gold standard data (no augmentation)\n",
    "#   \"gpt4o\"    - Augment with GPT-4o synthetic segmentations\n",
    "#   \"gpt5mini\" - Augment with GPT-5-mini synthetic segmentations\n",
    "SYNTHETIC_DATA_CHOICE = \"none\"  # Change this to \"none\", \"gpt4o\", or \"gpt5mini\"\n",
    "\n",
    "# =========================\n",
    "# CONFIGURATION: WORD SELECTION FOR AUGMENTATION\n",
    "# =========================\n",
    "# Choose how to select words from common words for augmentation:\n",
    "#   \"all\"      - Use all common words (default)\n",
    "#   \"first\"    - Use the first n common words (sorted alphabetically)\n",
    "#   \"random\"   - Use n randomly selected common words\n",
    "AUGMENTATION_WORD_SELECTION = \"random\"  # Change this to \"all\", \"first\", or \"random\"\n",
    "AUGMENTATION_N_WORDS = 100  # Number of words to use when selection is \"first\" or \"random\" (ignored if \"all\")\n",
    "\n",
    "# =========================\n",
    "# LOAD GOLD STANDARD DATA\n",
    "# =========================\n",
    "# The gold standard dataset contains high-quality morphological segmentations\n",
    "# This is the base training data that will always be used\n",
    "print(\"Loading gold standard data...\")\n",
    "gold_df = pd.read_parquet(os.path.join(DATA_FOLDER, \"Sue_kalt.parquet\"))\n",
    "gold_df['Word'] = gold_df['word']\n",
    "gold_df['morph'] = gold_df['morph'].str.replace('-', ' ')  # Normalize separators\n",
    "gold_df['Morph_split_str'] = gold_df['morph']  # String version\n",
    "gold_df['Morph_split'] = gold_df['morph'].str.split(' ')  # List version\n",
    "gold_df = gold_df[['Word', 'Morph_split', 'Morph_split_str']]\n",
    "gold_df.drop_duplicates(subset='Word', keep='first', inplace=True)\n",
    "gold_df.dropna(subset=['Word'], inplace=True)\n",
    "print(f\"Loaded {len(gold_df):,} gold standard examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "152d7934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6896, 3)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "affdfe37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No synthetic data augmentation selected.\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# LOAD AND PROCESS SYNTHETIC DATA (if augmentation is enabled)\n",
    "# =========================\n",
    "# This cell loads synthetic segmentations from GPT models if augmentation is enabled\n",
    "# The synthetic data is filtered to remove low-quality segmentations and formatted\n",
    "# to match the gold standard data structure\n",
    "\n",
    "def load_synthetic_data(choice):\n",
    "    \"\"\"\n",
    "    Load synthetic segmentation data based on the chosen augmentation method.\n",
    "    \n",
    "    Args:\n",
    "        choice: One of \"none\", \"gpt4o\", or \"gpt5mini\"\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with synthetic segmentations, or None if choice is \"none\"\n",
    "    \"\"\"\n",
    "    if choice == \"none\":\n",
    "        print(\"No synthetic data augmentation selected.\")\n",
    "        return None\n",
    "    \n",
    "    # Map choice to file name\n",
    "    file_map = {\n",
    "        \"gpt4o\": \"gpt4o_synthetic_segmentations.csv\",\n",
    "        \"gpt5mini\": \"gpt5mini_synthetic_segmentations.csv\"\n",
    "    }\n",
    "    \n",
    "    if choice not in file_map:\n",
    "        print(f\"Warning: Unknown synthetic data choice '{choice}'. Using 'none' instead.\")\n",
    "        return None\n",
    "    \n",
    "    file_path = os.path.join(DATA_FOLDER, file_map[choice])\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Warning: Synthetic data file not found: {file_path}\")\n",
    "        print(f\"Falling back to no augmentation.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Loading synthetic data from {file_path}...\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Remove duplicates (keep first occurrence)\n",
    "    df = df.drop_duplicates(subset=['Original_Word']).reset_index(drop=True)\n",
    "    \n",
    "    # Filter out low-quality segmentations that contain invalid strings\n",
    "    # These strings indicate the model failed or produced invalid output\n",
    "    strings_to_drop = ['can\\'t', 'quechua', 'sorry', 'could']\n",
    "    df = df[~df['Segmented_Morphemes'].str.contains('|'.join(strings_to_drop), case=False, na=False)]\n",
    "    \n",
    "    # Rename and format columns to match gold standard structure\n",
    "    df = df.rename(columns={'Original_Word': 'Word'})\n",
    "    df['Morph_split_str'] = df['Segmented_Morphemes']\n",
    "    df['Morph_split'] = df['Segmented_Morphemes'].str.split(' ')\n",
    "    df = df[['Word', 'Morph_split', 'Morph_split_str']]\n",
    "    \n",
    "    print(f\"Loaded {len(df):,} synthetic segmentations from {choice}\")\n",
    "    return df\n",
    "\n",
    "# Load synthetic data based on configuration\n",
    "synthetic_df = load_synthetic_data(SYNTHETIC_DATA_CHOICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cea78c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# LOAD GPT-5-MINI DATA (for comparison/analysis)\n",
    "# =========================\n",
    "# This cell loads GPT-5-mini data separately for comparison purposes\n",
    "# Note: This is separate from the augmentation choice above\n",
    "\n",
    "gpt_5_mini_df = pd.read_csv(os.path.join(DATA_FOLDER, \"gpt5mini_synthetic_segmentations.csv\"))\n",
    "gpt_5_mini_df = gpt_5_mini_df.drop_duplicates(subset=['Original_Word']).reset_index(drop=True)\n",
    "\n",
    "strings_to_drop = ['can\\'t', 'quechua', 'sorry', 'could']\n",
    "gpt_5_mini_df = gpt_5_mini_df[~gpt_5_mini_df['Segmented_Morphemes'].str.contains('|'.join(strings_to_drop), case=False, na=False)]\n",
    "\n",
    "# Rename and format columns\n",
    "gpt_5_mini_df = gpt_5_mini_df.rename(columns={'Original_Word': 'Word'})\n",
    "gpt_5_mini_df['Morph_split_str'] = gpt_5_mini_df['Segmented_Morphemes']\n",
    "gpt_5_mini_df['Morph_split'] = gpt_5_mini_df['Segmented_Morphemes'].str.split(' ')\n",
    "gpt_5_mini_df = gpt_5_mini_df[['Word', 'Morph_split', 'Morph_split_str']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "808a7758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 3)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_5_mini_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ac5da898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# LOAD GPT-4O DATA (for comparison/analysis)\n",
    "# =========================\n",
    "# This cell loads GPT-4o data separately for comparison purposes\n",
    "# Note: This is separate from the augmentation choice above\n",
    "\n",
    "gpt_4o_df = pd.read_csv(os.path.join(DATA_FOLDER, \"gpt4o_synthetic_segmentations.csv\"))\n",
    "gpt_4o_df = gpt_4o_df.drop_duplicates(subset=['Original_Word']).reset_index(drop=True)\n",
    "\n",
    "strings_to_drop = ['can\\'t', 'quechua', 'sorry', 'could']\n",
    "gpt_4o_df = gpt_4o_df[~gpt_4o_df['Segmented_Morphemes'].str.contains('|'.join(strings_to_drop), case=False, na=False)]\n",
    "\n",
    "# Rename and format columns\n",
    "gpt_4o_df = gpt_4o_df.rename(columns={'Original_Word': 'Word'})\n",
    "gpt_4o_df['Morph_split_str'] = gpt_4o_df['Segmented_Morphemes']\n",
    "gpt_4o_df['Morph_split'] = gpt_4o_df['Segmented_Morphemes'].str.split(' ')\n",
    "gpt_4o_df = gpt_4o_df[['Word', 'Morph_split', 'Morph_split_str']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "63c45c06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2382, 3)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_4o_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e90ca0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of common words: 469\n"
     ]
    }
   ],
   "source": [
    "gpt_5_mini_words = set(gpt_5_mini_df['Word'])\n",
    "gpt_4o_words = set(gpt_4o_df['Word'])\n",
    "\n",
    "common_words = gpt_4o_words.intersection(gpt_5_mini_words)\n",
    "\n",
    "print(\"Number of common words:\", len(common_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "aac5d217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using only gold standard data (no augmentation)\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# COMBINE GOLD AND SYNTHETIC DATA\n",
    "# =========================\n",
    "# Combine the gold standard data with synthetic data (if augmentation is enabled)\n",
    "# Only words that appear in both GPT models are used to ensure quality\n",
    "\n",
    "if synthetic_df is not None:\n",
    "    # Find common words between GPT-4o and GPT-5-mini for quality control\n",
    "    gpt_5_mini_words = set(gpt_5_mini_df['Word'])\n",
    "    gpt_4o_words = set(gpt_4o_df['Word'])\n",
    "    common_words = gpt_4o_words.intersection(gpt_5_mini_words)\n",
    "    print(f\"Number of common words between GPT models: {len(common_words):,}\")\n",
    "    \n",
    "    # Select words based on AUGMENTATION_WORD_SELECTION configuration\n",
    "    if AUGMENTATION_WORD_SELECTION == \"all\":\n",
    "        selected_words = common_words\n",
    "        print(f\"Using all {len(selected_words):,} common words for augmentation\")\n",
    "    elif AUGMENTATION_WORD_SELECTION == \"first\":\n",
    "        # Sort words alphabetically and take first n\n",
    "        sorted_words = sorted(common_words)\n",
    "        n = min(AUGMENTATION_N_WORDS, len(sorted_words))\n",
    "        selected_words = set(sorted_words[:n])\n",
    "        print(f\"Using first {n:,} common words (alphabetically sorted) for augmentation\")\n",
    "    elif AUGMENTATION_WORD_SELECTION == \"random\":\n",
    "        # Randomly sample n words\n",
    "        import random\n",
    "        # Use RNG if defined, otherwise use default seed of 42\n",
    "        seed = RNG if 'RNG' in globals() else 42\n",
    "        random.seed(seed)  # Use the same RNG seed for reproducibility\n",
    "        n = min(AUGMENTATION_N_WORDS, len(common_words))\n",
    "        selected_words = set(random.sample(list(common_words), n))\n",
    "        print(f\"Using {n:,} randomly selected common words for augmentation\")\n",
    "    else:\n",
    "        print(f\"Warning: Unknown AUGMENTATION_WORD_SELECTION '{AUGMENTATION_WORD_SELECTION}'. Using 'all' instead.\")\n",
    "        selected_words = common_words\n",
    "    \n",
    "    # Use only selected words from the chosen synthetic data\n",
    "    if SYNTHETIC_DATA_CHOICE == \"gpt5mini\":\n",
    "        df_sampled = synthetic_df[synthetic_df['Word'].isin(selected_words)]\n",
    "    elif SYNTHETIC_DATA_CHOICE == \"gpt4o\":\n",
    "        df_sampled = synthetic_df[synthetic_df['Word'].isin(selected_words)]\n",
    "    else:\n",
    "        df_sampled = None\n",
    "    \n",
    "    if df_sampled is not None and len(df_sampled) > 0:\n",
    "        # Combine with gold data\n",
    "        gold_df = pd.concat([df_sampled, gold_df], ignore_index=True)\n",
    "        print(f\"Combined dataset: {len(gold_df):,} examples ({len(df_sampled):,} synthetic + {len(gold_df) - len(df_sampled):,} gold)\")\n",
    "    else:\n",
    "        print(\"No synthetic data to add (no common words found)\")\n",
    "else:\n",
    "    print(\"Using only gold standard data (no augmentation)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "28e7043f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# SAVE COMMON WORDS (if synthetic data was used)\n",
    "# =========================\n",
    "# Save the common words used for augmentation to the data folder for reference\n",
    "\n",
    "if synthetic_df is not None and 'df_sampled' in locals() and df_sampled is not None:\n",
    "    df_sampled = df_sampled.sort_values(by=\"Word\")\n",
    "    output_file = os.path.join(DATA_FOLDER, f\"{SYNTHETIC_DATA_CHOICE}_common.parquet\")\n",
    "    df_sampled.to_parquet(output_file, index=False)\n",
    "    print(f\"Saved common words to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "69347c29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Morph_split</th>\n",
       "      <th>Morph_split_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cementerioman</td>\n",
       "      <td>[cementerio, man]</td>\n",
       "      <td>cementerio man</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kawsachkananta</td>\n",
       "      <td>[kawsa, chka, na, n, ta]</td>\n",
       "      <td>kawsa chka na n ta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mañakunpis</td>\n",
       "      <td>[maña, ku, n, pis]</td>\n",
       "      <td>maña ku n pis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>imaynapichus</td>\n",
       "      <td>[imayna, pi, chus]</td>\n",
       "      <td>imayna pi chus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>qipiyuq</td>\n",
       "      <td>[qipi, yuq]</td>\n",
       "      <td>qipi yuq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Quispepis</td>\n",
       "      <td>[Quispe, pis]</td>\n",
       "      <td>Quispe pis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ñichkanmanchá</td>\n",
       "      <td>[ñi, chka, nman, chá]</td>\n",
       "      <td>ñi chka nman chá</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>qukuni</td>\n",
       "      <td>[qu, ku, ni]</td>\n",
       "      <td>qu ku ni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dejasunpunichu</td>\n",
       "      <td>[deja, sun, puni, chu]</td>\n",
       "      <td>deja sun puni chu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>phutikunki</td>\n",
       "      <td>[phuti, ku, nki]</td>\n",
       "      <td>phuti ku nki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>kawsakunman</td>\n",
       "      <td>[kawsa, ku, nman]</td>\n",
       "      <td>kawsa ku nman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>yanapasqayki</td>\n",
       "      <td>[yanapa, sqayki]</td>\n",
       "      <td>yanapa sqayki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Pachamamaman</td>\n",
       "      <td>[Pachamama, man]</td>\n",
       "      <td>Pachamama man</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>pagawanki</td>\n",
       "      <td>[paga, wa, nki]</td>\n",
       "      <td>paga wa nki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>qhawanakun</td>\n",
       "      <td>[qhawa, naku, n]</td>\n",
       "      <td>qhawa naku n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>paypis</td>\n",
       "      <td>[pay, pis]</td>\n",
       "      <td>pay pis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>qhurasqita</td>\n",
       "      <td>[qhura, sqa, it]</td>\n",
       "      <td>qhura sqa it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>defendekusaq</td>\n",
       "      <td>[defende, ku, saq]</td>\n",
       "      <td>defende ku saq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>parlaspaqa</td>\n",
       "      <td>[parla, spa, qa]</td>\n",
       "      <td>parla spa qa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>tinqan</td>\n",
       "      <td>[tinqa, n]</td>\n",
       "      <td>tinqa n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>kutuykapun</td>\n",
       "      <td>[kutu, yka, pu, n]</td>\n",
       "      <td>kutu yka pu n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>qurikun</td>\n",
       "      <td>[qu, ri, ku, n]</td>\n",
       "      <td>qu ri ku n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>tortugapunichu</td>\n",
       "      <td>[tortuga, puni, chu]</td>\n",
       "      <td>tortuga puni chu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>qipirisqallataq</td>\n",
       "      <td>[qipi, ri, sqa, lla, taq]</td>\n",
       "      <td>qipi ri sqa lla taq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>sigue</td>\n",
       "      <td>[sigue]</td>\n",
       "      <td>sigue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>ratullachu</td>\n",
       "      <td>[ratu, lla, chu]</td>\n",
       "      <td>ratu lla chu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>tulluta</td>\n",
       "      <td>[tullu, ta]</td>\n",
       "      <td>tullu ta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>wawasninku</td>\n",
       "      <td>[wawa, s, ni, nku]</td>\n",
       "      <td>wawa s ni nku</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>ñawisitunta</td>\n",
       "      <td>[ñawi, situ, n, ta]</td>\n",
       "      <td>ñawi situ n ta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>nipuni</td>\n",
       "      <td>[ni, puni]</td>\n",
       "      <td>ni puni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>mirananpaq</td>\n",
       "      <td>[mira, na, n, paq]</td>\n",
       "      <td>mira na n paq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>yuthuwan</td>\n",
       "      <td>[yuthu, wan]</td>\n",
       "      <td>yuthu wan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>ñuqaptaqa</td>\n",
       "      <td>[ñuqa, p, ta, qa]</td>\n",
       "      <td>ñuqa p ta qa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>willachkanchá</td>\n",
       "      <td>[willa, chka, n, chá]</td>\n",
       "      <td>willa chka n chá</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>hampatitunta</td>\n",
       "      <td>[hampatu, it, n, ta]</td>\n",
       "      <td>hampatu it n ta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>paypachussina</td>\n",
       "      <td>[pay, pa, chus, sina]</td>\n",
       "      <td>pay pa chus sina</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>sufrichinankupaq</td>\n",
       "      <td>[sufri, chi, na, nku, paq]</td>\n",
       "      <td>sufri chi na nku paq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>mama</td>\n",
       "      <td>[mama]</td>\n",
       "      <td>mama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>zorritupis</td>\n",
       "      <td>[zorr, itu, pis]</td>\n",
       "      <td>zorr itu pis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>hanqarakuspa</td>\n",
       "      <td>[hanqara, ku, spa]</td>\n",
       "      <td>hanqara ku spa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>wachaytapis</td>\n",
       "      <td>[wacha, y, ta, pis]</td>\n",
       "      <td>wacha y ta pis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>waqanakunku</td>\n",
       "      <td>[waqa, naku, nku]</td>\n",
       "      <td>waqa naku nku</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>qalata</td>\n",
       "      <td>[qala, ta]</td>\n",
       "      <td>qala ta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>thatarpanchá</td>\n",
       "      <td>[thata, rpa, n, chá]</td>\n",
       "      <td>thata rpa n chá</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>saksasqita</td>\n",
       "      <td>[saksa, sqa, it]</td>\n",
       "      <td>saksa sqa it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>chayrayku</td>\n",
       "      <td>[chay, rayku]</td>\n",
       "      <td>chay rayku</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>purichkan</td>\n",
       "      <td>[puri, chka, n]</td>\n",
       "      <td>puri chka n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>maytachus</td>\n",
       "      <td>[may, ta, chus]</td>\n",
       "      <td>may ta chus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>rikhurimpuptin</td>\n",
       "      <td>[rikhuri, m, pu, pti, n]</td>\n",
       "      <td>rikhuri m pu pti n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>pachay</td>\n",
       "      <td>[pacha, y]</td>\n",
       "      <td>pacha y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Word                 Morph_split       Morph_split_str\n",
       "0      cementerioman           [cementerio, man]        cementerio man\n",
       "1     kawsachkananta    [kawsa, chka, na, n, ta]    kawsa chka na n ta\n",
       "2         mañakunpis          [maña, ku, n, pis]         maña ku n pis\n",
       "3       imaynapichus          [imayna, pi, chus]        imayna pi chus\n",
       "4            qipiyuq                 [qipi, yuq]              qipi yuq\n",
       "5          Quispepis               [Quispe, pis]            Quispe pis\n",
       "6      ñichkanmanchá       [ñi, chka, nman, chá]      ñi chka nman chá\n",
       "7             qukuni                [qu, ku, ni]              qu ku ni\n",
       "8     dejasunpunichu      [deja, sun, puni, chu]     deja sun puni chu\n",
       "9         phutikunki            [phuti, ku, nki]          phuti ku nki\n",
       "10       kawsakunman           [kawsa, ku, nman]         kawsa ku nman\n",
       "11      yanapasqayki            [yanapa, sqayki]         yanapa sqayki\n",
       "12      Pachamamaman            [Pachamama, man]         Pachamama man\n",
       "13         pagawanki             [paga, wa, nki]           paga wa nki\n",
       "14        qhawanakun            [qhawa, naku, n]          qhawa naku n\n",
       "15            paypis                  [pay, pis]               pay pis\n",
       "16        qhurasqita            [qhura, sqa, it]          qhura sqa it\n",
       "17      defendekusaq          [defende, ku, saq]        defende ku saq\n",
       "18        parlaspaqa            [parla, spa, qa]          parla spa qa\n",
       "19            tinqan                  [tinqa, n]               tinqa n\n",
       "20        kutuykapun          [kutu, yka, pu, n]         kutu yka pu n\n",
       "21           qurikun             [qu, ri, ku, n]            qu ri ku n\n",
       "22    tortugapunichu        [tortuga, puni, chu]      tortuga puni chu\n",
       "23   qipirisqallataq   [qipi, ri, sqa, lla, taq]   qipi ri sqa lla taq\n",
       "24             sigue                     [sigue]                 sigue\n",
       "25        ratullachu            [ratu, lla, chu]          ratu lla chu\n",
       "26           tulluta                 [tullu, ta]              tullu ta\n",
       "27        wawasninku          [wawa, s, ni, nku]         wawa s ni nku\n",
       "28       ñawisitunta         [ñawi, situ, n, ta]        ñawi situ n ta\n",
       "29            nipuni                  [ni, puni]               ni puni\n",
       "30        mirananpaq          [mira, na, n, paq]         mira na n paq\n",
       "31          yuthuwan                [yuthu, wan]             yuthu wan\n",
       "32         ñuqaptaqa           [ñuqa, p, ta, qa]          ñuqa p ta qa\n",
       "33     willachkanchá       [willa, chka, n, chá]      willa chka n chá\n",
       "34      hampatitunta        [hampatu, it, n, ta]       hampatu it n ta\n",
       "35     paypachussina       [pay, pa, chus, sina]      pay pa chus sina\n",
       "36  sufrichinankupaq  [sufri, chi, na, nku, paq]  sufri chi na nku paq\n",
       "37              mama                      [mama]                  mama\n",
       "38        zorritupis            [zorr, itu, pis]          zorr itu pis\n",
       "39      hanqarakuspa          [hanqara, ku, spa]        hanqara ku spa\n",
       "40       wachaytapis         [wacha, y, ta, pis]        wacha y ta pis\n",
       "41       waqanakunku           [waqa, naku, nku]         waqa naku nku\n",
       "42            qalata                  [qala, ta]               qala ta\n",
       "43      thatarpanchá        [thata, rpa, n, chá]       thata rpa n chá\n",
       "44        saksasqita            [saksa, sqa, it]          saksa sqa it\n",
       "45         chayrayku               [chay, rayku]            chay rayku\n",
       "46         purichkan             [puri, chka, n]           puri chka n\n",
       "47         maytachus             [may, ta, chus]           may ta chus\n",
       "48    rikhurimpuptin    [rikhuri, m, pu, pti, n]    rikhuri m pu pti n\n",
       "49            pachay                  [pacha, y]               pacha y"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "77972ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATASET SUMMARY\n",
      "============================================================\n",
      "Training data shape: (6896, 3)\n",
      "Test data shape: (913, 5)\n",
      "Synthetic augmentation: none\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# LOAD TEST DATA\n",
    "# =========================\n",
    "# Load the test/accuracy evaluation dataset\n",
    "# This dataset is used for final evaluation of the trained model\n",
    "\n",
    "acc_df = pd.read_parquet(os.path.join(DATA_FOLDER, \"cleaned_data_df.parquet\"))\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DATASET SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training data shape: {gold_df.shape}\")\n",
    "print(f\"Test data shape: {acc_df.shape}\")\n",
    "print(f\"Synthetic augmentation: {SYNTHETIC_DATA_CHOICE}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d291d434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Gold</th>\n",
       "      <th>Morph_split</th>\n",
       "      <th>Morph_split_str</th>\n",
       "      <th>Filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>unupas</td>\n",
       "      <td>[[unu, pas]]</td>\n",
       "      <td>[unu, pas]</td>\n",
       "      <td>unu pas</td>\n",
       "      <td>For_Annotation_1_LS.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>umankus</td>\n",
       "      <td>[[uma, nku, s]]</td>\n",
       "      <td>[uma, nku, s]</td>\n",
       "      <td>uma nku s</td>\n",
       "      <td>For_Annotation_1_LS.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hikurin</td>\n",
       "      <td>[[hikuri, n]]</td>\n",
       "      <td>[hikuri, n]</td>\n",
       "      <td>hikuri n</td>\n",
       "      <td>For_Annotation_1_LS.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sutipi</td>\n",
       "      <td>[[suti, pi]]</td>\n",
       "      <td>[suti, pi]</td>\n",
       "      <td>suti pi</td>\n",
       "      <td>For_Annotation_1_LS.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pikunas</td>\n",
       "      <td>[[pi, kuna, s]]</td>\n",
       "      <td>[pi, kuna, s]</td>\n",
       "      <td>pi kuna s</td>\n",
       "      <td>For_Annotation_1_LS.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>atipaq</td>\n",
       "      <td>[[ati, paq], [ati, pa, q]]</td>\n",
       "      <td>[ati, paq]</td>\n",
       "      <td>ati paq</td>\n",
       "      <td>For_Annotation_1_LS.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tomani</td>\n",
       "      <td>[[toma, ni]]</td>\n",
       "      <td>[toma, ni]</td>\n",
       "      <td>toma ni</td>\n",
       "      <td>For_Annotation_1_LS.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rantiq</td>\n",
       "      <td>[[ranti, q]]</td>\n",
       "      <td>[ranti, q]</td>\n",
       "      <td>ranti q</td>\n",
       "      <td>For_Annotation_1_LS.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>imakunas</td>\n",
       "      <td>[[ima, kuna, s]]</td>\n",
       "      <td>[ima, kuna, s]</td>\n",
       "      <td>ima kuna s</td>\n",
       "      <td>For_Annotation_1_LS.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>chiqaq</td>\n",
       "      <td>[[chiqaq]]</td>\n",
       "      <td>[chiqaq]</td>\n",
       "      <td>chiqaq</td>\n",
       "      <td>For_Annotation_1_LS.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>hampichu</td>\n",
       "      <td>[[hampi, chu]]</td>\n",
       "      <td>[hampi, chu]</td>\n",
       "      <td>hampi chu</td>\n",
       "      <td>For_Annotation_1_LS.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>misata</td>\n",
       "      <td>[[misa, ta]]</td>\n",
       "      <td>[misa, ta]</td>\n",
       "      <td>misa ta</td>\n",
       "      <td>For_Annotation_1_LS.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>nakuy</td>\n",
       "      <td>[[na, ku, y]]</td>\n",
       "      <td>[na, ku, y]</td>\n",
       "      <td>na ku y</td>\n",
       "      <td>For_Annotation_1_LS.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>hampi</td>\n",
       "      <td>[[hampi]]</td>\n",
       "      <td>[hampi]</td>\n",
       "      <td>hampi</td>\n",
       "      <td>For_Annotation_1_LS.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>hampiy</td>\n",
       "      <td>[[hampi, y]]</td>\n",
       "      <td>[hampi, y]</td>\n",
       "      <td>hampi y</td>\n",
       "      <td>For_Annotation_1_LS.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>limun</td>\n",
       "      <td>[[limun]]</td>\n",
       "      <td>[limun]</td>\n",
       "      <td>limun</td>\n",
       "      <td>For_Annotation_1_LS.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>hinas</td>\n",
       "      <td>[[hina, s]]</td>\n",
       "      <td>[hina, s]</td>\n",
       "      <td>hina s</td>\n",
       "      <td>For_Annotation_1_LS.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>atispa</td>\n",
       "      <td>[[ati, spa]]</td>\n",
       "      <td>[ati, spa]</td>\n",
       "      <td>ati spa</td>\n",
       "      <td>For_Annotation_1_LS.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>riman</td>\n",
       "      <td>[[rima, n]]</td>\n",
       "      <td>[rima, n]</td>\n",
       "      <td>rima n</td>\n",
       "      <td>For_Annotation_1_LS.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>matin</td>\n",
       "      <td>[[mati, n]]</td>\n",
       "      <td>[mati, n]</td>\n",
       "      <td>mati n</td>\n",
       "      <td>For_Annotation_1_LS.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>yachani</td>\n",
       "      <td>[[yacha, ni]]</td>\n",
       "      <td>[yacha, ni]</td>\n",
       "      <td>yacha ni</td>\n",
       "      <td>For_Annotation_1_LS.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>chaypiqa</td>\n",
       "      <td>[[chay, pi, qa]]</td>\n",
       "      <td>[chay, pi, qa]</td>\n",
       "      <td>chay pi qa</td>\n",
       "      <td>For_Annotation_1_LS.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>niqchu</td>\n",
       "      <td>[[ni, q, chu]]</td>\n",
       "      <td>[ni, q, chu]</td>\n",
       "      <td>ni q chu</td>\n",
       "      <td>For_Annotation_1_LS.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>napiqa</td>\n",
       "      <td>[[na, pi, qa]]</td>\n",
       "      <td>[na, pi, qa]</td>\n",
       "      <td>na pi qa</td>\n",
       "      <td>For_Annotation_1_LS.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ripuni</td>\n",
       "      <td>[[ri, pu, ni]]</td>\n",
       "      <td>[ri, pu, ni]</td>\n",
       "      <td>ri pu ni</td>\n",
       "      <td>For_Annotation_1_LS.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>kapunchik</td>\n",
       "      <td>[[ka, pu, nchik]]</td>\n",
       "      <td>[ka, pu, nchik]</td>\n",
       "      <td>ka pu nchik</td>\n",
       "      <td>For_Annotation_1_LS.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>raykum</td>\n",
       "      <td>[[ima, rayku, m]]</td>\n",
       "      <td>[ima, rayku, m]</td>\n",
       "      <td>ima rayku m</td>\n",
       "      <td>For_Annotation_1_LS.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>nispam</td>\n",
       "      <td>[[ni, spa, m]]</td>\n",
       "      <td>[ni, spa, m]</td>\n",
       "      <td>ni spa m</td>\n",
       "      <td>For_Annotation_1_LS.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>rimanku</td>\n",
       "      <td>[[rima, n, ku]]</td>\n",
       "      <td>[rima, n, ku]</td>\n",
       "      <td>rima n ku</td>\n",
       "      <td>For_Annotation_1_LS.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>punchay</td>\n",
       "      <td>[[punchay]]</td>\n",
       "      <td>[punchay]</td>\n",
       "      <td>punchay</td>\n",
       "      <td>For_Annotation_1_LS.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>tumankupas</td>\n",
       "      <td>[[tuma, nku, pas]]</td>\n",
       "      <td>[tuma, nku, pas]</td>\n",
       "      <td>tuma nku pas</td>\n",
       "      <td>For_Annotation_1_LS.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>chikan</td>\n",
       "      <td>[[chikan]]</td>\n",
       "      <td>[chikan]</td>\n",
       "      <td>chikan</td>\n",
       "      <td>For_Annotation_1_LS.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>munay</td>\n",
       "      <td>[[muna, y]]</td>\n",
       "      <td>[muna, y]</td>\n",
       "      <td>muna y</td>\n",
       "      <td>For_Annotation_1_LS.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>atikun</td>\n",
       "      <td>[[ati, ku, n]]</td>\n",
       "      <td>[ati, ku, n]</td>\n",
       "      <td>ati ku n</td>\n",
       "      <td>For_Annotation_1_LS.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>chunka</td>\n",
       "      <td>[[chunka]]</td>\n",
       "      <td>[chunka]</td>\n",
       "      <td>chunka</td>\n",
       "      <td>For_Annotation_1_LS.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>saniyapuni</td>\n",
       "      <td>[[sani, ya, puni]]</td>\n",
       "      <td>[sani, ya, puni]</td>\n",
       "      <td>sani ya puni</td>\n",
       "      <td>For_Annotation_1_LS.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>hampusaq</td>\n",
       "      <td>[[hampu, saq]]</td>\n",
       "      <td>[hampu, saq]</td>\n",
       "      <td>hampu saq</td>\n",
       "      <td>For_Annotation_1_LS.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>ñuqaykupis</td>\n",
       "      <td>[[ñuqa, yku, pis]]</td>\n",
       "      <td>[ñuqa, yku, pis]</td>\n",
       "      <td>ñuqa yku pis</td>\n",
       "      <td>For_Annotation_1_LS.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>chhikan</td>\n",
       "      <td>[[chhikan]]</td>\n",
       "      <td>[chhikan]</td>\n",
       "      <td>chhikan</td>\n",
       "      <td>For_Annotation_1_LS.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>santusmi</td>\n",
       "      <td>[[santus, mi]]</td>\n",
       "      <td>[santus, mi]</td>\n",
       "      <td>santus mi</td>\n",
       "      <td>For_Annotation_1_LS.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>sutin</td>\n",
       "      <td>[[suti, n]]</td>\n",
       "      <td>[suti, n]</td>\n",
       "      <td>suti n</td>\n",
       "      <td>For_Annotation_1_LS.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>intirukunapi</td>\n",
       "      <td>[[intiru, kuna, pi]]</td>\n",
       "      <td>[intiru, kuna, pi]</td>\n",
       "      <td>intiru kuna pi</td>\n",
       "      <td>For_Annotation_1_LS.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>wasinta</td>\n",
       "      <td>[[wasi, n, ta]]</td>\n",
       "      <td>[wasi, n, ta]</td>\n",
       "      <td>wasi n ta</td>\n",
       "      <td>For_Annotation_1_LS.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>tomansunchik</td>\n",
       "      <td>[[toman, su, nchik]]</td>\n",
       "      <td>[toman, su, nchik]</td>\n",
       "      <td>toman su nchik</td>\n",
       "      <td>For_Annotation_1_LS.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>uyarispaqa</td>\n",
       "      <td>[[uyari, spa, qa]]</td>\n",
       "      <td>[uyari, spa, qa]</td>\n",
       "      <td>uyari spa qa</td>\n",
       "      <td>For_Annotation_1_LS.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>turaykuna</td>\n",
       "      <td>[[tura, y, kuna]]</td>\n",
       "      <td>[tura, y, kuna]</td>\n",
       "      <td>tura y kuna</td>\n",
       "      <td>For_Annotation_1_LS.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>aparuni</td>\n",
       "      <td>[[apa, ru, ni]]</td>\n",
       "      <td>[apa, ru, ni]</td>\n",
       "      <td>apa ru ni</td>\n",
       "      <td>For_Annotation_1_LS.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>tiyakuq</td>\n",
       "      <td>[[tiya, ku, q]]</td>\n",
       "      <td>[tiya, ku, q]</td>\n",
       "      <td>tiya ku q</td>\n",
       "      <td>For_Annotation_2_LS.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>tumani</td>\n",
       "      <td>[[tuma, ni]]</td>\n",
       "      <td>[tuma, ni]</td>\n",
       "      <td>tuma ni</td>\n",
       "      <td>For_Annotation_2_LS.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>qhisakuwan</td>\n",
       "      <td>[[qhisaku, wan]]</td>\n",
       "      <td>[qhisaku, wan]</td>\n",
       "      <td>qhisaku wan</td>\n",
       "      <td>For_Annotation_2_LS.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Word                        Gold         Morph_split  \\\n",
       "0         unupas                [[unu, pas]]          [unu, pas]   \n",
       "1        umankus             [[uma, nku, s]]       [uma, nku, s]   \n",
       "2        hikurin               [[hikuri, n]]         [hikuri, n]   \n",
       "3         sutipi                [[suti, pi]]          [suti, pi]   \n",
       "4        pikunas             [[pi, kuna, s]]       [pi, kuna, s]   \n",
       "5         atipaq  [[ati, paq], [ati, pa, q]]          [ati, paq]   \n",
       "6         tomani                [[toma, ni]]          [toma, ni]   \n",
       "7         rantiq                [[ranti, q]]          [ranti, q]   \n",
       "8       imakunas            [[ima, kuna, s]]      [ima, kuna, s]   \n",
       "9         chiqaq                  [[chiqaq]]            [chiqaq]   \n",
       "10      hampichu              [[hampi, chu]]        [hampi, chu]   \n",
       "11        misata                [[misa, ta]]          [misa, ta]   \n",
       "12         nakuy               [[na, ku, y]]         [na, ku, y]   \n",
       "13         hampi                   [[hampi]]             [hampi]   \n",
       "14        hampiy                [[hampi, y]]          [hampi, y]   \n",
       "15         limun                   [[limun]]             [limun]   \n",
       "16         hinas                 [[hina, s]]           [hina, s]   \n",
       "17        atispa                [[ati, spa]]          [ati, spa]   \n",
       "18         riman                 [[rima, n]]           [rima, n]   \n",
       "19         matin                 [[mati, n]]           [mati, n]   \n",
       "20       yachani               [[yacha, ni]]         [yacha, ni]   \n",
       "21      chaypiqa            [[chay, pi, qa]]      [chay, pi, qa]   \n",
       "22        niqchu              [[ni, q, chu]]        [ni, q, chu]   \n",
       "23        napiqa              [[na, pi, qa]]        [na, pi, qa]   \n",
       "24        ripuni              [[ri, pu, ni]]        [ri, pu, ni]   \n",
       "25     kapunchik           [[ka, pu, nchik]]     [ka, pu, nchik]   \n",
       "26        raykum           [[ima, rayku, m]]     [ima, rayku, m]   \n",
       "27        nispam              [[ni, spa, m]]        [ni, spa, m]   \n",
       "28       rimanku             [[rima, n, ku]]       [rima, n, ku]   \n",
       "29       punchay                 [[punchay]]           [punchay]   \n",
       "30    tumankupas          [[tuma, nku, pas]]    [tuma, nku, pas]   \n",
       "31        chikan                  [[chikan]]            [chikan]   \n",
       "32         munay                 [[muna, y]]           [muna, y]   \n",
       "33        atikun              [[ati, ku, n]]        [ati, ku, n]   \n",
       "34        chunka                  [[chunka]]            [chunka]   \n",
       "35    saniyapuni          [[sani, ya, puni]]    [sani, ya, puni]   \n",
       "36      hampusaq              [[hampu, saq]]        [hampu, saq]   \n",
       "37    ñuqaykupis          [[ñuqa, yku, pis]]    [ñuqa, yku, pis]   \n",
       "38       chhikan                 [[chhikan]]           [chhikan]   \n",
       "39      santusmi              [[santus, mi]]        [santus, mi]   \n",
       "40         sutin                 [[suti, n]]           [suti, n]   \n",
       "41  intirukunapi        [[intiru, kuna, pi]]  [intiru, kuna, pi]   \n",
       "42       wasinta             [[wasi, n, ta]]       [wasi, n, ta]   \n",
       "43  tomansunchik        [[toman, su, nchik]]  [toman, su, nchik]   \n",
       "44    uyarispaqa          [[uyari, spa, qa]]    [uyari, spa, qa]   \n",
       "45     turaykuna           [[tura, y, kuna]]     [tura, y, kuna]   \n",
       "46       aparuni             [[apa, ru, ni]]       [apa, ru, ni]   \n",
       "47       tiyakuq             [[tiya, ku, q]]       [tiya, ku, q]   \n",
       "48        tumani                [[tuma, ni]]          [tuma, ni]   \n",
       "49    qhisakuwan            [[qhisaku, wan]]      [qhisaku, wan]   \n",
       "\n",
       "   Morph_split_str                 Filename  \n",
       "0          unu pas  For_Annotation_1_LS.csv  \n",
       "1        uma nku s  For_Annotation_1_LS.csv  \n",
       "2         hikuri n  For_Annotation_1_LS.csv  \n",
       "3          suti pi  For_Annotation_1_LS.csv  \n",
       "4        pi kuna s  For_Annotation_1_LS.csv  \n",
       "5          ati paq  For_Annotation_1_LS.csv  \n",
       "6          toma ni  For_Annotation_1_LS.csv  \n",
       "7          ranti q  For_Annotation_1_LS.csv  \n",
       "8       ima kuna s  For_Annotation_1_LS.csv  \n",
       "9           chiqaq  For_Annotation_1_LS.csv  \n",
       "10       hampi chu  For_Annotation_1_LS.csv  \n",
       "11         misa ta  For_Annotation_1_LS.csv  \n",
       "12         na ku y  For_Annotation_1_LS.csv  \n",
       "13           hampi  For_Annotation_1_LS.csv  \n",
       "14         hampi y  For_Annotation_1_LS.csv  \n",
       "15           limun  For_Annotation_1_LS.csv  \n",
       "16          hina s  For_Annotation_1_LS.csv  \n",
       "17         ati spa  For_Annotation_1_LS.csv  \n",
       "18          rima n  For_Annotation_1_LS.csv  \n",
       "19          mati n  For_Annotation_1_LS.csv  \n",
       "20        yacha ni  For_Annotation_1_LS.csv  \n",
       "21      chay pi qa  For_Annotation_1_LS.csv  \n",
       "22        ni q chu  For_Annotation_1_LS.csv  \n",
       "23        na pi qa  For_Annotation_1_LS.csv  \n",
       "24        ri pu ni  For_Annotation_1_LS.csv  \n",
       "25     ka pu nchik  For_Annotation_1_LS.csv  \n",
       "26     ima rayku m  For_Annotation_1_LS.csv  \n",
       "27        ni spa m  For_Annotation_1_LS.csv  \n",
       "28       rima n ku  For_Annotation_1_LS.csv  \n",
       "29         punchay  For_Annotation_1_LS.csv  \n",
       "30    tuma nku pas  For_Annotation_1_LS.csv  \n",
       "31          chikan  For_Annotation_1_LS.csv  \n",
       "32          muna y  For_Annotation_1_LS.csv  \n",
       "33        ati ku n  For_Annotation_1_LS.csv  \n",
       "34          chunka  For_Annotation_1_LS.csv  \n",
       "35    sani ya puni  For_Annotation_1_LS.csv  \n",
       "36       hampu saq  For_Annotation_1_LS.csv  \n",
       "37    ñuqa yku pis  For_Annotation_1_LS.csv  \n",
       "38         chhikan  For_Annotation_1_LS.csv  \n",
       "39       santus mi  For_Annotation_1_LS.csv  \n",
       "40          suti n  For_Annotation_1_LS.csv  \n",
       "41  intiru kuna pi  For_Annotation_1_LS.csv  \n",
       "42       wasi n ta  For_Annotation_1_LS.csv  \n",
       "43  toman su nchik  For_Annotation_1_LS.csv  \n",
       "44    uyari spa qa  For_Annotation_1_LS.csv  \n",
       "45     tura y kuna  For_Annotation_1_LS.csv  \n",
       "46       apa ru ni  For_Annotation_1_LS.csv  \n",
       "47       tiya ku q  For_Annotation_2_LS.csv  \n",
       "48         tuma ni  For_Annotation_2_LS.csv  \n",
       "49     qhisaku wan  For_Annotation_2_LS.csv  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ec5eed04",
   "metadata": {},
   "outputs": [],
   "source": [
    "graphemes = [\n",
    "    \"ch\",\"ll\",\"rr\",\"tr\",\"kw\",\"ph\",  # digraphs/trigraphs\n",
    "    \"a\",\"b\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\",\"k\",\"l\",\"m\",\"n\",\"ñ\",\"o\",\"p\",\"q\",\n",
    "    \"r\",\"s\",\"t\",\"u\",\"v\",\"w\",\"x\",\"y\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e48798ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "95db6a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(\"|\".join(sorted(graphemes, key=len, reverse=True)))\n",
    "\n",
    "def tokenize_morphemes(morphs):\n",
    "    return [pattern.findall(m.lower()) for m in morphs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6b21262d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_df[\"Char_split\"] = gold_df[\"Morph_split\"].apply(tokenize_morphemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8941c70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vowels = {\"a\", \"i\", \"e\", \"o\", \"u\"}\n",
    "\n",
    "def grapheme_to_cv(grapheme):\n",
    "    return \"V\" if grapheme in vowels else \"C\"\n",
    "\n",
    "def morphs_to_cv(morphs):\n",
    "    return [[grapheme_to_cv(g) for g in morph] for morph in morphs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5da0120c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_df[\"CV_split\"] = gold_df[\"Char_split\"].apply(morphs_to_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "66284fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_to_string(cv_split):\n",
    "    \"\"\"Convert nested CV list to dash-separated string.\"\"\"\n",
    "    return \"-\".join(\"\".join(m) for m in cv_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e2e4fcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ec611f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "dd6db475",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_df[\"Full_chain\"] = gold_df[\"CV_split\"].apply(cv_to_string)\n",
    "\n",
    "# Create Trimmed_chain, but use NaN if no dash\n",
    "str_df[\"Trimmed_chain\"] = str_df[\"Full_chain\"].apply(\n",
    "    lambda x: x.split(\"-\", 1)[1] if \"-\" in x else np.nan\n",
    ")\n",
    "\n",
    "str_df[\"Word\"] = gold_df[\"Word\"]\n",
    "str_df[\"Char_split\"] = gold_df[\"Char_split\"]\n",
    "str_df[\"Morph_split\"] = gold_df[\"Morph_split\"]\n",
    "\n",
    "# Drop rows where Trimmed_chain is NaN\n",
    "str_df = str_df.dropna(subset=[\"Trimmed_chain\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c92caf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word length\n",
    "str_df[\"Word_len\"] = str_df[\"Word\"].str.len()\n",
    "\n",
    "# consonant and vowel count from Full_chain\n",
    "str_df[\"Vowel_no\"] = str_df[\"Full_chain\"].str.count(\"V\")\n",
    "str_df[\"Cons_no\"] = str_df[\"Full_chain\"].str.count(\"C\")\n",
    "\n",
    "# tail consonant and vowel counts (last segment in Full_chain after '-')\n",
    "str_df[\"Tail_cons_no\"] = str_df[\"Trimmed_chain\"].str.count(\"C\")\n",
    "str_df[\"Tail_vowel_no\"] = str_df[\"Trimmed_chain\"].str.count(\"V\")\n",
    "\n",
    "# number of splits from Morph_split\n",
    "str_df[\"No_splits\"] = str_df[\"Morph_split\"].str.len()\n",
    "\n",
    "# total y/w count in word\n",
    "str_df[\"YW_count\"] = str_df[\"Word\"].str.count(\"[yw]\")\n",
    "\n",
    "# tail y/w count (all morphs except first)\n",
    "str_df[\"Tail_YW_count\"] = str_df[\"Morph_split\"].apply(\n",
    "    lambda ms: sum(m.count(\"y\") + m.count(\"w\") for m in ms[1:])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "822afc9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Full_chain</th>\n",
       "      <th>Trimmed_chain</th>\n",
       "      <th>Word</th>\n",
       "      <th>Char_split</th>\n",
       "      <th>Morph_split</th>\n",
       "      <th>Word_len</th>\n",
       "      <th>Vowel_no</th>\n",
       "      <th>Cons_no</th>\n",
       "      <th>Tail_cons_no</th>\n",
       "      <th>Tail_vowel_no</th>\n",
       "      <th>No_splits</th>\n",
       "      <th>YW_count</th>\n",
       "      <th>Tail_YW_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VCVCCVCVV-CVC</td>\n",
       "      <td>CVC</td>\n",
       "      <td>cementerioman</td>\n",
       "      <td>[[e, m, e, n, t, e, r, i, o], [m, a, n]]</td>\n",
       "      <td>[cementerio, man]</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CVCCV-CCV-CV-C-CV</td>\n",
       "      <td>CCV-CV-C-CV</td>\n",
       "      <td>kawsachkananta</td>\n",
       "      <td>[[k, a, w, s, a], [ch, k, a], [n, a], [n], [t,...</td>\n",
       "      <td>[kawsa, chka, na, n, ta]</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CVCV-CV-C-CVC</td>\n",
       "      <td>CV-C-CVC</td>\n",
       "      <td>mañakunpis</td>\n",
       "      <td>[[m, a, ñ, a], [k, u], [n], [p, i, s]]</td>\n",
       "      <td>[maña, ku, n, pis]</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VCVCCV-CV-CVC</td>\n",
       "      <td>CV-CVC</td>\n",
       "      <td>imaynapichus</td>\n",
       "      <td>[[i, m, a, y, n, a], [p, i], [ch, u, s]]</td>\n",
       "      <td>[imayna, pi, chus]</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CVCV-CVC</td>\n",
       "      <td>CVC</td>\n",
       "      <td>qipiyuq</td>\n",
       "      <td>[[q, i, p, i], [y, u, q]]</td>\n",
       "      <td>[qipi, yuq]</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Full_chain Trimmed_chain            Word  \\\n",
       "0      VCVCCVCVV-CVC           CVC   cementerioman   \n",
       "1  CVCCV-CCV-CV-C-CV   CCV-CV-C-CV  kawsachkananta   \n",
       "2      CVCV-CV-C-CVC      CV-C-CVC      mañakunpis   \n",
       "3      VCVCCV-CV-CVC        CV-CVC    imaynapichus   \n",
       "4           CVCV-CVC           CVC         qipiyuq   \n",
       "\n",
       "                                          Char_split  \\\n",
       "0           [[e, m, e, n, t, e, r, i, o], [m, a, n]]   \n",
       "1  [[k, a, w, s, a], [ch, k, a], [n, a], [n], [t,...   \n",
       "2             [[m, a, ñ, a], [k, u], [n], [p, i, s]]   \n",
       "3           [[i, m, a, y, n, a], [p, i], [ch, u, s]]   \n",
       "4                          [[q, i, p, i], [y, u, q]]   \n",
       "\n",
       "                Morph_split  Word_len  Vowel_no  Cons_no  Tail_cons_no  \\\n",
       "0         [cementerio, man]        13         6        6             2   \n",
       "1  [kawsa, chka, na, n, ta]        14         5        8             5   \n",
       "2        [maña, ku, n, pis]        10         4        6             4   \n",
       "3        [imayna, pi, chus]        12         5        6             3   \n",
       "4               [qipi, yuq]         7         3        4             2   \n",
       "\n",
       "   Tail_vowel_no  No_splits  YW_count  Tail_YW_count  \n",
       "0              1          2         0              0  \n",
       "1              3          5         1              0  \n",
       "2              2          4         0              0  \n",
       "3              2          3         1              0  \n",
       "4              1          2         1              1  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9d8c983b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "85804240",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast, re, numpy as np, pandas as pd, torch, torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b68083a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "RNG = 42\n",
    "torch.manual_seed(RNG)\n",
    "np.random.seed(RNG)\n",
    "\n",
    "NEW_NUM_FEATS = [\n",
    "    \"Word_len\", \"Vowel_no\", \"Cons_no\",\n",
    "    \"Tail_cons_no\", \"Tail_vowel_no\",\n",
    "    \"No_splits\", \"YW_count\", \"Tail_YW_count\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e5894398",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_list(x):\n",
    "    if isinstance(x, list): return x\n",
    "    s = str(x)\n",
    "    try:\n",
    "        return ast.literal_eval(s)\n",
    "    except Exception:\n",
    "        s2 = s.replace(\"[[\", \"[['\").replace(\"]]\", \"']]\").replace(\"], [\", \"'],['\").replace(\", \", \"','\")\n",
    "        return ast.literal_eval(s2)\n",
    "\n",
    "def flatten(list_of_lists):\n",
    "    out=[]\n",
    "    for seg in list_of_lists: out.extend(seg)\n",
    "    return [str(t) for t in out]\n",
    "\n",
    "def extract_priv_features_from_row(row, feat_names):\n",
    "    vec=[]\n",
    "    for k in feat_names:\n",
    "        val = row[k] if (k in row and pd.notna(row[k])) else 0.0\n",
    "        try: vec.append(float(val))\n",
    "        except Exception: vec.append(0.0)\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bcb8fc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ===================================================================\n",
    "# NEW CODE: Suffix HMM Prior Model (Replaces Decision Tree)\n",
    "# ===================================================================\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "class SuffixHMMPrior:\n",
    "    \"\"\"\n",
    "    Calculates boundary priors using a suffix list and the Forward-Backward algorithm.\n",
    "    This model assumes segmentation proceeds from right to left.\n",
    "    \"\"\"\n",
    "    def __init__(self, suffix_log_probs, max_suffix_len, unk_penalty=-15.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            suffix_log_probs (dict): A dictionary mapping a suffix string to its log probability.\n",
    "            max_suffix_len (int): The maximum length of a suffix to consider.\n",
    "            unk_penalty (float): The log probability assigned to any substring not in our list\n",
    "                                 (i.e., the cost of it being part of the root).\n",
    "        \"\"\"\n",
    "        self.log_probs = suffix_log_probs\n",
    "        self.max_len = max_suffix_len\n",
    "        self.unk_penalty = unk_penalty\n",
    "        self.LOG_ZERO = -1e9 # A very small number representing log(0)\n",
    "\n",
    "    def _get_log_prob(self, segment):\n",
    "        # The cost of a segment is its suffix probability, or a penalty if it's unknown (part of the root).\n",
    "        return self.log_probs.get(segment, self.unk_penalty)\n",
    "\n",
    "    def _forward_pass(self, word):\n",
    "        \"\"\"Calculates the log probability of all segmentations for each prefix.\"\"\"\n",
    "        n = len(word)\n",
    "        alpha = [self.LOG_ZERO] * (n + 1)\n",
    "        alpha[0] = 0.0  # log(1) for the empty prefix\n",
    "\n",
    "        for i in range(1, n + 1):\n",
    "            # To calculate alpha[i], we sum probabilities from all previous split points j\n",
    "            log_sums = []\n",
    "            for j in range(max(0, i - self.max_len), i):\n",
    "                segment = word[j:i]\n",
    "                log_p_segment = self._get_log_prob(segment)\n",
    "                log_sums.append(alpha[j] + log_p_segment)\n",
    "            \n",
    "            if log_sums:\n",
    "                alpha[i] = torch.logsumexp(torch.tensor(log_sums), dim=0).item()\n",
    "        return alpha\n",
    "\n",
    "    def _backward_pass(self, word):\n",
    "        \"\"\"Calculates the log probability of all segmentations for each suffix.\"\"\"\n",
    "        n = len(word)\n",
    "        beta = [self.LOG_ZERO] * (n + 1)\n",
    "        beta[n] = 0.0  # log(1) for the empty suffix\n",
    "\n",
    "        for i in range(n - 1, -1, -1):\n",
    "            log_sums = []\n",
    "            for j in range(i + 1, min(n + 1, i + self.max_len + 1)):\n",
    "                segment = word[i:j]\n",
    "                log_p_segment = self._get_log_prob(segment)\n",
    "                log_sums.append(beta[j] + log_p_segment)\n",
    "\n",
    "            if log_sums:\n",
    "                beta[i] = torch.logsumexp(torch.tensor(log_sums), dim=0).item()\n",
    "        return beta\n",
    "\n",
    "    def get_boundary_priors(self, word):\n",
    "        \"\"\"\n",
    "        Calculate the posterior probability of a boundary at each position i.\n",
    "        P(boundary at i | word) is proportional to alpha[i] * beta[i].\n",
    "        \"\"\"\n",
    "        n = len(word)\n",
    "        if n <= 1:\n",
    "            return []\n",
    "\n",
    "        alpha = self._forward_pass(word)\n",
    "        beta = self._backward_pass(word)\n",
    "        \n",
    "        log_total_prob = alpha[n]\n",
    "        if log_total_prob == self.LOG_ZERO: # No valid segmentation found\n",
    "             return [0.0] * (n - 1)\n",
    "\n",
    "        log_priors = []\n",
    "        for i in range(1, n):\n",
    "            # Log probability of a boundary at i is log(alpha[i]) + log(beta[i])\n",
    "            log_p_boundary = alpha[i] + beta[i]\n",
    "            log_priors.append(log_p_boundary)\n",
    "        \n",
    "        # Normalize to get probabilities\n",
    "        log_priors_tensor = torch.tensor(log_priors)\n",
    "        # We subtract the log probability of the whole word to normalize\n",
    "        normalized_log_priors = log_priors_tensor - log_total_prob\n",
    "        \n",
    "        return torch.exp(normalized_log_priors).tolist()\n",
    "\n",
    "def train_hmm_prior(samples):\n",
    "    \"\"\"\n",
    "    Creates the SuffixHMMPrior by calculating suffix frequencies from training data.\n",
    "    This replaces `train_dt_prior`.\n",
    "    \"\"\"\n",
    "    suffix_counts = Counter()\n",
    "    max_suffix_len = 0\n",
    "    \n",
    "    for s in samples:\n",
    "        cs = s[\"tokens\"] # This is now a list of grapheme tokens\n",
    "        morph_lens = [len(seg) for seg in safe_list(s['y_morphs'])] # Assuming y_morphs is available\n",
    "        \n",
    "        current_idx = len(cs)\n",
    "        # Iterate backwards through morphemes (which are the suffixes)\n",
    "        for morph_len in reversed(morph_lens[1:]): # Skip the root\n",
    "            start_idx = current_idx - morph_len\n",
    "            suffix_tokens = cs[start_idx:current_idx]\n",
    "            suffix_str = \"\".join(suffix_tokens)\n",
    "            \n",
    "            suffix_counts[suffix_str] += 1\n",
    "            max_suffix_len = max(max_suffix_len, len(suffix_str))\n",
    "            current_idx = start_idx\n",
    "\n",
    "    total_suffix_obs = sum(suffix_counts.values())\n",
    "    \n",
    "    # Calculate log probabilities with Laplace smoothing\n",
    "    log_probs = {\n",
    "        suffix: math.log((count + 1) / (total_suffix_obs + len(suffix_counts)))\n",
    "        for suffix, count in suffix_counts.items()\n",
    "    }\n",
    "\n",
    "    # Heuristic penalty for unknown segments (roots). Should be lower than rare suffixes.\n",
    "    avg_log_prob = sum(log_probs.values()) / len(log_probs) if log_probs else 0\n",
    "    unk_penalty = avg_log_prob * 1.5 \n",
    "\n",
    "    print(f\"HMM Prior: Found {len(log_probs)} unique suffixes. Max length: {max_suffix_len}. Unk penalty: {unk_penalty:.2f}\")\n",
    "    \n",
    "    return SuffixHMMPrior(log_probs, max_suffix_len, unk_penalty=unk_penalty)\n",
    "\n",
    "# ===================================================================\n",
    "# REVISED CODE: Create HMM Prior from a user-provided suffix list\n",
    "# ===================================================================\n",
    "import math\n",
    "\n",
    "def create_hmm_prior_from_list(allowed_suffixes: list, unk_penalty: float = -15.0):\n",
    "    \"\"\"\n",
    "    Creates the SuffixHMMPrior using a predefined list of allowed suffixes.\n",
    "    This replaces `train_hmm_prior`.\n",
    "\n",
    "    Args:\n",
    "        allowed_suffixes (list): A list of valid Quechua suffix strings.\n",
    "    \"\"\"\n",
    "    if not allowed_suffixes:\n",
    "        raise ValueError(\"The provided suffix list cannot be empty.\")\n",
    "\n",
    "    # Assign a high, uniform log probability (e.g., log(1)=0) to all known suffixes.\n",
    "    # This expresses a strong preference for using these segments.\n",
    "    suffix_log_probs = {suffix: 0.0 for suffix in allowed_suffixes}\n",
    "\n",
    "    # The max length is determined by your list.\n",
    "    max_suffix_len = len(max(allowed_suffixes, key=len))\n",
    "\n",
    "    # A penalty for any segment NOT in the list (i.e., part of a root).\n",
    "    # This should be a reasonably large negative number.\n",
    "\n",
    "    print(f\"HMM Prior: Initialized with {len(allowed_suffixes)} provided suffixes. Max length: {max_suffix_len}.\")\n",
    "\n",
    "    # The SuffixHMMPrior class itself does not need to change.\n",
    "    return SuffixHMMPrior(suffix_log_probs, max_suffix_len, unk_penalty=unk_penalty)\n",
    "\n",
    "# We need to add the gold morphemes to the sample builder for training the HMM\n",
    "def build_samples_with_priv(df, feat_names=NEW_NUM_FEATS):\n",
    "    rows = []\n",
    "    for _, r in df.iterrows():\n",
    "        cs = safe_list(r[\"Char_split\"])\n",
    "        toks = flatten(cs)\n",
    "        lens = [len(seg) for seg in cs]\n",
    "        cut_idxs = set(np.cumsum(lens)[:-1].tolist())\n",
    "        y = [1 if (i+1) in cut_idxs else 0 for i in range(len(toks)-1)]\n",
    "        priv = extract_priv_features_from_row(r, feat_names)\n",
    "        \n",
    "        # ADD GOLD MORPHEMES (needed for HMM training)\n",
    "        gold_morphs = [\"\".join(seg) for seg in cs]\n",
    "\n",
    "        rows.append({\"tokens\": toks, \"y\": y, \"priv\": priv, \"y_morphs\": gold_morphs})\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "96de39b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize_window(tokens, i, k_left=2, k_right=2):\n",
    "    feats = {}\n",
    "    for k in range(1, k_left+1):\n",
    "        idx = i-(k-1); feats[f\"L{k}\"] = tokens[idx] if idx >= 0 else \"<BOS>\"\n",
    "    for k in range(1, k_right+1):\n",
    "        idx = i+k; feats[f\"R{k}\"] = tokens[idx] if idx < len(tokens) else \"<EOS>\"\n",
    "    def is_vowel(ch): return ch.lower() in \"aeiouáéíóú\"\n",
    "    L1 = feats[\"L1\"]; R1 = feats[\"R1\"]\n",
    "    feats[\"L1_cv\"] = 'V' if is_vowel(L1[-1]) else 'C'\n",
    "    feats[\"R1_cv\"] = 'V' if (R1 != \"<EOS>\" and is_vowel(R1[0])) else 'C'\n",
    "    feats[\"L1_last\"] = L1[-1]\n",
    "    feats[\"R1_first\"] = R1[0] if R1 != \"<EOS>\" else \"<EOS>\"\n",
    "    return feats\n",
    "\n",
    "# %%\n",
    "# ===================================================================\n",
    "# MODIFIED CODE: Use HMM for prior calculation\n",
    "# ===================================================================\n",
    "\n",
    "def prior_probs_for_sample(hmm_prior, tokens):\n",
    "    \"\"\"\n",
    "    Generates prior probabilities for a single tokenized sample using the HMM.\n",
    "    This replaces the DT-based version.\n",
    "    \"\"\"\n",
    "    if hmm_prior is None or len(tokens) <= 1:\n",
    "        return [0.5] * (max(len(tokens) - 1, 0))\n",
    "\n",
    "    word = \"\".join(tokens)\n",
    "    # HMM gives character-level boundary probabilities\n",
    "    char_priors = hmm_prior.get_boundary_priors(word)\n",
    "\n",
    "    # Map character-level priors to token-level boundary priors\n",
    "    token_boundary_indices = np.cumsum([len(t) for t in tokens[:-1]]) - 1\n",
    "    \n",
    "    token_priors = []\n",
    "    for idx in token_boundary_indices:\n",
    "        if 0 <= idx < len(char_priors):\n",
    "            token_priors.append(char_priors[idx])\n",
    "        else:\n",
    "            token_priors.append(0.5) # Fallback for any index issue\n",
    "\n",
    "    return token_priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "880ff69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_k_teacher_priv(samples, feat_dim):\n",
    "    \"\"\"\n",
    "    Train a regressor to predict K (number of cuts) from priv feature vector.\n",
    "    \"\"\"\n",
    "    X = np.array([s[\"priv\"] for s in samples], dtype=float)   # (N, F)\n",
    "    y = np.array([int(np.sum(s[\"y\"])) for s in samples], dtype=float)\n",
    "    reg = DecisionTreeRegressor(max_depth=6, min_samples_leaf=10, random_state=RNG)\n",
    "    reg.fit(X, y)\n",
    "    return reg\n",
    "\n",
    "def predict_k_hat_priv(reg, priv_batch):\n",
    "    # priv_batch: (B, F) float tensor\n",
    "    with torch.no_grad():\n",
    "        k = reg.predict(priv_batch.cpu().numpy())\n",
    "    return torch.tensor(k, dtype=torch.float32, device=priv_batch.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "0031c849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using existing HMM prior from loaded model\n",
      "🔍 Searching through 7713 words for correct segmentations...\n",
      "\n",
      "======================================================================\n",
      "HMM PRIOR PROCESSING: 'umankus' (length=7)\n",
      "======================================================================\n",
      "\n",
      "HMM Prior Configuration:\n",
      "  Max suffix length: 8\n",
      "  Unknown penalty: -11.3393\n",
      "  Number of known suffixes: 511\n",
      "  Sample suffixes (first 10):\n",
      "    'pis': log P = -4.1271 (P = 0.016129)\n",
      "    'y': log P = -3.6520 (P = 0.025939)\n",
      "    'ita': log P = -4.9406 (P = 0.007150)\n",
      "    'ña': log P = -4.1801 (P = 0.015298)\n",
      "    'alla': log P = -4.4750 (P = 0.011390)\n",
      "    'r': log P = -5.6814 (P = 0.003409)\n",
      "    'yman': log P = -5.9293 (P = 0.002660)\n",
      "    'spa': log P = -4.3451 (P = 0.012970)\n",
      "    'taq': log P = -3.6329 (P = 0.026438)\n",
      "    'n': log P = -2.4625 (P = 0.085218)\n",
      "    ... and 501 more\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "FORWARD PASS (α) - Computing log probabilities of all prefixes\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "  α[0] = 0.0000  (empty prefix, base case)\n",
      "  α[1] = -5.3345  (prefix: 'u')\n",
      "      Candidate: j=0, segment='u', log P(seg)=-5.3345, α[0]=0.0000, total=-5.3345\n",
      "  α[2] = -7.1914  (prefix: 'um')\n",
      "      Candidate: j=0, segment='um', log P(seg)=-7.1978, α[0]=0.0000, total=-7.1978\n",
      "      Candidate: j=1, segment='m', log P(seg)=-6.9101, α[1]=-5.3345, total=-12.2446\n",
      "  α[3] = -7.5809  (prefix: 'uma')\n",
      "      Candidate: j=0, segment='uma', log P(seg)=-7.6032, α[0]=0.0000, total=-7.6032\n",
      "      Candidate: j=2, segment='a', log P(seg)=-4.2076, α[2]=-7.1914, total=-11.3990\n",
      "      Candidate: j=1, segment='ma', log P(seg)=-11.3393, α[1]=-5.3345, total=-16.6739\n",
      "  α[4] = -8.9523  (prefix: 'uman')\n",
      "      Candidate: j=1, segment='man', log P(seg)=-4.2076, α[1]=-5.3345, total=-9.5422\n",
      "      Candidate: j=3, segment='n', log P(seg)=-2.4625, α[3]=-7.5809, total=-10.0434\n",
      "      Candidate: j=0, segment='uman', log P(seg)=-11.3393, α[0]=0.0000, total=-11.3393\n",
      "  α[5] = -11.1481  (prefix: 'umank')\n",
      "      Candidate: j=0, segment='umank', log P(seg)=-11.3393, α[0]=0.0000, total=-11.3393\n",
      "      Candidate: j=2, segment='ank', log P(seg)=-5.7314, α[2]=-7.1914, total=-12.9228\n",
      "      Candidate: j=1, segment='mank', log P(seg)=-11.3393, α[1]=-5.3345, total=-16.6739\n",
      "  α[6] = -10.4759  (prefix: 'umanku')\n",
      "      Candidate: j=3, segment='nku', log P(seg)=-3.6912, α[3]=-7.5809, total=-11.2721\n",
      "      Candidate: j=0, segment='umanku', log P(seg)=-11.3393, α[0]=0.0000, total=-11.3393\n",
      "      Candidate: j=4, segment='ku', log P(seg)=-3.6236, α[4]=-8.9523, total=-12.5758\n",
      "  α[7] = -11.2695  (prefix: 'umankus')\n",
      "      Candidate: j=0, segment='umankus', log P(seg)=-11.3393, α[0]=0.0000, total=-11.3393\n",
      "      Candidate: j=6, segment='s', log P(seg)=-3.5809, α[6]=-10.4759, total=-14.0567\n",
      "      Candidate: j=1, segment='mankus', log P(seg)=-11.3393, α[1]=-5.3345, total=-16.6739\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "BACKWARD PASS (β) - Computing log probabilities of all suffixes\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "  β[7] = 0.0000  (empty suffix, base case)\n",
      "  β[6] = -3.5809  (suffix: 's')\n",
      "      Candidate: j=7, segment='s', log P(seg)=-3.5809, β[7]=0.0000, total=-3.5809\n",
      "  β[5] = -8.8305  (suffix: 'us')\n",
      "      Candidate: j=6, segment='u', log P(seg)=-5.3345, β[6]=-3.5809, total=-8.9154\n",
      "      Candidate: j=7, segment='us', log P(seg)=-11.3393, β[7]=0.0000, total=-11.3393\n",
      "  β[4] = -7.1885  (suffix: 'kus')\n",
      "      Candidate: j=6, segment='ku', log P(seg)=-3.6236, β[6]=-3.5809, total=-7.2044\n",
      "      Candidate: j=7, segment='kus', log P(seg)=-11.3393, β[7]=0.0000, total=-11.3393\n",
      "      Candidate: j=5, segment='k', log P(seg)=-11.3393, β[5]=-8.8305, total=-20.1699\n",
      "  β[3] = -7.1679  (suffix: 'nkus')\n",
      "      Candidate: j=6, segment='nku', log P(seg)=-3.6912, β[6]=-3.5809, total=-7.2721\n",
      "      Candidate: j=4, segment='n', log P(seg)=-2.4625, β[4]=-7.1885, total=-9.6511\n",
      "      Candidate: j=7, segment='nkus', log P(seg)=-11.3393, β[7]=0.0000, total=-11.3393\n",
      "  β[2] = -10.5385  (suffix: 'ankus')\n",
      "      Candidate: j=7, segment='ankus', log P(seg)=-11.3393, β[7]=0.0000, total=-11.3393\n",
      "      Candidate: j=3, segment='a', log P(seg)=-4.2076, β[3]=-7.1679, total=-11.3755\n",
      "      Candidate: j=4, segment='an', log P(seg)=-5.7841, β[4]=-7.1885, total=-12.9726\n",
      "  β[1] = -10.6584  (suffix: 'mankus')\n",
      "      Candidate: j=7, segment='mankus', log P(seg)=-11.3393, β[7]=0.0000, total=-11.3393\n",
      "      Candidate: j=4, segment='man', log P(seg)=-4.2076, β[4]=-7.1885, total=-11.3961\n",
      "      Candidate: j=6, segment='manku', log P(seg)=-11.3393, β[6]=-3.5809, total=-14.9202\n",
      "  β[0] = -11.2695  (suffix: 'umankus')\n",
      "      Candidate: j=7, segment='umankus', log P(seg)=-11.3393, β[7]=0.0000, total=-11.3393\n",
      "      Candidate: j=3, segment='uma', log P(seg)=-7.6032, β[3]=-7.1679, total=-14.7712\n",
      "      Candidate: j=6, segment='umanku', log P(seg)=-11.3393, β[6]=-3.5809, total=-14.9202\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "BOUNDARY PRIOR COMPUTATION\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "  Total log probability: α[7] = -11.2695\n",
      "  Total probability: P(word) = 0.000013\n",
      "\n",
      "  Computing P(boundary at i | word) = exp(α[i] + β[i] - α[n])\n",
      "\n",
      "  Position-by-position boundary probabilities:\n",
      "    Position 1 (after 'u'):\n",
      "      α[1] + β[1] = -5.3345 + -10.6584 = -15.9929\n",
      "      Normalized: -15.9929 - -11.2695 = -4.7234\n",
      "      P(boundary) = exp(-4.7234) = 0.0089\n",
      "    Position 2 (after 'm'):\n",
      "      α[2] + β[2] = -7.1914 + -10.5385 = -17.7298\n",
      "      Normalized: -17.7298 - -11.2695 = -6.4603\n",
      "      P(boundary) = exp(-6.4603) = 0.0016\n",
      "    Position 3 (after 'a'):\n",
      "      α[3] + β[3] = -7.5809 + -7.1679 = -14.7488\n",
      "      Normalized: -14.7488 - -11.2695 = -3.4793\n",
      "      P(boundary) = exp(-3.4793) = 0.0308\n",
      "    Position 4 (after 'n'):\n",
      "      α[4] + β[4] = -8.9523 + -7.1885 = -16.1408\n",
      "      Normalized: -16.1408 - -11.2695 = -4.8713\n",
      "      P(boundary) = exp(-4.8713) = 0.0077\n",
      "    Position 5 (after 'k'):\n",
      "      α[5] + β[5] = -11.1481 + -8.8305 = -19.9786\n",
      "      Normalized: -19.9786 - -11.2695 = -8.7091\n",
      "      P(boundary) = exp(-8.7091) = 0.0002\n",
      "    Position 6 (after 'u'):\n",
      "      α[6] + β[6] = -10.4759 + -3.5809 = -14.0567\n",
      "      Normalized: -14.0567 - -11.2695 = -2.7872\n",
      "      P(boundary) = exp(-2.7872) = 0.0616\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "FINAL OUTPUT\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "Word: 'umankus'\n",
      "\n",
      "Character sequence: u m a n k u s\n",
      "Boundary probabilities:\n",
      "  0.009 0.002 0.031 0.008 0.000 0.062\n",
      "\n",
      "Visualization:\n",
      "  u m a n k u s\n",
      "             \n",
      "  0.01 0.00 0.03 0.01 0.00 0.06\n",
      "\n",
      "✅ CORRECT SEGMENTATION: 'umankus'\n",
      "   Predicted: uma-nku-s\n",
      "   Gold:      uma-nku-s\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "HMM PRIOR PROCESSING: 'sutipi' (length=6)\n",
      "======================================================================\n",
      "\n",
      "HMM Prior Configuration:\n",
      "  Max suffix length: 8\n",
      "  Unknown penalty: -11.3393\n",
      "  Number of known suffixes: 511\n",
      "  Sample suffixes (first 10):\n",
      "    'pis': log P = -4.1271 (P = 0.016129)\n",
      "    'y': log P = -3.6520 (P = 0.025939)\n",
      "    'ita': log P = -4.9406 (P = 0.007150)\n",
      "    'ña': log P = -4.1801 (P = 0.015298)\n",
      "    'alla': log P = -4.4750 (P = 0.011390)\n",
      "    'r': log P = -5.6814 (P = 0.003409)\n",
      "    'yman': log P = -5.9293 (P = 0.002660)\n",
      "    'spa': log P = -4.3451 (P = 0.012970)\n",
      "    'taq': log P = -3.6329 (P = 0.026438)\n",
      "    'n': log P = -2.4625 (P = 0.085218)\n",
      "    ... and 501 more\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "FORWARD PASS (α) - Computing log probabilities of all prefixes\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "  α[0] = 0.0000  (empty prefix, base case)\n",
      "  α[1] = -3.5809  (prefix: 's')\n",
      "      Candidate: j=0, segment='s', log P(seg)=-3.5809, α[0]=0.0000, total=-3.5809\n",
      "  α[2] = -7.1316  (prefix: 'su')\n",
      "      Candidate: j=0, segment='su', log P(seg)=-7.3156, α[0]=0.0000, total=-7.3156\n",
      "      Candidate: j=1, segment='u', log P(seg)=-5.3345, α[1]=-3.5809, total=-8.9154\n",
      "  α[3] = -10.5351  (prefix: 'sut')\n",
      "      Candidate: j=1, segment='ut', log P(seg)=-7.6032, α[1]=-3.5809, total=-11.1841\n",
      "      Candidate: j=0, segment='sut', log P(seg)=-11.3393, α[0]=0.0000, total=-11.3393\n",
      "      Candidate: j=2, segment='t', log P(seg)=-6.9101, α[2]=-7.1316, total=-14.0417\n",
      "  α[4] = -11.2898  (prefix: 'suti')\n",
      "      Candidate: j=0, segment='suti', log P(seg)=-11.3393, α[0]=0.0000, total=-11.3393\n",
      "      Candidate: j=1, segment='uti', log P(seg)=-11.3393, α[1]=-3.5809, total=-14.9202\n",
      "      Candidate: j=2, segment='ti', log P(seg)=-8.2964, α[2]=-7.1316, total=-15.4280\n",
      "  α[5] = -11.3060  (prefix: 'sutip')\n",
      "      Candidate: j=0, segment='sutip', log P(seg)=-11.3393, α[0]=0.0000, total=-11.3393\n",
      "      Candidate: j=1, segment='utip', log P(seg)=-11.3393, α[1]=-3.5809, total=-14.9202\n",
      "      Candidate: j=3, segment='ip', log P(seg)=-6.6869, α[3]=-10.5351, total=-17.2220\n",
      "  α[6] = -11.2848  (prefix: 'sutipi')\n",
      "      Candidate: j=0, segment='sutipi', log P(seg)=-11.3393, α[0]=0.0000, total=-11.3393\n",
      "      Candidate: j=1, segment='utipi', log P(seg)=-11.3393, α[1]=-3.5809, total=-14.9202\n",
      "      Candidate: j=4, segment='pi', log P(seg)=-3.7602, α[4]=-11.2898, total=-15.0500\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "BACKWARD PASS (β) - Computing log probabilities of all suffixes\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "  β[6] = 0.0000  (empty suffix, base case)\n",
      "  β[5] = -5.8985  (suffix: 'i')\n",
      "      Candidate: j=6, segment='i', log P(seg)=-5.8985, β[6]=0.0000, total=-5.8985\n",
      "  β[4] = -3.7599  (suffix: 'pi')\n",
      "      Candidate: j=6, segment='pi', log P(seg)=-3.7602, β[6]=0.0000, total=-3.7602\n",
      "      Candidate: j=5, segment='p', log P(seg)=-6.0628, β[5]=-5.8985, total=-11.9613\n",
      "  β[3] = -9.4435  (suffix: 'ipi')\n",
      "      Candidate: j=4, segment='i', log P(seg)=-5.8985, β[4]=-3.7599, total=-9.6584\n",
      "      Candidate: j=6, segment='ipi', log P(seg)=-11.3393, β[6]=0.0000, total=-11.3393\n",
      "      Candidate: j=5, segment='ip', log P(seg)=-6.6869, β[5]=-5.8985, total=-12.5854\n",
      "  β[2] = -10.9354  (suffix: 'tipi')\n",
      "      Candidate: j=6, segment='tipi', log P(seg)=-11.3393, β[6]=0.0000, total=-11.3393\n",
      "      Candidate: j=4, segment='ti', log P(seg)=-8.2964, β[4]=-3.7599, total=-12.0563\n",
      "      Candidate: j=3, segment='t', log P(seg)=-6.9101, β[3]=-9.4435, total=-16.3536\n",
      "  β[1] = -11.3034  (suffix: 'utipi')\n",
      "      Candidate: j=6, segment='utipi', log P(seg)=-11.3393, β[6]=0.0000, total=-11.3393\n",
      "      Candidate: j=4, segment='uti', log P(seg)=-11.3393, β[4]=-3.7599, total=-15.0992\n",
      "      Candidate: j=2, segment='u', log P(seg)=-5.3345, β[2]=-10.9354, total=-16.2700\n",
      "  β[0] = -11.2848  (suffix: 'sutipi')\n",
      "      Candidate: j=6, segment='sutipi', log P(seg)=-11.3393, β[6]=0.0000, total=-11.3393\n",
      "      Candidate: j=1, segment='s', log P(seg)=-3.5809, β[1]=-11.3034, total=-14.8843\n",
      "      Candidate: j=4, segment='suti', log P(seg)=-11.3393, β[4]=-3.7599, total=-15.0992\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "BOUNDARY PRIOR COMPUTATION\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "  Total log probability: α[6] = -11.2848\n",
      "  Total probability: P(word) = 0.000013\n",
      "\n",
      "  Computing P(boundary at i | word) = exp(α[i] + β[i] - α[n])\n",
      "\n",
      "  Position-by-position boundary probabilities:\n",
      "    Position 1 (after 's'):\n",
      "      α[1] + β[1] = -3.5809 + -11.3034 = -14.8843\n",
      "      Normalized: -14.8843 - -11.2848 = -3.5994\n",
      "      P(boundary) = exp(-3.5994) = 0.0273\n",
      "    Position 2 (after 'u'):\n",
      "      α[2] + β[2] = -7.1316 + -10.9354 = -18.0671\n",
      "      Normalized: -18.0671 - -11.2848 = -6.7822\n",
      "      P(boundary) = exp(-6.7822) = 0.0011\n",
      "    Position 3 (after 't'):\n",
      "      α[3] + β[3] = -10.5351 + -9.4435 = -19.9786\n",
      "      Normalized: -19.9786 - -11.2848 = -8.6937\n",
      "      P(boundary) = exp(-8.6937) = 0.0002\n",
      "    Position 4 (after 'i'):\n",
      "      α[4] + β[4] = -11.2898 + -3.7599 = -15.0497\n",
      "      Normalized: -15.0497 - -11.2848 = -3.7649\n",
      "      P(boundary) = exp(-3.7649) = 0.0232\n",
      "    Position 5 (after 'p'):\n",
      "      α[5] + β[5] = -11.3060 + -5.8985 = -17.2045\n",
      "      Normalized: -17.2045 - -11.2848 = -5.9196\n",
      "      P(boundary) = exp(-5.9196) = 0.0027\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "FINAL OUTPUT\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "Word: 'sutipi'\n",
      "\n",
      "Character sequence: s u t i p i\n",
      "Boundary probabilities:\n",
      "  0.027 0.001 0.000 0.023 0.003\n",
      "\n",
      "Visualization:\n",
      "  s u t i p i\n",
      "           \n",
      "  0.03 0.00 0.00 0.02 0.00\n",
      "\n",
      "✅ CORRECT SEGMENTATION: 'sutipi'\n",
      "   Predicted: suti-pi\n",
      "   Gold:      suti-pi\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "HMM PRIOR PROCESSING: 'pikunas' (length=7)\n",
      "======================================================================\n",
      "\n",
      "HMM Prior Configuration:\n",
      "  Max suffix length: 8\n",
      "  Unknown penalty: -11.3393\n",
      "  Number of known suffixes: 511\n",
      "  Sample suffixes (first 10):\n",
      "    'pis': log P = -4.1271 (P = 0.016129)\n",
      "    'y': log P = -3.6520 (P = 0.025939)\n",
      "    'ita': log P = -4.9406 (P = 0.007150)\n",
      "    'ña': log P = -4.1801 (P = 0.015298)\n",
      "    'alla': log P = -4.4750 (P = 0.011390)\n",
      "    'r': log P = -5.6814 (P = 0.003409)\n",
      "    'yman': log P = -5.9293 (P = 0.002660)\n",
      "    'spa': log P = -4.3451 (P = 0.012970)\n",
      "    'taq': log P = -3.6329 (P = 0.026438)\n",
      "    'n': log P = -2.4625 (P = 0.085218)\n",
      "    ... and 501 more\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "FORWARD PASS (α) - Computing log probabilities of all prefixes\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "  α[0] = 0.0000  (empty prefix, base case)\n",
      "  α[1] = -6.0628  (prefix: 'p')\n",
      "      Candidate: j=0, segment='p', log P(seg)=-6.0628, α[0]=0.0000, total=-6.0628\n",
      "  α[2] = -3.7599  (prefix: 'pi')\n",
      "      Candidate: j=0, segment='pi', log P(seg)=-3.7602, α[0]=0.0000, total=-3.7602\n",
      "      Candidate: j=1, segment='i', log P(seg)=-5.8985, α[1]=-6.0628, total=-11.9613\n",
      "  α[3] = -10.5241  (prefix: 'pik')\n",
      "      Candidate: j=1, segment='ik', log P(seg)=-5.0643, α[1]=-6.0628, total=-11.1270\n",
      "      Candidate: j=0, segment='pik', log P(seg)=-11.3393, α[0]=0.0000, total=-11.3393\n",
      "      Candidate: j=2, segment='k', log P(seg)=-11.3393, α[2]=-3.7599, total=-15.0992\n",
      "  α[4] = -7.3637  (prefix: 'piku')\n",
      "      Candidate: j=2, segment='ku', log P(seg)=-3.6236, α[2]=-3.7599, total=-7.3835\n",
      "      Candidate: j=0, segment='piku', log P(seg)=-11.3393, α[0]=0.0000, total=-11.3393\n",
      "      Candidate: j=1, segment='iku', log P(seg)=-8.7018, α[1]=-6.0628, total=-14.7646\n",
      "  α[5] = -9.5694  (prefix: 'pikun')\n",
      "      Candidate: j=4, segment='n', log P(seg)=-2.4625, α[4]=-7.3637, total=-9.8262\n",
      "      Candidate: j=0, segment='pikun', log P(seg)=-11.3393, α[0]=0.0000, total=-11.3393\n",
      "      Candidate: j=2, segment='kun', log P(seg)=-8.7018, α[2]=-3.7599, total=-12.4618\n",
      "  α[6] = -9.0327  (prefix: 'pikuna')\n",
      "      Candidate: j=2, segment='kuna', log P(seg)=-5.4632, α[2]=-3.7599, total=-9.2231\n",
      "      Candidate: j=0, segment='pikuna', log P(seg)=-11.3393, α[0]=0.0000, total=-11.3393\n",
      "      Candidate: j=4, segment='na', log P(seg)=-4.4046, α[4]=-7.3637, total=-11.7683\n",
      "  α[7] = -11.0723  (prefix: 'pikunas')\n",
      "      Candidate: j=0, segment='pikunas', log P(seg)=-11.3393, α[0]=0.0000, total=-11.3393\n",
      "      Candidate: j=6, segment='s', log P(seg)=-3.5809, α[6]=-9.0327, total=-12.6135\n",
      "      Candidate: j=2, segment='kunas', log P(seg)=-11.3393, α[2]=-3.7599, total=-15.0992\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "BACKWARD PASS (β) - Computing log probabilities of all suffixes\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "  β[7] = 0.0000  (empty suffix, base case)\n",
      "  β[6] = -3.5809  (suffix: 's')\n",
      "      Candidate: j=7, segment='s', log P(seg)=-3.5809, β[7]=0.0000, total=-3.5809\n",
      "  β[5] = -7.7602  (suffix: 'as')\n",
      "      Candidate: j=6, segment='a', log P(seg)=-4.2076, β[6]=-3.5809, total=-7.7885\n",
      "      Candidate: j=7, segment='as', log P(seg)=-11.3393, β[7]=0.0000, total=-11.3393\n",
      "  β[4] = -7.8529  (suffix: 'nas')\n",
      "      Candidate: j=6, segment='na', log P(seg)=-4.4046, β[6]=-3.5809, total=-7.9854\n",
      "      Candidate: j=5, segment='n', log P(seg)=-2.4625, β[5]=-7.7602, total=-10.2227\n",
      "      Candidate: j=7, segment='nas', log P(seg)=-11.3393, β[7]=0.0000, total=-11.3393\n",
      "  β[3] = -11.1420  (suffix: 'unas')\n",
      "      Candidate: j=7, segment='unas', log P(seg)=-11.3393, β[7]=0.0000, total=-11.3393\n",
      "      Candidate: j=4, segment='u', log P(seg)=-5.3345, β[4]=-7.8529, total=-13.1875\n",
      "      Candidate: j=5, segment='un', log P(seg)=-6.9971, β[5]=-7.7602, total=-14.7573\n",
      "  β[2] = -8.8708  (suffix: 'kunas')\n",
      "      Candidate: j=6, segment='kuna', log P(seg)=-5.4632, β[6]=-3.5809, total=-9.0440\n",
      "      Candidate: j=7, segment='kunas', log P(seg)=-11.3393, β[7]=0.0000, total=-11.3393\n",
      "      Candidate: j=4, segment='ku', log P(seg)=-3.6236, β[4]=-7.8529, total=-11.4765\n",
      "  β[1] = -11.2681  (suffix: 'ikunas')\n",
      "      Candidate: j=7, segment='ikunas', log P(seg)=-11.3393, β[7]=0.0000, total=-11.3393\n",
      "      Candidate: j=2, segment='i', log P(seg)=-5.8985, β[2]=-8.8708, total=-14.7693\n",
      "      Candidate: j=6, segment='ikuna', log P(seg)=-11.3393, β[6]=-3.5809, total=-14.9202\n",
      "  β[0] = -11.0723  (suffix: 'pikunas')\n",
      "      Candidate: j=7, segment='pikunas', log P(seg)=-11.3393, β[7]=0.0000, total=-11.3393\n",
      "      Candidate: j=2, segment='pi', log P(seg)=-3.7602, β[2]=-8.8708, total=-12.6310\n",
      "      Candidate: j=6, segment='pikuna', log P(seg)=-11.3393, β[6]=-3.5809, total=-14.9202\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "BOUNDARY PRIOR COMPUTATION\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "  Total log probability: α[7] = -11.0723\n",
      "  Total probability: P(word) = 0.000016\n",
      "\n",
      "  Computing P(boundary at i | word) = exp(α[i] + β[i] - α[n])\n",
      "\n",
      "  Position-by-position boundary probabilities:\n",
      "    Position 1 (after 'p'):\n",
      "      α[1] + β[1] = -6.0628 + -11.2681 = -17.3309\n",
      "      Normalized: -17.3309 - -11.0723 = -6.2586\n",
      "      P(boundary) = exp(-6.2586) = 0.0019\n",
      "    Position 2 (after 'i'):\n",
      "      α[2] + β[2] = -3.7599 + -8.8708 = -12.6307\n",
      "      Normalized: -12.6307 - -11.0723 = -1.5584\n",
      "      P(boundary) = exp(-1.5584) = 0.2105\n",
      "    Position 3 (after 'k'):\n",
      "      α[3] + β[3] = -10.5241 + -11.1420 = -21.6660\n",
      "      Normalized: -21.6660 - -11.0723 = -10.5937\n",
      "      P(boundary) = exp(-10.5937) = 0.0000\n",
      "    Position 4 (after 'u'):\n",
      "      α[4] + β[4] = -7.3637 + -7.8529 = -15.2166\n",
      "      Normalized: -15.2166 - -11.0723 = -4.1443\n",
      "      P(boundary) = exp(-4.1443) = 0.0159\n",
      "    Position 5 (after 'n'):\n",
      "      α[5] + β[5] = -9.5694 + -7.7602 = -17.3295\n",
      "      Normalized: -17.3295 - -11.0723 = -6.2572\n",
      "      P(boundary) = exp(-6.2572) = 0.0019\n",
      "    Position 6 (after 'a'):\n",
      "      α[6] + β[6] = -9.0327 + -3.5809 = -12.6135\n",
      "      Normalized: -12.6135 - -11.0723 = -1.5412\n",
      "      P(boundary) = exp(-1.5412) = 0.2141\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "FINAL OUTPUT\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "Word: 'pikunas'\n",
      "\n",
      "Character sequence: p i k u n a s\n",
      "Boundary probabilities:\n",
      "  0.002 0.210 0.000 0.016 0.002 0.214\n",
      "\n",
      "Visualization:\n",
      "  p i k u n a s\n",
      "             \n",
      "  0.00 0.21 0.00 0.02 0.00 0.21\n",
      "\n",
      "✅ CORRECT SEGMENTATION: 'pikunas'\n",
      "   Predicted: pi-kuna-s\n",
      "   Gold:      pi-kuna-s\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "📊 Summary: Found 3 correct segmentation(s) out of 5 words checked.\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# DEMONSTRATION: HMM Prior Processing with Actual Model\n",
    "# ===================================================================\n",
    "import torch\n",
    "import math\n",
    "\n",
    "# Add verbose method to SuffixHMMPrior class for demonstration\n",
    "def get_boundary_priors_verbose(self, word):\n",
    "    \"\"\"\n",
    "    Calculate boundary priors with detailed verbose output showing\n",
    "    forward pass, backward pass, and intermediate calculations.\n",
    "    \"\"\"\n",
    "    n = len(word)\n",
    "    if n <= 1:\n",
    "        return []\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"HMM PRIOR PROCESSING: '{word}' (length={n})\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Show known suffixes (sample of them)\n",
    "    print(f\"\\nHMM Prior Configuration:\")\n",
    "    print(f\"  Max suffix length: {self.max_len}\")\n",
    "    print(f\"  Unknown penalty: {self.unk_penalty:.4f}\")\n",
    "    print(f\"  Number of known suffixes: {len(self.log_probs)}\")\n",
    "    if len(self.log_probs) > 0:\n",
    "        sample_suffixes = list(self.log_probs.items())[:10]\n",
    "        print(f\"  Sample suffixes (first 10):\")\n",
    "        for suffix, log_prob in sample_suffixes:\n",
    "            print(f\"    '{suffix}': log P = {log_prob:.4f} (P = {math.exp(log_prob):.6f})\")\n",
    "        if len(self.log_probs) > 10:\n",
    "            print(f\"    ... and {len(self.log_probs) - 10} more\")\n",
    "    \n",
    "    # Forward pass\n",
    "    print(f\"\\n{'─'*70}\")\n",
    "    print(\"FORWARD PASS (α) - Computing log probabilities of all prefixes\")\n",
    "    print(f\"{'─'*70}\")\n",
    "    alpha = self._forward_pass(word)\n",
    "    \n",
    "    for i in range(n + 1):\n",
    "        prefix = word[:i] if i > 0 else \"<empty>\"\n",
    "        if i == 0:\n",
    "            print(f\"  α[{i}] = {alpha[i]:.4f}  (empty prefix, base case)\")\n",
    "        else:\n",
    "            # Show which segments were considered\n",
    "            candidates = []\n",
    "            for j in range(max(0, i - self.max_len), i):\n",
    "                segment = word[j:i]\n",
    "                log_p_seg = self._get_log_prob(segment)\n",
    "                candidates.append((j, segment, log_p_seg, alpha[j]))\n",
    "            \n",
    "            # Show top 3 candidates\n",
    "            candidates_sorted = sorted(candidates, key=lambda x: x[3] + x[2], reverse=True)[:3]\n",
    "            print(f\"  α[{i}] = {alpha[i]:.4f}  (prefix: '{prefix}')\")\n",
    "            for j, seg, log_p, prev_alpha in candidates_sorted:\n",
    "                total = prev_alpha + log_p\n",
    "                print(f\"      Candidate: j={j}, segment='{seg}', log P(seg)={log_p:.4f}, \"\n",
    "                      f\"α[{j}]={prev_alpha:.4f}, total={total:.4f}\")\n",
    "    \n",
    "    # Backward pass\n",
    "    print(f\"\\n{'─'*70}\")\n",
    "    print(\"BACKWARD PASS (β) - Computing log probabilities of all suffixes\")\n",
    "    print(f\"{'─'*70}\")\n",
    "    beta = self._backward_pass(word)\n",
    "    \n",
    "    for i in range(n, -1, -1):\n",
    "        suffix = word[i:] if i < n else \"<empty>\"\n",
    "        if i == n:\n",
    "            print(f\"  β[{i}] = {beta[i]:.4f}  (empty suffix, base case)\")\n",
    "        else:\n",
    "            # Show which segments were considered\n",
    "            candidates = []\n",
    "            for j in range(i + 1, min(n + 1, i + self.max_len + 1)):\n",
    "                segment = word[i:j]\n",
    "                log_p_seg = self._get_log_prob(segment)\n",
    "                candidates.append((j, segment, log_p_seg, beta[j]))\n",
    "            \n",
    "            # Show top 3 candidates\n",
    "            candidates_sorted = sorted(candidates, key=lambda x: x[3] + x[2], reverse=True)[:3]\n",
    "            print(f\"  β[{i}] = {beta[i]:.4f}  (suffix: '{suffix}')\")\n",
    "            for j, seg, log_p, next_beta in candidates_sorted:\n",
    "                total = next_beta + log_p\n",
    "                print(f\"      Candidate: j={j}, segment='{seg}', log P(seg)={log_p:.4f}, \"\n",
    "                      f\"β[{j}]={next_beta:.4f}, total={total:.4f}\")\n",
    "    \n",
    "    # Boundary prior computation\n",
    "    print(f\"\\n{'─'*70}\")\n",
    "    print(\"BOUNDARY PRIOR COMPUTATION\")\n",
    "    print(f\"{'─'*70}\")\n",
    "    log_total_prob = alpha[n]\n",
    "    print(f\"  Total log probability: α[{n}] = {log_total_prob:.4f}\")\n",
    "    print(f\"  Total probability: P(word) = {math.exp(log_total_prob):.6f}\")\n",
    "    \n",
    "    if log_total_prob == self.LOG_ZERO:\n",
    "        print(\"  WARNING: No valid segmentation found!\")\n",
    "        return [0.0] * (n - 1)\n",
    "    \n",
    "    print(f\"\\n  Computing P(boundary at i | word) = exp(α[i] + β[i] - α[n])\")\n",
    "    print(f\"\\n  Position-by-position boundary probabilities:\")\n",
    "    \n",
    "    log_priors = []\n",
    "    priors = []\n",
    "    for i in range(1, n):\n",
    "        log_p_boundary = alpha[i] + beta[i]\n",
    "        normalized_log_prior = log_p_boundary - log_total_prob\n",
    "        prior = math.exp(normalized_log_prior)\n",
    "        log_priors.append(log_p_boundary)\n",
    "        priors.append(prior)\n",
    "        \n",
    "        char_before = word[i-1]\n",
    "        char_after = word[i] if i < n else \"\"\n",
    "        print(f\"    Position {i} (after '{char_before}'):\")\n",
    "        print(f\"      α[{i}] + β[{i}] = {alpha[i]:.4f} + {beta[i]:.4f} = {log_p_boundary:.4f}\")\n",
    "        print(f\"      Normalized: {log_p_boundary:.4f} - {log_total_prob:.4f} = {normalized_log_prior:.4f}\")\n",
    "        print(f\"      P(boundary) = exp({normalized_log_prior:.4f}) = {prior:.4f}\")\n",
    "    \n",
    "    # Final summary\n",
    "    print(f\"\\n{'─'*70}\")\n",
    "    print(\"FINAL OUTPUT\")\n",
    "    print(f\"{'─'*70}\")\n",
    "    print(f\"Word: '{word}'\")\n",
    "    print(f\"\\nCharacter sequence: {' '.join(word)}\")\n",
    "    print(f\"Boundary probabilities:\")\n",
    "    print(f\"  {' '.join([f'{p:.3f}' for p in priors])}\")\n",
    "    print(f\"\\nVisualization:\")\n",
    "    print(f\"  {' '.join(word)}\")\n",
    "    print(f\"  {' '.join([' ' if p < 0.3 else '|' if p < 0.7 else '||' for p in priors])}\")\n",
    "    print(f\"  {' '.join([f'{p:.2f}' for p in priors])}\")\n",
    "    \n",
    "    return priors\n",
    "\n",
    "# Monkey-patch the verbose method to SuffixHMMPrior\n",
    "SuffixHMMPrior.get_boundary_priors_verbose = get_boundary_priors_verbose\n",
    "\n",
    "# Try to use existing model if available, otherwise show instructions\n",
    "try:\n",
    "    # Check if 'out' exists in namespace\n",
    "    if 'out' in globals() and 'hmm_prior' in out:\n",
    "        hmm_prior = out['hmm_prior']\n",
    "        print(\"✅ Using existing HMM prior from loaded model\")\n",
    "        \n",
    "        # Process words from dataset - only output for correct segmentations\n",
    "        # Try words from acc_df first, then gold_df if needed\n",
    "        max_words_to_show = 3  # Maximum number of correct segmentations to display\n",
    "        correct_count = 0\n",
    "        total_count = 0\n",
    "        \n",
    "        # Combine words from both dataframes (acc_df first, then gold_df)\n",
    "        words_to_try = []\n",
    "        if 'acc_df' in globals() and len(acc_df) > 0:\n",
    "            words_to_try.extend(acc_df['Word'].tolist())\n",
    "        if 'gold_df' in globals() and len(gold_df) > 0:\n",
    "            # Add words from gold_df that aren't already in the list\n",
    "            gold_words = gold_df['Word'].tolist()\n",
    "            words_to_try.extend([w for w in gold_words if w not in words_to_try])\n",
    "        \n",
    "        if len(words_to_try) == 0:\n",
    "            print(\"⚠️  No words found in acc_df or gold_df\")\n",
    "        else:\n",
    "            print(f\"🔍 Searching through {len(words_to_try)} words for correct segmentations...\")\n",
    "            \n",
    "            for word in words_to_try:\n",
    "                try:\n",
    "                    total_count += 1\n",
    "                    \n",
    "                    # Get predicted segmentation\n",
    "                    tokens = tokenize_with_vocab(word, vocab, max_token_len=4)\n",
    "                    seg_string, probs = segment_tokens(model, vocab, tokens, hmm_prior=hmm_prior, thr=thr)\n",
    "                    predicted_morphs = seg_string.split('-')\n",
    "                    \n",
    "                    # Normalize predicted morphs to lowercase\n",
    "                    pred_normalized = [m.lower().strip() for m in predicted_morphs if m.strip()]\n",
    "                    \n",
    "                    # Get gold segmentation from acc_df (test data)\n",
    "                    gold_row = acc_df[acc_df['Word'] == word] if 'acc_df' in globals() else pd.DataFrame()\n",
    "                    if len(gold_row) == 0:\n",
    "                        # Try gold_df as fallback\n",
    "                        gold_row = gold_df[gold_df['Word'] == word] if 'gold_df' in globals() else pd.DataFrame()\n",
    "                        if len(gold_row) == 0:\n",
    "                            continue  # Skip silently if word not found\n",
    "                        # Use Morph_split from gold_df\n",
    "                        gold_morphs = gold_row['Morph_split'].iloc[0]\n",
    "                        if not isinstance(gold_morphs, list):\n",
    "                            gold_morphs = list(gold_morphs) if hasattr(gold_morphs, '__iter__') else [str(gold_morphs)]\n",
    "                        gold_variants = [gold_morphs]\n",
    "                    else:\n",
    "                        # Use Gold column from acc_df (list of variants)\n",
    "                        gold_variants_raw = gold_row['Gold'].iloc[0]\n",
    "                        # Normalize gold_variants (handle numpy arrays, nested structures)\n",
    "                        gold_variants = normalize_gold_variants(gold_variants_raw)\n",
    "                        if not isinstance(gold_variants, list) or len(gold_variants) == 0:\n",
    "                            continue  # Skip silently if no valid gold variants\n",
    "                    \n",
    "                    # Check if prediction matches any gold variant exactly\n",
    "                    is_correct = False\n",
    "                    matched_gold = None\n",
    "                    for gold_variant in gold_variants:\n",
    "                        if not isinstance(gold_variant, list):\n",
    "                            gold_variant = list(gold_variant) if hasattr(gold_variant, '__iter__') else [str(gold_variant)]\n",
    "                        gold_normalized = [m.lower().strip() for m in gold_variant if m.strip()]\n",
    "                        if pred_normalized == gold_normalized:\n",
    "                            is_correct = True\n",
    "                            matched_gold = gold_variant\n",
    "                            break\n",
    "                    \n",
    "                    if is_correct:\n",
    "                        correct_count += 1\n",
    "                        # Only output verbose information for correct segmentations\n",
    "                        priors = hmm_prior.get_boundary_priors_verbose(word)\n",
    "                        print(f\"\\n✅ CORRECT SEGMENTATION: '{word}'\")\n",
    "                        print(f\"   Predicted: {seg_string}\")\n",
    "                        print(f\"   Gold:      {'-'.join(matched_gold)}\")\n",
    "                        print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "                        \n",
    "                        # Stop after finding max_words_to_show correct segmentations\n",
    "                        if correct_count >= max_words_to_show:\n",
    "                            break\n",
    "                    # Silently skip incorrect segmentations\n",
    "                            \n",
    "                except Exception as e:\n",
    "                    # Silently skip errors, continue to next word\n",
    "                    continue\n",
    "            \n",
    "            print(f\"\\n📊 Summary: Found {correct_count} correct segmentation(s) out of {total_count} words checked.\")\n",
    "            if correct_count == 0:\n",
    "                print(\"   No correct segmentations found. Try checking more words or adjusting the threshold.\")\n",
    "    else:\n",
    "        print(\"⚠️  No model found in memory.\")\n",
    "        print(\"\\nTo use this demonstration:\")\n",
    "        print(\"1. First run your model training/loading cell (e.g., run_segmentation_with_privK)\")\n",
    "        print(\"2. Then run this cell again\")\n",
    "        print(\"\\nAlternatively, you can manually specify:\")\n",
    "        print(\"  hmm_prior = out['hmm_prior']\")\n",
    "        print(\"  priors = hmm_prior.get_boundary_priors_verbose('pikunas')\")\n",
    "except NameError as e:\n",
    "    print(f\"❌ {e}\")\n",
    "    print(\"\\nTo use this demonstration:\")\n",
    "    print(\"1. First run your model training/loading cell (e.g., run_segmentation_with_privK)\")\n",
    "    print(\"2. Then run this cell again\")\n",
    "    print(\"\\nAlternatively, you can manually specify:\")\n",
    "    print(\"  hmm_prior = out['hmm_prior']\")\n",
    "    print(\"  priors = hmm_prior.get_boundary_priors_verbose('pikunas')\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8b5d05f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(samples, min_freq=1):\n",
    "    from collections import Counter\n",
    "    ctr = Counter()\n",
    "    for s in samples: ctr.update(s[\"tokens\"])\n",
    "    vocab = {\"<PAD>\":0, \"<UNK>\":1}\n",
    "    for t,c in sorted(ctr.items(), key=lambda x: (-x[1], x[0])):\n",
    "        if c>=min_freq and t not in vocab:\n",
    "            vocab[t] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "class SegDataset(Dataset):\n",
    "    # --- MODIFIED to take hmm_prior instead of dt_clf/dt_vec ---\n",
    "    def __init__(self, samples, vocab, hmm_prior=None, feat_dim=0):\n",
    "        self.samples = samples\n",
    "        self.vocab = vocab\n",
    "        self.hmm_prior = hmm_prior # Changed from dt_clf, dt_vec\n",
    "        self.feat_dim = feat_dim\n",
    "\n",
    "    def __len__(self): return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        s = self.samples[idx]\n",
    "        tokens = s[\"tokens\"]\n",
    "        ids = [self.vocab.get(t, self.vocab[\"<UNK>\"]) for t in tokens]\n",
    "        y = s[\"y\"]\n",
    "        # --- THIS LINE IS THE KEY CHANGE ---\n",
    "        prior = prior_probs_for_sample(self.hmm_prior, tokens)\n",
    "        priv = s[\"priv\"] if self.feat_dim>0 else []\n",
    "        return {\"ids\": ids, \"y\": y, \"prior\": prior, \"priv\": priv, \"tokens\": tokens}\n",
    "\n",
    "def collate(batch):\n",
    "    maxT = max(len(b[\"ids\"]) for b in batch)\n",
    "    maxB = maxT-1\n",
    "    B = len(batch)\n",
    "\n",
    "    ids = torch.full((B, maxT), 0, dtype=torch.long)\n",
    "    mask_tok = torch.zeros((B, maxT), dtype=torch.bool)\n",
    "    y = torch.full((B, maxB), -100, dtype=torch.long)\n",
    "    prior = torch.zeros((B, maxB), dtype=torch.float32)\n",
    "    mask_b = torch.zeros((B, maxB), dtype=torch.bool)\n",
    "\n",
    "    feat_dim = len(batch[0][\"priv\"]) if isinstance(batch[0][\"priv\"], list) else 0\n",
    "    priv = torch.zeros((B, feat_dim), dtype=torch.float32) if feat_dim>0 else None\n",
    "\n",
    "    for i, b in enumerate(batch):\n",
    "        T = len(b[\"ids\"])\n",
    "        ids[i,:T] = torch.tensor(b[\"ids\"], dtype=torch.long)\n",
    "        mask_tok[i,:T] = True\n",
    "        if T>1:\n",
    "            L = T-1\n",
    "            y[i,:L] = torch.tensor(b[\"y\"], dtype=torch.long)\n",
    "            p = b[\"prior\"] if len(b[\"prior\"])==L else [0.5]*L\n",
    "            prior[i,:L] = torch.tensor(p, dtype=torch.float32)\n",
    "            mask_b[i,:L] = True\n",
    "        if feat_dim>0:\n",
    "            priv[i] = torch.tensor(b[\"priv\"], dtype=torch.float32)\n",
    "\n",
    "    return {\n",
    "        \"ids\": ids, \"mask_tok\": mask_tok,\n",
    "        \"y\": y, \"prior\": prior, \"mask_b\": mask_b,\n",
    "        \"priv\": priv  # (B, F) or None\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "79f1c9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=16, hidden_size=64, num_layers=2,\n",
    "                 use_prior=True, dropout=0.1, freeze_emb=False, fuse_mode=\"logit_add\"):\n",
    "        super().__init__()\n",
    "        self.use_prior = use_prior\n",
    "        self.fuse_mode = fuse_mode\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        if freeze_emb:\n",
    "            for p in self.emb.parameters(): p.requires_grad = False\n",
    "        lstm_dropout = dropout if num_layers > 1 else 0.0\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=emb_dim, hidden_size=hidden_size//2,\n",
    "            num_layers=num_layers, dropout=lstm_dropout,\n",
    "            bidirectional=True, batch_first=True\n",
    "        )\n",
    "        in_mlp = hidden_size + (1 if (use_prior and fuse_mode==\"concat\") else 0)\n",
    "        self.boundary_mlp = nn.Sequential(\n",
    "            nn.Linear(in_mlp, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size // 2, 2)\n",
    "        )\n",
    "        if use_prior and fuse_mode == \"logit_add\":\n",
    "            self.alpha = nn.Parameter(torch.tensor(1.0))\n",
    "\n",
    "    def forward(self, ids, prior, mask_tok):\n",
    "        emb = self.emb(ids)\n",
    "        h, _ = self.lstm(emb)          # (B,T,H)\n",
    "        left = h[:, :-1, :]            # (B,T-1,H)\n",
    "        if self.use_prior and self.fuse_mode == \"concat\":\n",
    "            feat = torch.cat([left, prior.unsqueeze(-1)], dim=-1)\n",
    "            return self.boundary_mlp(feat)\n",
    "        logits = self.boundary_mlp(left)\n",
    "        if self.use_prior and self.fuse_mode == \"logit_add\":\n",
    "            eps = 1e-6\n",
    "            p = prior.clamp(eps, 1-eps)\n",
    "            prior_logit = torch.log(p) - torch.log(1-p)\n",
    "            logits[..., 1] = logits[..., 1] + self.alpha * prior_logit\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0619d1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boundary_metrics_from_lists(probs_list, gold_list, thr=0.5):\n",
    "    if not probs_list: return 0.0,0.0,0.0\n",
    "    p = torch.cat([t for t in probs_list if t.numel()>0], dim=0).numpy()\n",
    "    g = torch.cat([t for t in gold_list if t.numel()>0], dim=0).numpy()\n",
    "    pred = (p >= thr).astype(int)\n",
    "    P,R,F1,_ = precision_recall_fscore_support(g, pred, average='binary', zero_division=0)\n",
    "    return P,R,F1\n",
    "\n",
    "def exact_match_rate_from_lists(probs_list, gold_list, thr=0.5):\n",
    "    if not probs_list: return 0.0\n",
    "    em=[]\n",
    "    for p,g in zip(probs_list, gold_list):\n",
    "        if g.numel()==0: em.append(1.0)\n",
    "        else:\n",
    "            pred = (p.numpy() >= thr).astype(int)\n",
    "            em.append(float(np.array_equal(pred, g.numpy())))\n",
    "    return float(np.mean(em))\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict(model, loader):\n",
    "    model.eval()\n",
    "    probs_list, gold_list = [], []\n",
    "    for batch in loader:\n",
    "        logits = model(batch[\"ids\"], batch[\"prior\"], batch[\"mask_tok\"])\n",
    "        probs = torch.softmax(logits, dim=-1)[..., 1]      # (B,T-1)\n",
    "        y = batch[\"y\"]; mask = batch[\"mask_b\"]\n",
    "        B = probs.shape[0]\n",
    "        for b in range(B):\n",
    "            L = int(mask[b].sum().item())\n",
    "            if L==0:\n",
    "                probs_list.append(torch.empty(0))\n",
    "                gold_list.append(torch.empty(0, dtype=torch.long))\n",
    "            else:\n",
    "                probs_list.append(probs[b,:L].cpu())\n",
    "                gold_list.append(y[b,:L].cpu())\n",
    "    return probs_list, gold_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e2aa916e",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_ce  = nn.CrossEntropyLoss()\n",
    "criterion_bce = nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
    "mse = nn.MSELoss(reduction=\"mean\")\n",
    "\n",
    "def train_epoch(model, loader, opt, lambda_prior=0.1, lambda_k=0.1, k_reg=None):\n",
    "    model.train()\n",
    "    tot=0; n=0\n",
    "    for batch in loader:\n",
    "        ids, prior, y, mask_b = batch[\"ids\"], batch[\"prior\"], batch[\"y\"], batch[\"mask_b\"]\n",
    "        priv = batch[\"priv\"]  # (B,F) or None\n",
    "\n",
    "        logits = model(ids, prior, batch[\"mask_tok\"])    # (B,T-1,2)\n",
    "        logits_flat = logits[mask_b]                     # (N,2)\n",
    "        y_true = y[mask_b]                               # (N,)\n",
    "\n",
    "        # (1) CE on gold boundaries\n",
    "        loss = criterion_ce(logits_flat, y_true)\n",
    "\n",
    "        # (2) Optional: distill toward DT prior on cut-logit\n",
    "        if lambda_prior > 0:\n",
    "            cut_logit = logits[..., 1]                   # (B,T-1)\n",
    "            prior_flat = prior[mask_b]                   # (N,)\n",
    "            loss_pr = criterion_bce(cut_logit[mask_b], prior_flat)\n",
    "            loss = loss + lambda_prior * loss_pr\n",
    "\n",
    "        # (3) K-regularizer using privileged K-hat\n",
    "        if (lambda_k > 0) and (k_reg is not None) and (priv is not None):\n",
    "            with torch.no_grad():\n",
    "                k_hat = predict_k_hat_priv(k_reg, priv)  # (B,)\n",
    "            # expected number of cuts from model = sum(sigmoid(cut_logit))\n",
    "            cut_logit = logits[..., 1]                   # (B,T-1)\n",
    "            p_cut = torch.sigmoid(cut_logit)             # (B,T-1)\n",
    "            exp_K = p_cut.sum(dim=1)                     # (B,)\n",
    "            loss_k = mse(exp_K, k_hat)\n",
    "            loss = loss + lambda_k * loss_k\n",
    "\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "        tot += loss.item(); n += 1\n",
    "    return tot/max(n,1)\n",
    "\n",
    "def split_train_test(samples, test_ratio=0.2):\n",
    "    n = len(samples); idx = np.arange(n); np.random.shuffle(idx)\n",
    "    cut = int(n*(1-test_ratio))\n",
    "    tr = [samples[i] for i in idx[:cut]]\n",
    "    te = [samples[i] for i in idx[cut:]]\n",
    "    return tr, te\n",
    "\n",
    "def best_threshold_for_exact(probs_list, gold_list, grid=None):\n",
    "    if grid is None: grid = np.linspace(0.3, 0.9, 61)\n",
    "    best_thr, best_em, best_f1 = 0.5, -1.0, 0.0\n",
    "    p_all = np.concatenate([t.numpy() for t in probs_list if t.numel()>0], axis=0)\n",
    "    g_all = np.concatenate([t.numpy() for t in gold_list  if t.numel()>0], axis=0)\n",
    "    for thr in grid:\n",
    "        ems=[]\n",
    "        for p,g in zip(probs_list, gold_list):\n",
    "            if g.numel()==0: ems.append(1.0); continue\n",
    "            ems.append(float(np.array_equal((p.numpy()>=thr).astype(int), g.numpy())))\n",
    "        em = float(np.mean(ems))\n",
    "        pred_all = (p_all>=thr).astype(int)\n",
    "        P,R,F1,_ = precision_recall_fscore_support(g_all, pred_all, average='binary', zero_division=0)\n",
    "        if em>best_em or (np.isclose(em,best_em) and F1>best_f1):\n",
    "            best_thr, best_em, best_f1 = thr, em, F1\n",
    "    print(f\"[Exact-opt threshold] thr={best_thr:.3f} | exact={best_em:.3f} | boundaryF1={best_f1:.3f}\")\n",
    "    return best_thr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "12cebb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# MODEL SAVING AND LOADING FUNCTIONS\n",
    "# =========================\n",
    "# These functions handle saving and loading trained models to avoid retraining\n",
    "# Models are saved to the models folder with a unique identifier based on parameters\n",
    "\n",
    "def generate_model_id(df, provided_suffix_list, use_suffix_list, unk_penalty, epochs,\n",
    "                     use_prior, fuse_mode, lambda_prior, lambda_k, batch_size, hparams, synthetic_choice,\n",
    "                     augmentation_word_selection=None, augmentation_n_words=None):\n",
    "    \"\"\"\n",
    "    Generate a unique identifier for a model based on its training parameters.\n",
    "    This ensures that models with the same parameters can be reused.\n",
    "    \n",
    "    Args:\n",
    "        All training parameters that affect the model\n",
    "        augmentation_word_selection: How words were selected for augmentation (\"all\", \"first\", \"random\")\n",
    "        augmentation_n_words: Number of words used when selection is \"first\" or \"random\"\n",
    "    \n",
    "    Returns:\n",
    "        A string identifier (hash) for the model\n",
    "    \"\"\"\n",
    "    # Get word selection parameters from globals if not provided\n",
    "    if augmentation_word_selection is None:\n",
    "        augmentation_word_selection = globals().get('AUGMENTATION_WORD_SELECTION', 'all')\n",
    "    if augmentation_n_words is None:\n",
    "        augmentation_n_words = globals().get('AUGMENTATION_N_WORDS', None)\n",
    "    \n",
    "    # Create a dictionary of all parameters\n",
    "    params_dict = {\n",
    "        'synthetic_choice': synthetic_choice,\n",
    "        'use_suffix_list': use_suffix_list,\n",
    "        'unk_penalty': unk_penalty,\n",
    "        'epochs': epochs,\n",
    "        'use_prior': use_prior,\n",
    "        'fuse_mode': fuse_mode,\n",
    "        'lambda_prior': lambda_prior,\n",
    "        'lambda_k': lambda_k,\n",
    "        'batch_size': batch_size,\n",
    "        'hparams': hparams,\n",
    "        'suffix_list_len': len(provided_suffix_list) if provided_suffix_list else 0,\n",
    "        'df_shape': df.shape if df is not None else (0, 0),\n",
    "        'augmentation_word_selection': augmentation_word_selection,\n",
    "        'augmentation_n_words': augmentation_n_words\n",
    "    }\n",
    "    \n",
    "    # Convert to JSON string and hash it\n",
    "    params_str = json.dumps(params_dict, sort_keys=True)\n",
    "    model_id = hashlib.md5(params_str.encode()).hexdigest()[:16]\n",
    "    return model_id\n",
    "\n",
    "def save_model(model, vocab, out, model_id, models_folder=MODELS_FOLDER, \n",
    "               synthetic_choice=None, augmentation_word_selection=None, augmentation_n_words=None):\n",
    "    \"\"\"\n",
    "    Save a trained model and its associated artifacts.\n",
    "    \n",
    "    Args:\n",
    "        model: The trained PyTorch model\n",
    "        vocab: Vocabulary dictionary\n",
    "        out: Dictionary containing hmm_prior, k_teacher, best_thr, etc.\n",
    "        model_id: Unique identifier for this model\n",
    "        models_folder: Folder to save models in\n",
    "        synthetic_choice: Which synthetic data was used\n",
    "        augmentation_word_selection: How words were selected for augmentation\n",
    "        augmentation_n_words: Number of words used for augmentation\n",
    "    \"\"\"\n",
    "    model_dir = os.path.join(models_folder, model_id)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    # Save model state\n",
    "    model_path = os.path.join(model_dir, \"model.pt\")\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    \n",
    "    # Save vocabulary\n",
    "    vocab_path = os.path.join(model_dir, \"vocab.pkl\")\n",
    "    with open(vocab_path, \"wb\") as f:\n",
    "        pickle.dump(vocab, f)\n",
    "    \n",
    "    # Save other artifacts (hmm_prior, k_teacher, best_thr, etc.)\n",
    "    artifacts_path = os.path.join(model_dir, \"artifacts.pkl\")\n",
    "    with open(artifacts_path, \"wb\") as f:\n",
    "        pickle.dump(out, f)\n",
    "    \n",
    "    # Get configuration values from globals if not provided\n",
    "    if synthetic_choice is None:\n",
    "        synthetic_choice = globals().get('SYNTHETIC_DATA_CHOICE', 'none')\n",
    "    if augmentation_word_selection is None:\n",
    "        augmentation_word_selection = globals().get('AUGMENTATION_WORD_SELECTION', 'all')\n",
    "    if augmentation_n_words is None:\n",
    "        augmentation_n_words = globals().get('AUGMENTATION_N_WORDS', None)\n",
    "    \n",
    "    # Save metadata (parameters used)\n",
    "    metadata_path = os.path.join(model_dir, \"metadata.json\")\n",
    "    with open(metadata_path, \"w\") as f:\n",
    "        metadata = {\n",
    "            'model_id': model_id,\n",
    "            'vocab_size': len(vocab),\n",
    "            'synthetic_choice': synthetic_choice,\n",
    "            'augmentation_word_selection': augmentation_word_selection,\n",
    "        }\n",
    "        if augmentation_n_words is not None:\n",
    "            metadata['augmentation_n_words'] = augmentation_n_words\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"Model saved to {model_dir}\")\n",
    "    return model_dir\n",
    "\n",
    "def load_model(model_id, models_folder=MODELS_FOLDER, vocab_size=None):\n",
    "    \"\"\"\n",
    "    Load a trained model and its associated artifacts.\n",
    "    \n",
    "    Args:\n",
    "        model_id: Unique identifier for the model\n",
    "        models_folder: Folder where models are saved\n",
    "        vocab_size: Vocabulary size (needed to reconstruct model architecture)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with 'vocab', 'out', 'model_state_path', 'model_dir' or None if not found\n",
    "    \"\"\"\n",
    "    model_dir = os.path.join(models_folder, model_id)\n",
    "    \n",
    "    if not os.path.exists(model_dir):\n",
    "        return None\n",
    "    \n",
    "    # Load vocabulary\n",
    "    vocab_path = os.path.join(model_dir, \"vocab.pkl\")\n",
    "    if not os.path.exists(vocab_path):\n",
    "        return None\n",
    "    \n",
    "    with open(vocab_path, \"rb\") as f:\n",
    "        vocab = pickle.load(f)\n",
    "    \n",
    "    # Load artifacts to get model architecture info\n",
    "    artifacts_path = os.path.join(model_dir, \"artifacts.pkl\")\n",
    "    if not os.path.exists(artifacts_path):\n",
    "        return None\n",
    "    \n",
    "    with open(artifacts_path, \"rb\") as f:\n",
    "        out = pickle.load(f)\n",
    "    \n",
    "    # Load model state\n",
    "    model_path = os.path.join(model_dir, \"model.pt\")\n",
    "    if not os.path.exists(model_path):\n",
    "        return None\n",
    "    \n",
    "    print(f\"Model artifacts loaded from {model_dir}\")\n",
    "    return {\n",
    "        'vocab': vocab,\n",
    "        'out': out,\n",
    "        'model_state_path': model_path,\n",
    "        'model_dir': model_dir\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "49fd85cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# MAIN TRAINING FUNCTION WITH MODEL CHECKPOINTING\n",
    "# ===================================================================\n",
    "# This function trains a morphology parser model. It checks if a model with\n",
    "# the same parameters already exists and loads it instead of retraining.\n",
    "\n",
    "def run_segmentation_with_privK(\n",
    "    df,\n",
    "    provided_suffix_list,\n",
    "    use_suffix_list=True,\n",
    "    unk_penalty=-15.0,\n",
    "    epochs=15,\n",
    "    use_prior=True, # This now controls the HMM prior\n",
    "    fuse_mode=\"logit_add\",\n",
    "    lambda_prior=0.1,\n",
    "    lambda_k=0.2,\n",
    "    batch_size=64,\n",
    "    hparams=None,\n",
    "    synthetic_choice=None  # Added to track which synthetic data was used\n",
    "):\n",
    "    \"\"\"\n",
    "    Train or load a morphology parser model.\n",
    "    \n",
    "    This function will:\n",
    "    1. Check if a model with the same parameters already exists\n",
    "    2. If found, load it and return it (skipping training)\n",
    "    3. If not found, train a new model and save it\n",
    "    \n",
    "    Args:\n",
    "        df: Training DataFrame\n",
    "        provided_suffix_list: List of valid suffixes for HMM prior\n",
    "        use_suffix_list: Whether to use the provided suffix list\n",
    "        unk_penalty: Penalty for unknown suffixes in HMM\n",
    "        epochs: Number of training epochs\n",
    "        use_prior: Whether to use HMM prior\n",
    "        fuse_mode: How to fuse prior with model predictions\n",
    "        lambda_prior: Weight for prior distillation loss\n",
    "        lambda_k: Weight for K-regularizer loss\n",
    "        batch_size: Training batch size\n",
    "        hparams: Model hyperparameters dictionary\n",
    "        synthetic_choice: Which synthetic data was used (\"none\", \"gpt4o\", \"gpt5mini\")\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (model, vocab, out_dict)\n",
    "    \"\"\"\n",
    "    if hparams is None:\n",
    "        hparams = dict(emb_dim=16, hidden_size=64, num_layers=2,\n",
    "                       dropout=0.25, lr=1e-3, weight_decay=1e-4, freeze_emb=False)\n",
    "    \n",
    "    if synthetic_choice is None:\n",
    "        synthetic_choice = SYNTHETIC_DATA_CHOICE if 'SYNTHETIC_DATA_CHOICE' in globals() else \"none\"\n",
    "    \n",
    "    # Get word selection parameters from globals\n",
    "    augmentation_word_selection = globals().get('AUGMENTATION_WORD_SELECTION', 'all')\n",
    "    augmentation_n_words = globals().get('AUGMENTATION_N_WORDS', None)\n",
    "    \n",
    "    # Generate model identifier based on parameters\n",
    "    model_id = generate_model_id(\n",
    "        df, provided_suffix_list, use_suffix_list, unk_penalty, epochs,\n",
    "        use_prior, fuse_mode, lambda_prior, lambda_k, batch_size, hparams, synthetic_choice,\n",
    "        augmentation_word_selection=augmentation_word_selection,\n",
    "        augmentation_n_words=augmentation_n_words\n",
    "    )\n",
    "    \n",
    "    # Try to load existing model\n",
    "    print(f\"Checking for existing model with ID: {model_id}\")\n",
    "    loaded = load_model(model_id, models_folder=MODELS_FOLDER)\n",
    "    \n",
    "    if loaded is not None:\n",
    "        print(f\"✅ Found existing model! Loading from {loaded['model_dir']}\")\n",
    "        # Reconstruct model architecture\n",
    "        vocab = loaded['vocab']\n",
    "        out = loaded['out']\n",
    "        model_state_path = loaded['model_state_path']\n",
    "        \n",
    "        model = BiLSTMTagger(\n",
    "            vocab_size=len(vocab),\n",
    "            emb_dim=hparams.get(\"emb_dim\", 16),\n",
    "            hidden_size=hparams.get(\"hidden_size\", 64),\n",
    "            num_layers=hparams.get(\"num_layers\", 2),\n",
    "            use_prior=(use_prior and fuse_mode!=\"none\"),\n",
    "            dropout=hparams.get(\"dropout\", 0.25),\n",
    "            freeze_emb=hparams.get(\"freeze_emb\", False),\n",
    "            fuse_mode=fuse_mode\n",
    "        )\n",
    "        \n",
    "        # Load model weights\n",
    "        model.load_state_dict(torch.load(model_state_path))\n",
    "        model.eval()\n",
    "        \n",
    "        print(\"Model loaded successfully. Skipping training.\")\n",
    "        return model, vocab, out\n",
    "    \n",
    "    # Model doesn't exist, need to train\n",
    "    print(f\"No existing model found. Training new model...\")\n",
    "    \n",
    "    # Rebuild samples to include gold morphemes for HMM training\n",
    "    samples = build_samples_with_priv(df, feat_names=NEW_NUM_FEATS)\n",
    "    train_s, test_s = split_train_test(samples, 0.2)\n",
    "\n",
    "    # --- HMM prior (token-window) trained on TRAIN ONLY ---\n",
    "    hmm_prior = None\n",
    "    if use_prior and use_suffix_list:\n",
    "        # THE KEY CHANGE IS HERE: Call the new function with your list\n",
    "        hmm_prior = create_hmm_prior_from_list(provided_suffix_list, unk_penalty)\n",
    "    \n",
    "    # --- HMM prior (token-window) trained on TRAIN ONLY ---\n",
    "    if use_prior and not use_suffix_list:\n",
    "        hmm_prior = train_hmm_prior(train_s)\n",
    "\n",
    "    # K-teacher (privileged) on TRAIN ONLY (this part remains the same)\n",
    "    feat_dim = len(NEW_NUM_FEATS)\n",
    "    k_reg = train_k_teacher_priv(train_s, feat_dim=feat_dim)\n",
    "\n",
    "    vocab = build_vocab(train_s, min_freq=1)\n",
    "\n",
    "    # --- datasets/loaders now use the HMM prior ---\n",
    "    train_ds = SegDataset(train_s, vocab, hmm_prior=hmm_prior, feat_dim=feat_dim)\n",
    "    test_ds  = SegDataset(test_s,  vocab, hmm_prior=hmm_prior, feat_dim=feat_dim)\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  collate_fn=collate)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, collate_fn=collate)\n",
    "\n",
    "    model = BiLSTMTagger(\n",
    "        vocab_size=len(vocab),\n",
    "        emb_dim=hparams.get(\"emb_dim\", 16),\n",
    "        hidden_size=hparams.get(\"hidden_size\", 64),\n",
    "        num_layers=hparams.get(\"num_layers\", 2),\n",
    "        use_prior=(use_prior and fuse_mode!=\"none\"),\n",
    "        dropout=hparams.get(\"dropout\", 0.25),\n",
    "        freeze_emb=hparams.get(\"freeze_emb\", False),\n",
    "        fuse_mode=fuse_mode\n",
    "    )\n",
    "\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=hparams.get(\"lr\", 1e-3), weight_decay=hparams.get(\"weight_decay\", 1e-4))\n",
    "\n",
    "    final_probs_list, final_gold_list = None, None\n",
    "    for ep in range(1, epochs+1):\n",
    "        loss = train_epoch(model, train_loader, opt, lambda_prior=lambda_prior, lambda_k=lambda_k, k_reg=k_reg)\n",
    "        probs_list, gold_list = predict(model, test_loader)\n",
    "        P,R,F1 = boundary_metrics_from_lists(probs_list, gold_list, thr=0.5)\n",
    "        EM = exact_match_rate_from_lists(probs_list, gold_list, thr=0.5)\n",
    "        print(f\"Epoch {ep:02d} | loss={loss:.4f} | boundary P/R/F1={P:.3f}/{R:.3f}/{F1:.3f} | exact={EM:.3f}\")\n",
    "        final_probs_list, final_gold_list = probs_list, gold_list\n",
    "\n",
    "    best_thr = best_threshold_for_exact(final_probs_list, final_gold_list)\n",
    "\n",
    "    out = {\n",
    "        \"probs_list\": final_probs_list,\n",
    "        \"gold_list\": final_gold_list,\n",
    "        # Return the hmm_prior instead of dt_clf/dt_vec\n",
    "        \"hmm_prior\": hmm_prior,\n",
    "        \"k_teacher\": k_reg,\n",
    "        \"best_thr\": best_thr\n",
    "    }\n",
    "    \n",
    "    # Save the trained model\n",
    "    print(f\"\\nSaving trained model with ID: {model_id}\")\n",
    "    save_model(model, vocab, out, model_id, models_folder=MODELS_FOLDER,\n",
    "               synthetic_choice=synthetic_choice,\n",
    "               augmentation_word_selection=augmentation_word_selection,\n",
    "               augmentation_n_words=augmentation_n_words)\n",
    "\n",
    "    return model, vocab, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3712be86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_with_vocab(word: str, vocab: dict, max_token_len: int = 4):\n",
    "    i, toks = 0, []\n",
    "    while i < len(word):\n",
    "        matched = None\n",
    "        Lmax = min(max_token_len, len(word)-i)\n",
    "        for L in range(Lmax, 0, -1):\n",
    "            seg = word[i:i+L]\n",
    "            if seg in vocab:\n",
    "                matched = seg; break\n",
    "        toks.append(matched if matched else word[i])\n",
    "        i += len(toks[-1])\n",
    "    return toks\n",
    "\n",
    "# And your prediction function needs to be updated slightly\n",
    "@torch.no_grad()\n",
    "def segment_tokens(model, vocab, tokens, hmm_prior=None, thr=0.5): # changed arguments\n",
    "    ids = torch.tensor([[vocab.get(t, vocab[\"<UNK>\"]) for t in tokens]], dtype=torch.long)\n",
    "    mask_tok = torch.ones_like(ids, dtype=torch.bool)\n",
    "    T = len(tokens)\n",
    "    if T<=1: return \"\".join(tokens), np.array([])\n",
    "    \n",
    "    # Use the new prior function\n",
    "    prior_list = prior_probs_for_sample(hmm_prior, tokens)\n",
    "    \n",
    "    prior = torch.tensor([prior_list], dtype=torch.float32)\n",
    "    logits = model(ids, prior, mask_tok)\n",
    "    probs = torch.softmax(logits, dim=-1)[0, :, 1].cpu().numpy()\n",
    "    cuts = (probs >= thr).astype(int)\n",
    "    out=[]\n",
    "    for i, tok in enumerate(tokens):\n",
    "        out.append(tok)\n",
    "        if i < T-1 and cuts[i]==1: out.append(\"-\")\n",
    "    return \"\".join(out), probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4ab4900a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 88 suffixes from data\\suffixesCQ-Anettte-Rios_LS.txt\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# LOAD SUFFIX LIST FOR HMM PRIOR\n",
    "# =========================\n",
    "# The suffix list is used to create the HMM prior that guides segmentation\n",
    "# This file should be in the data folder\n",
    "\n",
    "def read_suffixes(filename):\n",
    "    \"\"\"\n",
    "    Read a list of suffixes from a file.\n",
    "    Expected format: lines with \"number suffix\" (e.g., \"1 -ta\")\n",
    "    \"\"\"\n",
    "    suffixes = []\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            # Split into number and suffix part\n",
    "            parts = line.split(maxsplit=1)\n",
    "            if len(parts) == 2:\n",
    "                _, suffix = parts\n",
    "                suffixes.append(suffix[1:])  # Remove the leading dash\n",
    "    return suffixes\n",
    "\n",
    "# Load suffix list from data folder\n",
    "suffix_filename = os.path.join(DATA_FOLDER, \"suffixesCQ-Anettte-Rios_LS.txt\")\n",
    "if not os.path.exists(suffix_filename):\n",
    "    # Try in root directory as fallback\n",
    "    suffix_filename = \"suffixesCQ-Anettte-Rios_LS.txt\"\n",
    "    if not os.path.exists(suffix_filename):\n",
    "        print(f\"Warning: Suffix file not found at {os.path.join(DATA_FOLDER, 'suffixesCQ-Anettte-Rios_LS.txt')}\")\n",
    "        print(\"Please ensure the suffix file is in the data folder.\")\n",
    "        suffix_list = []\n",
    "    else:\n",
    "        suffix_list = read_suffixes(suffix_filename)\n",
    "        print(f\"Loaded {len(suffix_list)} suffixes from {suffix_filename}\")\n",
    "else:\n",
    "    suffix_list = read_suffixes(suffix_filename)\n",
    "    print(f\"Loaded {len(suffix_list)} suffixes from {suffix_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c3cf3667",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Set, Tuple\n",
    "\n",
    "# ---------- helpers to turn segs into boundary sets (char offsets) ----------\n",
    "def offsets_from_morphemes(morphs: List[str]) -> Set[int]:\n",
    "    # boundaries after each morph except the last\n",
    "    offs = []\n",
    "    s = 0\n",
    "    for i, m in enumerate(morphs):\n",
    "        s += len(m)\n",
    "        if i < len(morphs) - 1:\n",
    "            offs.append(s)\n",
    "    return set(offs)\n",
    "\n",
    "def offsets_from_tokens_and_mask(tokens: List[str], mask01: np.ndarray) -> Set[int]:\n",
    "    # boundaries after token i where mask01[i]==1, measured in character offsets\n",
    "    offs = set()\n",
    "    cum = 0\n",
    "    for i, t in enumerate(tokens):\n",
    "        cum += len(t)\n",
    "        if i < len(tokens) - 1 and mask01[i] == 1:\n",
    "            offs.add(cum)\n",
    "    return offs\n",
    "\n",
    "def f1_from_sets(pred: Set[int], gold: Set[int]) -> Tuple[float, float, float, int, int, int]:\n",
    "    tp = len(pred & gold)\n",
    "    fp = len(pred - gold)\n",
    "    fn = len(gold - pred)\n",
    "    P = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    R = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    F1 = 2 * P * R / (P + R) if (P + R) > 0 else 0.0\n",
    "    return P, R, F1, tp, fp, fn\n",
    "\n",
    "def normalize_gold_variants(gold_variants):\n",
    "    \"\"\"\n",
    "    Convert gold_variants to a list format, handling numpy arrays and nested structures.\n",
    "    \"\"\"\n",
    "    if gold_variants is None:\n",
    "        return []\n",
    "    \n",
    "    # If it's a numpy array, convert to list\n",
    "    if isinstance(gold_variants, np.ndarray):\n",
    "        gold_variants = gold_variants.tolist()\n",
    "    \n",
    "    # If it's already a list, ensure nested elements are also lists (not numpy arrays)\n",
    "    if isinstance(gold_variants, list):\n",
    "        normalized = []\n",
    "        for variant in gold_variants:\n",
    "            if isinstance(variant, np.ndarray):\n",
    "                normalized.append(variant.tolist())\n",
    "            elif isinstance(variant, list):\n",
    "                # Recursively normalize nested lists\n",
    "                normalized.append([item.tolist() if isinstance(item, np.ndarray) else item for item in variant])\n",
    "            else:\n",
    "                normalized.append(variant)\n",
    "        return normalized\n",
    "    \n",
    "    return []\n",
    "\n",
    "# ---------- main evaluation ----------\n",
    "def evaluate_on_gold_df(df, model, vocab, out, max_token_len=4, use_tuned_thr=True, show_sample=5):\n",
    "    hmm_prior = out[\"hmm_prior\"]\n",
    "    thr = float(out.get(\"best_thr\", 0.5)) if use_tuned_thr else 0.5\n",
    "\n",
    "    total_tp = total_fp = total_fn = 0\n",
    "    exact_hits = 0\n",
    "    n_eval = 0\n",
    "    examples = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        word = str(row[\"Word\"])\n",
    "        gold_variants = row[\"Gold\"]  # e.g., [['pi','kuna','s'], ['pi','ku','nas']]\n",
    "\n",
    "        # Normalize gold_variants (convert numpy arrays to lists)\n",
    "        gold_variants = normalize_gold_variants(gold_variants)\n",
    "\n",
    "        # skip if no gold\n",
    "        if not isinstance(gold_variants, list) or len(gold_variants) == 0:\n",
    "            continue\n",
    "\n",
    "        # tokenize & predict\n",
    "        toks = tokenize_with_vocab(word, vocab, max_token_len=max_token_len)\n",
    "        seg_string, probs = segment_tokens(model, vocab, toks, hmm_prior=hmm_prior, thr=thr)\n",
    "        mask01 = (probs >= thr).astype(int)\n",
    "        pred_set = offsets_from_tokens_and_mask(toks, mask01)\n",
    "\n",
    "        # build gold sets for all variants\n",
    "        gold_sets = [offsets_from_morphemes(gv) for gv in gold_variants]\n",
    "\n",
    "        # exact match if we match ANY gold variant\n",
    "        if any(pred_set == gs for gs in gold_sets):\n",
    "            exact_hits += 1\n",
    "\n",
    "        # choose the gold variant that gives best F1 for this word\n",
    "        best = max((f1_from_sets(pred_set, gs) + (gs,) for gs in gold_sets), key=lambda z: z[2])\n",
    "        P, R, F1, tp, fp, fn, best_gs = best\n",
    "\n",
    "        total_tp += tp; total_fp += fp; total_fn += fn\n",
    "        n_eval += 1\n",
    "\n",
    "        if len(examples) < show_sample:\n",
    "            # reconstruct a nice gold string for the best variant\n",
    "            best_morphs = None\n",
    "            for gv in gold_variants:\n",
    "                if offsets_from_morphemes(gv) == best_gs:\n",
    "                    best_morphs = gv; break\n",
    "            gold_str = \"-\".join(best_morphs) if best_morphs else \"(ambig)\"\n",
    "            examples.append({\n",
    "                \"word\": word,\n",
    "                \"tokens\": toks,\n",
    "                \"pred_seg\": seg_string,\n",
    "                \"gold_best\": gold_str,\n",
    "                \"P\": round(P,3), \"R\": round(R,3), \"F1\": round(F1,3)\n",
    "            })\n",
    "\n",
    "    # micro metrics\n",
    "    micro_P = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0.0\n",
    "    micro_R = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0.0\n",
    "    micro_F1 = 2 * micro_P * micro_R / (micro_P + micro_R) if (micro_P + micro_R) > 0 else 0.0\n",
    "    exact_rate = exact_hits / n_eval if n_eval > 0 else 0.0\n",
    "\n",
    "    print(f\"Evaluated {n_eval} words\")\n",
    "    print(f\"Boundary (micro)  P/R/F1 = {micro_P:.3f}/{micro_R:.3f}/{micro_F1:.3f}\")\n",
    "    print(f\"Exact-match rate  = {exact_rate:.3f}\")\n",
    "    if examples:\n",
    "        print(\"\\nSample predictions:\")\n",
    "        for ex in examples:\n",
    "            print(f\"- {ex['word']}\\n  tokens: {ex['tokens']}\\n  pred  : {ex['pred_seg']}\\n  gold  : {ex['gold_best']}\\n  P/R/F1: {ex['P']}/{ex['R']}/{ex['F1']}\\n\")\n",
    "\n",
    "    return {\n",
    "        \"n_eval\": n_eval,\n",
    "        \"micro_precision\": micro_P,\n",
    "        \"micro_recall\": micro_R,\n",
    "        \"micro_f1\": micro_F1,\n",
    "        \"exact_match_rate\": exact_rate,\n",
    "        \"examples\": examples\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "71e0449e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "# ===================================================================\n",
    "# Hyperparameter Tuning with Optuna\n",
    "# ===================================================================\n",
    "\n",
    "def objective(trial: optuna.Trial) -> float:\n",
    "    \"\"\"\n",
    "    This function defines one trial of the hyperparameter optimization.\n",
    "    Optuna will call this function multiple times with different hyperparameter values.\n",
    "    \"\"\"\n",
    "    # 1. Suggest hyperparameters\n",
    "    hparams = {\n",
    "        \"emb_dim\": trial.suggest_categorical(\"emb_dim\", [16, 32, 64]),\n",
    "        \"hidden_size\": trial.suggest_categorical(\"hidden_size\", [32, 64, 128]),\n",
    "        \"num_layers\": trial.suggest_int(\"num_layers\", 1, 3),\n",
    "        \"dropout\": trial.suggest_float(\"dropout\", 0.1, 0.5, step=0.05),\n",
    "        \"lr\": trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True),\n",
    "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 1e-5, 1e-3, log=True),\n",
    "        \"freeze_emb\": False,\n",
    "    }\n",
    "    # Also tune the lambda_prior weight\n",
    "    lambda_prior_val = trial.suggest_float(\"lambda_prior\", 0.0, 0.5)\n",
    "\n",
    "    # 2. Train the model using the training data (str_df)\n",
    "    print(f\"\\n--- Starting Trial {trial.number} with params: {hparams} | lambda_prior: {lambda_prior_val:.4f} ---\")\n",
    "    model, vocab, out = run_segmentation_with_privK(\n",
    "        df=str_df,\n",
    "        provided_suffix_list=suffix_list,\n",
    "        use_suffix_list=True,\n",
    "        unk_penalty=-15,\n",
    "        epochs=15,  # Use fewer epochs during tuning for speed\n",
    "        use_prior=True,\n",
    "        lambda_prior=lambda_prior_val,\n",
    "        lambda_k=0.2,\n",
    "        hparams=hparams,\n",
    "        synthetic_choice=SYNTHETIC_DATA_CHOICE  # Pass the synthetic data choice\n",
    "    )\n",
    "\n",
    "    # 3. Evaluate the trained model on the final hold-out test set (acc_df)\n",
    "    # This score will be used by Optuna to find the best parameters.\n",
    "    test_set_results = evaluate_on_gold_df(\n",
    "        df=acc_df,\n",
    "        model=model,\n",
    "        vocab=vocab,\n",
    "        out=out,\n",
    "        max_token_len=4,\n",
    "        use_tuned_thr=True,\n",
    "        show_sample=0\n",
    "    )\n",
    "    test_exact_match = test_set_results[\"exact_match_rate\"]\n",
    "    \n",
    "    print(f\"--- Finished Trial {trial.number} | Test Set Exact Match: {test_exact_match:.4f} ---\")\n",
    "    \n",
    "    # 4. Return the metric from the test set to be maximized\n",
    "    return test_exact_match\n",
    "\n",
    "\n",
    "# # Create a study object and specify the optimization direction.\n",
    "# study = optuna.create_study(direction=\"maximize\")\n",
    "\n",
    "# # Start the optimization. Optuna will run `n_trials` of the `objective` function.\n",
    "# # You can increase n_trials for a more thorough search.\n",
    "# study.optimize(objective, n_trials=50)\n",
    "\n",
    "# # Print the results of the best trial.\n",
    "# print(\"\\n\\n==========================================================\")\n",
    "# print(\"             Hyperparameter Tuning Finished             \")\n",
    "# print(\"==========================================================\")\n",
    "# print(\"Number of finished trials: \", len(study.trials))\n",
    "# print(\"Best trial:\")\n",
    "# best_trial = study.best_trial\n",
    "\n",
    "# print(f\"  Value (Test Set Exact Match Rate): {best_trial.value:.4f}\")\n",
    "# print(\"  Params: \")\n",
    "# for key, value in best_trial.params.items():\n",
    "#     print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "fccdfc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "best = {\n",
    "  \"emb_dim\": 32, \"hidden_size\": 128, \"num_layers\": 3,\n",
    "  \"dropout\": 0.4, \"lr\": 0.009213045798657327, \"weight_decay\": 0.0001132283214088801, \"freeze_emb\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "644fe445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for existing model with ID: 38f132fa19705d49\n",
      "Model artifacts loaded from models_Markov-LSTM-MarkovFilter\\38f132fa19705d49\n",
      "✅ Found existing model! Loading from models_Markov-LSTM-MarkovFilter\\38f132fa19705d49\n",
      "Model loaded successfully. Skipping training.\n"
     ]
    }
   ],
   "source": [
    "model, vocab, out = run_segmentation_with_privK(\n",
    "    df=str_df,\n",
    "    provided_suffix_list=suffix_list,\n",
    "    use_suffix_list=False,\n",
    "    unk_penalty=-15.0,\n",
    "    epochs=15,\n",
    "    use_prior=True,\n",
    "    lambda_prior=0.15289202508573396, # Weight for the HMM prior\n",
    "    lambda_k=0.2, \n",
    "    hparams=best,\n",
    "    synthetic_choice=SYNTHETIC_DATA_CHOICE  # Pass the synthetic data choice\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "942c49a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['p', 'i', 'k', 'u', 'n', 'a', 's']\n",
      "Boundary probs: [0.0, 0.9670000076293945, 0.0, 0.0010000000474974513, 0.0, 0.8569999933242798]\n",
      "Segmentation (thr=0.420): pi-kuna-s\n"
     ]
    }
   ],
   "source": [
    "word = \"pikunas\"\n",
    "tokens = tokenize_with_vocab(word, vocab, max_token_len=4)\n",
    "thr = out.get(\"best_thr\", 0.5)\n",
    "\n",
    "# Note the change in arguments here\n",
    "seg_string, boundary_probs = segment_tokens(\n",
    "    model, vocab, tokens, hmm_prior=out[\"hmm_prior\"], thr=thr\n",
    ")\n",
    "\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Boundary probs:\", np.round(boundary_probs, 3).tolist())\n",
    "print(f\"Segmentation (thr={thr:.3f}):\", seg_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "010699b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Set, Tuple\n",
    "\n",
    "# ---------- helpers to turn segs into boundary sets (char offsets) ----------\n",
    "def offsets_from_morphemes(morphs: List[str]) -> Set[int]:\n",
    "    # boundaries after each morph except the last\n",
    "    offs = []\n",
    "    s = 0\n",
    "    for i, m in enumerate(morphs):\n",
    "        s += len(m)\n",
    "        if i < len(morphs) - 1:\n",
    "            offs.append(s)\n",
    "    return set(offs)\n",
    "\n",
    "def offsets_from_tokens_and_mask(tokens: List[str], mask01: np.ndarray) -> Set[int]:\n",
    "    # boundaries after token i where mask01[i]==1, measured in character offsets\n",
    "    offs = set()\n",
    "    cum = 0\n",
    "    for i, t in enumerate(tokens):\n",
    "        cum += len(t)\n",
    "        if i < len(tokens) - 1 and mask01[i] == 1:\n",
    "            offs.add(cum)\n",
    "    return offs\n",
    "\n",
    "def f1_from_sets(pred: Set[int], gold: Set[int]) -> Tuple[float, float, float, int, int, int]:\n",
    "    tp = len(pred & gold)\n",
    "    fp = len(pred - gold)\n",
    "    fn = len(gold - pred)\n",
    "    P = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    R = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    F1 = 2 * P * R / (P + R) if (P + R) > 0 else 0.0\n",
    "    return P, R, F1, tp, fp, fn\n",
    "\n",
    "def normalize_gold_variants(gold_variants):\n",
    "    \"\"\"\n",
    "    Convert gold_variants to a list format, handling numpy arrays and nested structures.\n",
    "    \"\"\"\n",
    "    if gold_variants is None:\n",
    "        return []\n",
    "    \n",
    "    # If it's a numpy array, convert to list\n",
    "    if isinstance(gold_variants, np.ndarray):\n",
    "        gold_variants = gold_variants.tolist()\n",
    "    \n",
    "    # If it's already a list, ensure nested elements are also lists (not numpy arrays)\n",
    "    if isinstance(gold_variants, list):\n",
    "        normalized = []\n",
    "        for variant in gold_variants:\n",
    "            if isinstance(variant, np.ndarray):\n",
    "                normalized.append(variant.tolist())\n",
    "            elif isinstance(variant, list):\n",
    "                # Recursively normalize nested lists\n",
    "                normalized.append([item.tolist() if isinstance(item, np.ndarray) else item for item in variant])\n",
    "            else:\n",
    "                normalized.append(variant)\n",
    "        return normalized\n",
    "    \n",
    "    return []\n",
    "\n",
    "# ---------- main evaluation ----------\n",
    "def evaluate_on_gold_df(df, model, vocab, out, max_token_len=4, use_tuned_thr=True, show_sample=5):\n",
    "    hmm_prior = out[\"hmm_prior\"]\n",
    "    thr = float(out.get(\"best_thr\", 0.5)) if use_tuned_thr else 0.5\n",
    "\n",
    "    total_tp = total_fp = total_fn = 0\n",
    "    exact_hits = 0\n",
    "    n_eval = 0\n",
    "    examples = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        word = str(row[\"Word\"])\n",
    "        gold_variants = row[\"Gold\"]  # e.g., [['pi','kuna','s'], ['pi','ku','nas']]\n",
    "\n",
    "        # Normalize gold_variants (convert numpy arrays to lists)\n",
    "        gold_variants = normalize_gold_variants(gold_variants)\n",
    "\n",
    "        # skip if no gold\n",
    "        if not isinstance(gold_variants, list) or len(gold_variants) == 0:\n",
    "            continue\n",
    "\n",
    "        # tokenize & predict\n",
    "        toks = tokenize_with_vocab(word, vocab, max_token_len=max_token_len)\n",
    "        seg_string, probs = segment_tokens(model, vocab, toks, hmm_prior=hmm_prior, thr=thr)\n",
    "        mask01 = (probs >= thr).astype(int)\n",
    "        pred_set = offsets_from_tokens_and_mask(toks, mask01)\n",
    "\n",
    "        # build gold sets for all variants\n",
    "        gold_sets = [offsets_from_morphemes(gv) for gv in gold_variants]\n",
    "\n",
    "        # exact match if we match ANY gold variant\n",
    "        if any(pred_set == gs for gs in gold_sets):\n",
    "            exact_hits += 1\n",
    "\n",
    "        # choose the gold variant that gives best F1 for this word\n",
    "        best = max((f1_from_sets(pred_set, gs) + (gs,) for gs in gold_sets), key=lambda z: z[2])\n",
    "        P, R, F1, tp, fp, fn, best_gs = best\n",
    "\n",
    "        total_tp += tp; total_fp += fp; total_fn += fn\n",
    "        n_eval += 1\n",
    "\n",
    "        if len(examples) < show_sample:\n",
    "            # reconstruct a nice gold string for the best variant\n",
    "            best_morphs = None\n",
    "            for gv in gold_variants:\n",
    "                if offsets_from_morphemes(gv) == best_gs:\n",
    "                    best_morphs = gv; break\n",
    "            gold_str = \"-\".join(best_morphs) if best_morphs else \"(ambig)\"\n",
    "            examples.append({\n",
    "                \"word\": word,\n",
    "                \"tokens\": toks,\n",
    "                \"pred_seg\": seg_string,\n",
    "                \"gold_best\": gold_str,\n",
    "                \"P\": round(P,3), \"R\": round(R,3), \"F1\": round(F1,3)\n",
    "            })\n",
    "\n",
    "    # micro metrics\n",
    "    micro_P = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0.0\n",
    "    micro_R = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0.0\n",
    "    micro_F1 = 2 * micro_P * micro_R / (micro_P + micro_R) if (micro_P + micro_R) > 0 else 0.0\n",
    "    exact_rate = exact_hits / n_eval if n_eval > 0 else 0.0\n",
    "\n",
    "    print(f\"Evaluated {n_eval} words\")\n",
    "    print(f\"Boundary (micro)  P/R/F1 = {micro_P:.3f}/{micro_R:.3f}/{micro_F1:.3f}\")\n",
    "    print(f\"Exact-match rate  = {exact_rate:.3f}\")\n",
    "    if examples:\n",
    "        print(\"\\nSample predictions:\")\n",
    "        for ex in examples:\n",
    "            print(f\"- {ex['word']}\\n  tokens: {ex['tokens']}\\n  pred  : {ex['pred_seg']}\\n  gold  : {ex['gold_best']}\\n  P/R/F1: {ex['P']}/{ex['R']}/{ex['F1']}\\n\")\n",
    "\n",
    "    return {\n",
    "        \"n_eval\": n_eval,\n",
    "        \"micro_precision\": micro_P,\n",
    "        \"micro_recall\": micro_R,\n",
    "        \"micro_f1\": micro_F1,\n",
    "        \"exact_match_rate\": exact_rate,\n",
    "        \"examples\": examples\n",
    "    }\n",
    "\n",
    "# ===================================================================\n",
    "# NEW CODE: Suffix Validator Function\n",
    "# ===================================================================\n",
    "\n",
    "def is_segmentation_valid(\n",
    "    segmentation: list[str],\n",
    "    allowed_suffixes: set[str]\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Checks if a segmentation is valid based on a list of allowed suffixes.\n",
    "\n",
    "    The first morpheme is assumed to be the root and is ignored. All subsequent\n",
    "    morphemes must be in the `allowed_suffixes` set.\n",
    "\n",
    "    Args:\n",
    "        segmentation (list[str]): The predicted segmentation, e.g., ['pay', 'kunaq'].\n",
    "        allowed_suffixes (set[str]): A set of valid suffix strings.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the segmentation is valid, False otherwise.\n",
    "    \"\"\"\n",
    "    if len(segmentation) <= 1:\n",
    "        # A word with no splits is always valid.\n",
    "        return True\n",
    "\n",
    "    # Check every morpheme starting from the second one.\n",
    "    for morpheme in segmentation[1:]:\n",
    "        if morpheme not in allowed_suffixes:\n",
    "            return False  # Found a suffix that is not in the allowed list.\n",
    "\n",
    "    return True\n",
    "\n",
    "# ===================================================================\n",
    "# REVISED CODE: Evaluation function that ignores rejected predictions\n",
    "# ===================================================================\n",
    "\n",
    "def evaluate_and_ignore_rejected(\n",
    "    df, model, vocab, out,\n",
    "    allowed_suffixes: list[str], # Required argument for the validator\n",
    "    max_token_len=4,\n",
    "    use_tuned_thr=True,\n",
    "    show_sample=5\n",
    "):\n",
    "    hmm_prior = out[\"hmm_prior\"]\n",
    "    thr = float(out.get(\"best_thr\", 0.5)) if use_tuned_thr else 0.5\n",
    "    allowed_suffixes_set = set(allowed_suffixes)\n",
    "\n",
    "    total_tp = total_fp = total_fn = 0\n",
    "    exact_hits = 0\n",
    "    \n",
    "    n_total_words = 0      # Counts all words we attempt to evaluate\n",
    "    n_evaluated_words = 0  # Counts only words with valid, scored predictions\n",
    "    rejection_count = 0\n",
    "    false_rejection_count = 0  # Count of CORRECT segmentations that were rejected\n",
    "    correct_kept_count = 0     # Count of CORRECT segmentations that were kept\n",
    "    examples = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        word = str(row[\"Word\"])\n",
    "        gold_variants = row[\"Gold\"]\n",
    "\n",
    "        # Normalize gold_variants (convert numpy arrays to lists)\n",
    "        gold_variants = normalize_gold_variants(gold_variants)\n",
    "\n",
    "        if not isinstance(gold_variants, list) or len(gold_variants) == 0:\n",
    "            continue\n",
    "        \n",
    "        n_total_words += 1\n",
    "\n",
    "        # 1. Get the model's prediction\n",
    "        toks = tokenize_with_vocab(word, vocab, max_token_len=max_token_len)\n",
    "        seg_string, probs = segment_tokens(model, vocab, toks, hmm_prior=hmm_prior, thr=thr)\n",
    "        predicted_morphs = seg_string.split('-')\n",
    "        \n",
    "        # 2. Check if prediction is correct BEFORE validating (for false rejection analysis)\n",
    "        mask01 = (probs >= thr).astype(int)\n",
    "        pred_set = offsets_from_tokens_and_mask(toks, mask01)\n",
    "        gold_sets = [offsets_from_morphemes(gv) for gv in gold_variants]\n",
    "        is_correct = any(pred_set == gs for gs in gold_sets)\n",
    "\n",
    "        # 3. Validate the prediction. If invalid, check if it was correct (false rejection)\n",
    "        if not is_segmentation_valid(predicted_morphs, allowed_suffixes_set):\n",
    "            rejection_count += 1\n",
    "            if is_correct:\n",
    "                false_rejection_count += 1\n",
    "            continue  # <-- KEY CHANGE: Skip the rest of the loop for this word\n",
    "\n",
    "        # --- If we reach this point, the prediction is valid and will be scored ---\n",
    "        n_evaluated_words += 1\n",
    "        \n",
    "        # 4. Track if kept prediction is correct\n",
    "        if is_correct:\n",
    "            correct_kept_count += 1\n",
    "            exact_hits += 1\n",
    "\n",
    "        best = max((f1_from_sets(pred_set, gs) + (gs,) for gs in gold_sets), key=lambda z: z[2])\n",
    "        P, R, F1, tp, fp, fn, best_gs = best\n",
    "\n",
    "        total_tp += tp\n",
    "        total_fp += fp\n",
    "        total_fn += fn\n",
    "\n",
    "        if len(examples) < show_sample:\n",
    "            # reconstruct a nice gold string for the best variant\n",
    "            best_morphs = None\n",
    "            for gv in gold_variants:\n",
    "                if offsets_from_morphemes(gv) == best_gs:\n",
    "                    best_morphs = gv; break\n",
    "            gold_str = \"-\".join(best_morphs) if best_morphs else \"(ambig)\"\n",
    "            examples.append({\n",
    "                \"word\": word,\n",
    "                \"tokens\": toks,\n",
    "                \"pred_seg\": seg_string,\n",
    "                \"gold_best\": gold_str,\n",
    "                \"P\": round(P,3), \"R\": round(R,3), \"F1\": round(F1,3)\n",
    "            })\n",
    "\n",
    "    # --- Final Metrics ---\n",
    "    # Note: Denominators now use n_evaluated_words, which is smaller than n_total_words\n",
    "    micro_P = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0.0\n",
    "    micro_R = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0.0\n",
    "    micro_F1 = 2 * micro_P * micro_R / (micro_P + micro_R) if (micro_P + micro_R) > 0 else 0.0\n",
    "    exact_rate = exact_hits / n_evaluated_words if n_evaluated_words > 0 else 0.0\n",
    "    \n",
    "    # --- Rejection Filter Analysis ---\n",
    "    # Filter precision: of the kept predictions, how many are correct?\n",
    "    filter_precision = correct_kept_count / n_evaluated_words if n_evaluated_words > 0 else 0.0\n",
    "    \n",
    "    # False rejection rate: of all correct predictions, how many were rejected?\n",
    "    total_correct = correct_kept_count + false_rejection_count\n",
    "    false_rejection_rate = false_rejection_count / total_correct if total_correct > 0 else 0.0\n",
    "\n",
    "    print(f\"Attempted to evaluate {n_total_words} words\")\n",
    "    print(f\"Predictions Rejected by Suffix Validator: {rejection_count} ({rejection_count/n_total_words:.2%})\")\n",
    "    print(f\"Final scores are based on the remaining {n_evaluated_words} valid predictions.\")\n",
    "    print(\"\\n--- Rejection Filter False Rejection Analysis ---\")\n",
    "    print(f\"Filter achieves {filter_precision:.1%} precision but rejects {false_rejection_rate:.1%} valid segmentations\")\n",
    "    print(f\"  - Correct predictions kept: {correct_kept_count}\")\n",
    "    print(f\"  - Correct predictions rejected (false rejections): {false_rejection_count}\")\n",
    "    print(f\"  - Total correct predictions: {total_correct}\")\n",
    "    print(\"\\n--- Final Scores (on non-rejected predictions only) ---\")\n",
    "    print(f\"Boundary (micro)  P/R/F1 = {micro_P:.3f}/{micro_R:.3f}/{micro_F1:.3f}\")\n",
    "    print(f\"Exact-match rate  = {exact_rate:.3f}\")\n",
    "\n",
    "    if examples:\n",
    "        print(\"\\nSample predictions:\")\n",
    "        for ex in examples:\n",
    "            print(f\"- {ex['word']}\\n  tokens: {ex['tokens']}\\n  pred  : {ex['pred_seg']}\\n  gold  : {ex['gold_best']}\\n  P/R/F1: {ex['P']}/{ex['R']}/{ex['F1']}\\n\")\n",
    "    return { \n",
    "        \"micro_f1\": micro_F1, \n",
    "        \"exact_match_rate\": exact_rate, \n",
    "        \"rejection_count\": rejection_count,\n",
    "        \"false_rejection_count\": false_rejection_count,\n",
    "        \"filter_precision\": filter_precision,\n",
    "        \"false_rejection_rate\": false_rejection_rate\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "439ae857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove words with length > 16\n",
    "# acc_df = acc_df[acc_df['Word'].str.len() <= 14].reset_index(drop=True)\n",
    "\n",
    "# # Remove rows where all gold variants have only one morpheme\n",
    "# acc_df = acc_df[acc_df['Gold'].apply(lambda variants: any(len(variant) > 1 for variant in variants))].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "641ca619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Gold</th>\n",
       "      <th>Morph_split</th>\n",
       "      <th>Morph_split_str</th>\n",
       "      <th>Filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>unupas</td>\n",
       "      <td>[[unu, pas]]</td>\n",
       "      <td>[unu, pas]</td>\n",
       "      <td>unu pas</td>\n",
       "      <td>For_Annotation_1_LS.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>umankus</td>\n",
       "      <td>[[uma, nku, s]]</td>\n",
       "      <td>[uma, nku, s]</td>\n",
       "      <td>uma nku s</td>\n",
       "      <td>For_Annotation_1_LS.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hikurin</td>\n",
       "      <td>[[hikuri, n]]</td>\n",
       "      <td>[hikuri, n]</td>\n",
       "      <td>hikuri n</td>\n",
       "      <td>For_Annotation_1_LS.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sutipi</td>\n",
       "      <td>[[suti, pi]]</td>\n",
       "      <td>[suti, pi]</td>\n",
       "      <td>suti pi</td>\n",
       "      <td>For_Annotation_1_LS.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pikunas</td>\n",
       "      <td>[[pi, kuna, s]]</td>\n",
       "      <td>[pi, kuna, s]</td>\n",
       "      <td>pi kuna s</td>\n",
       "      <td>For_Annotation_1_LS.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Word             Gold    Morph_split Morph_split_str  \\\n",
       "0   unupas     [[unu, pas]]     [unu, pas]         unu pas   \n",
       "1  umankus  [[uma, nku, s]]  [uma, nku, s]       uma nku s   \n",
       "2  hikurin    [[hikuri, n]]    [hikuri, n]        hikuri n   \n",
       "3   sutipi     [[suti, pi]]     [suti, pi]         suti pi   \n",
       "4  pikunas  [[pi, kuna, s]]  [pi, kuna, s]       pi kuna s   \n",
       "\n",
       "                  Filename  \n",
       "0  For_Annotation_1_LS.csv  \n",
       "1  For_Annotation_1_LS.csv  \n",
       "2  For_Annotation_1_LS.csv  \n",
       "3  For_Annotation_1_LS.csv  \n",
       "4  For_Annotation_1_LS.csv  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8c82d09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated 913 words\n",
      "Boundary (micro)  P/R/F1 = 0.794/0.852/0.822\n",
      "Exact-match rate  = 0.574\n",
      "\n",
      "Sample predictions:\n",
      "- unupas\n",
      "  tokens: ['u', 'n', 'u', 'p', 'a', 's']\n",
      "  pred  : unupa-s\n",
      "  gold  : unu-pas\n",
      "  P/R/F1: 0.0/0.0/0.0\n",
      "\n",
      "- umankus\n",
      "  tokens: ['u', 'm', 'a', 'n', 'k', 'u', 's']\n",
      "  pred  : uma-nku-s\n",
      "  gold  : uma-nku-s\n",
      "  P/R/F1: 1.0/1.0/1.0\n",
      "\n",
      "- hikurin\n",
      "  tokens: ['h', 'i', 'k', 'u', 'r', 'i', 'n']\n",
      "  pred  : hiku-ri-n\n",
      "  gold  : hikuri-n\n",
      "  P/R/F1: 0.5/1.0/0.667\n",
      "\n",
      "- sutipi\n",
      "  tokens: ['s', 'u', 't', 'i', 'p', 'i']\n",
      "  pred  : suti-pi\n",
      "  gold  : suti-pi\n",
      "  P/R/F1: 1.0/1.0/1.0\n",
      "\n",
      "- pikunas\n",
      "  tokens: ['p', 'i', 'k', 'u', 'n', 'a', 's']\n",
      "  pred  : pi-kuna-s\n",
      "  gold  : pi-kuna-s\n",
      "  P/R/F1: 1.0/1.0/1.0\n",
      "\n",
      "- atipaq\n",
      "  tokens: ['a', 't', 'i', 'p', 'a', 'q']\n",
      "  pred  : ati-paq\n",
      "  gold  : ati-paq\n",
      "  P/R/F1: 1.0/1.0/1.0\n",
      "\n",
      "- tomani\n",
      "  tokens: ['t', 'o', 'm', 'a', 'n', 'i']\n",
      "  pred  : toma-ni\n",
      "  gold  : toma-ni\n",
      "  P/R/F1: 1.0/1.0/1.0\n",
      "\n",
      "- rantiq\n",
      "  tokens: ['r', 'a', 'n', 't', 'i', 'q']\n",
      "  pred  : rantiq\n",
      "  gold  : ranti-q\n",
      "  P/R/F1: 0.0/0.0/0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_on_gold_df(\n",
    "    acc_df,                     # your concatenated DataFrame with Word + Gold (list of variants)\n",
    "    model, vocab, out,      # from training\n",
    "    max_token_len=4,        # must match your tokenize scheme\n",
    "    use_tuned_thr=True,     # use the best threshold found on dev\n",
    "    show_sample=8           # print a few qualitative examples\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f08cbbfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating with Post-Processing Rejection Filter ---\n",
      "Attempted to evaluate 913 words\n",
      "Predictions Rejected by Suffix Validator: 245 (26.83%)\n",
      "Final scores are based on the remaining 668 valid predictions.\n",
      "\n",
      "--- Rejection Filter False Rejection Analysis ---\n",
      "Filter achieves 66.6% precision but rejects 15.1% valid segmentations\n",
      "  - Correct predictions kept: 445\n",
      "  - Correct predictions rejected (false rejections): 79\n",
      "  - Total correct predictions: 524\n",
      "\n",
      "--- Final Scores (on non-rejected predictions only) ---\n",
      "Boundary (micro)  P/R/F1 = 0.826/0.901/0.862\n",
      "Exact-match rate  = 0.666\n",
      "\n",
      "Sample predictions:\n",
      "- unupas\n",
      "  tokens: ['u', 'n', 'u', 'p', 'a', 's']\n",
      "  pred  : unupa-s\n",
      "  gold  : unu-pas\n",
      "  P/R/F1: 0.0/0.0/0.0\n",
      "\n",
      "- umankus\n",
      "  tokens: ['u', 'm', 'a', 'n', 'k', 'u', 's']\n",
      "  pred  : uma-nku-s\n",
      "  gold  : uma-nku-s\n",
      "  P/R/F1: 1.0/1.0/1.0\n",
      "\n",
      "- hikurin\n",
      "  tokens: ['h', 'i', 'k', 'u', 'r', 'i', 'n']\n",
      "  pred  : hiku-ri-n\n",
      "  gold  : hikuri-n\n",
      "  P/R/F1: 0.5/1.0/0.667\n",
      "\n",
      "- sutipi\n",
      "  tokens: ['s', 'u', 't', 'i', 'p', 'i']\n",
      "  pred  : suti-pi\n",
      "  gold  : suti-pi\n",
      "  P/R/F1: 1.0/1.0/1.0\n",
      "\n",
      "- pikunas\n",
      "  tokens: ['p', 'i', 'k', 'u', 'n', 'a', 's']\n",
      "  pred  : pi-kuna-s\n",
      "  gold  : pi-kuna-s\n",
      "  P/R/F1: 1.0/1.0/1.0\n",
      "\n",
      "- atipaq\n",
      "  tokens: ['a', 't', 'i', 'p', 'a', 'q']\n",
      "  pred  : ati-paq\n",
      "  gold  : ati-paq\n",
      "  P/R/F1: 1.0/1.0/1.0\n",
      "\n",
      "- tomani\n",
      "  tokens: ['t', 'o', 'm', 'a', 'n', 'i']\n",
      "  pred  : toma-ni\n",
      "  gold  : toma-ni\n",
      "  P/R/F1: 1.0/1.0/1.0\n",
      "\n",
      "- rantiq\n",
      "  tokens: ['r', 'a', 'n', 't', 'i', 'q']\n",
      "  pred  : rantiq\n",
      "  gold  : ranti-q\n",
      "  P/R/F1: 0.0/0.0/0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. Call the NEW evaluation function with your suffix list\n",
    "print(\"\\n--- Evaluating with Post-Processing Rejection Filter ---\")\n",
    "results_with_rejection = evaluate_and_ignore_rejected(\n",
    "    acc_df,              # The test dataframe\n",
    "    model, vocab, out,   # The trained model and its artifacts\n",
    "    allowed_suffixes=suffix_list, # Your list of rules!\n",
    "    show_sample=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "69e95e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ===================================================================\n",
    "# # SYNTHETIC DATA AUGMENTATION COMPARISON TABLE\n",
    "# # ===================================================================\n",
    "# # This cell creates a comprehensive comparison table showing the impact\n",
    "# # of different augmentation strategies on model performance\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from IPython.display import display, HTML\n",
    "\n",
    "# # Show the best hyperparameters being used\n",
    "# print(\"=\"*80)\n",
    "# print(\"BEST HYPERPARAMETERS\")\n",
    "# print(\"=\"*80)\n",
    "# print(f\"emb_dim: {best['emb_dim']}\")\n",
    "# print(f\"hidden_size: {best['hidden_size']}\")\n",
    "# print(f\"num_layers: {best['num_layers']}\")\n",
    "# print(f\"dropout: {best['dropout']}\")\n",
    "# print(f\"lr: {best['lr']}\")\n",
    "# print(f\"weight_decay: {best['weight_decay']}\")\n",
    "# print(f\"freeze_emb: {best['freeze_emb']}\")\n",
    "# print(f\"lambda_prior: 0.15289202508573396\")\n",
    "# print(f\"lambda_k: 0.2\")\n",
    "# print(\"=\"*80)\n",
    "# print(\"\\n\")\n",
    "\n",
    "# # Function to run evaluation for a specific augmentation configuration\n",
    "# def evaluate_augmentation_config(\n",
    "#     synthetic_choice,\n",
    "#     word_selection,\n",
    "#     n_words,\n",
    "#     str_df_base,\n",
    "#     acc_df,\n",
    "#     suffix_list,\n",
    "#     best_hparams,\n",
    "#     lambda_prior=0.15289202508573396,\n",
    "#     lambda_k=0.2\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Evaluate model with a specific augmentation configuration.\n",
    "    \n",
    "#     Args:\n",
    "#         synthetic_choice: \"none\", \"gpt4o\", or \"gpt5mini\"\n",
    "#         word_selection: \"first\" or \"random\"\n",
    "#         n_words: number of words (None for \"none\")\n",
    "#         str_df_base: base training dataframe (gold data)\n",
    "#         acc_df: test dataframe\n",
    "#         suffix_list: list of allowed suffixes\n",
    "#         best_hparams: hyperparameters dictionary\n",
    "#         lambda_prior: weight for prior distillation\n",
    "#         lambda_k: weight for K-regularizer\n",
    "    \n",
    "#     Returns:\n",
    "#         Dictionary with results\n",
    "#     \"\"\"\n",
    "#     print(f\"\\n{'='*80}\")\n",
    "#     print(f\"Evaluating: {synthetic_choice} | {word_selection} | {n_words}\")\n",
    "#     print(f\"{'='*80}\")\n",
    "    \n",
    "#     # Prepare training data based on configuration\n",
    "#     if synthetic_choice == \"none\":\n",
    "#         # For \"none\", use the already processed str_df_base\n",
    "#         train_str_df = str_df_base.copy()\n",
    "#     else:\n",
    "#         # Load synthetic data\n",
    "#         synthetic_df = load_synthetic_data(synthetic_choice)\n",
    "#         if synthetic_df is None:\n",
    "#             print(f\"Warning: Could not load {synthetic_choice} data\")\n",
    "#             return None\n",
    "        \n",
    "#         # Get common words\n",
    "#         gpt_5_mini_words = set(gpt_5_mini_df['Word'])\n",
    "#         gpt_4o_words = set(gpt_4o_df['Word'])\n",
    "#         common_words = gpt_4o_words.intersection(gpt_5_mini_words)\n",
    "        \n",
    "#         # Select words based on strategy\n",
    "#         if word_selection == \"first\":\n",
    "#             sorted_words = sorted(common_words)\n",
    "#             n = min(n_words, len(sorted_words))\n",
    "#             selected_words = set(sorted_words[:n])\n",
    "#         elif word_selection == \"random\":\n",
    "#             import random\n",
    "#             # Use same seed for reproducibility (42 for all random selections)\n",
    "#             random.seed(42)\n",
    "#             n = min(n_words, len(common_words))\n",
    "#             selected_words = set(random.sample(list(common_words), n))\n",
    "#         else:\n",
    "#             selected_words = common_words\n",
    "        \n",
    "#         # Filter synthetic data\n",
    "#         df_sampled = synthetic_df[synthetic_df['Word'].isin(selected_words)]\n",
    "        \n",
    "#         # Combine with gold data (need to use the base gold_df format for processing)\n",
    "#         # Load base gold data\n",
    "#         gold_df_temp = pd.read_parquet(os.path.join(DATA_FOLDER, \"Sue_kalt.parquet\"))\n",
    "#         gold_df_temp['Word'] = gold_df_temp['word']\n",
    "#         gold_df_temp['morph'] = gold_df_temp['morph'].str.replace('-', ' ')\n",
    "#         gold_df_temp['Morph_split_str'] = gold_df_temp['morph']\n",
    "#         gold_df_temp['Morph_split'] = gold_df_temp['morph'].str.split(' ')\n",
    "#         gold_df_temp = gold_df_temp[['Word', 'Morph_split', 'Morph_split_str']]\n",
    "#         gold_df_temp.drop_duplicates(subset='Word', keep='first', inplace=True)\n",
    "#         gold_df_temp.dropna(subset=['Word'], inplace=True)\n",
    "        \n",
    "#         train_df = pd.concat([df_sampled, gold_df_temp], ignore_index=True)\n",
    "        \n",
    "#         # Process training data (same as in main notebook)\n",
    "#         train_df[\"Char_split\"] = train_df[\"Morph_split\"].apply(tokenize_morphemes)\n",
    "#         train_df[\"CV_split\"] = train_df[\"Char_split\"].apply(morphs_to_cv)\n",
    "        \n",
    "#         # Create str_df format\n",
    "#         train_str_df = pd.DataFrame()\n",
    "#         train_str_df[\"Full_chain\"] = train_df[\"CV_split\"].apply(cv_to_string)\n",
    "#         train_str_df[\"Trimmed_chain\"] = train_str_df[\"Full_chain\"].apply(\n",
    "#             lambda x: x.split(\"-\", 1)[1] if \"-\" in x else np.nan\n",
    "#         )\n",
    "#         train_str_df[\"Word\"] = train_df[\"Word\"]\n",
    "#         train_str_df[\"Char_split\"] = train_df[\"Char_split\"]\n",
    "#         train_str_df[\"Morph_split\"] = train_df[\"Morph_split\"]\n",
    "#         train_str_df = train_str_df.dropna(subset=[\"Trimmed_chain\"]).reset_index(drop=True)\n",
    "        \n",
    "#         # Add features\n",
    "#         train_str_df[\"Word_len\"] = train_str_df[\"Word\"].str.len()\n",
    "#         train_str_df[\"Vowel_no\"] = train_str_df[\"Full_chain\"].str.count(\"V\")\n",
    "#         train_str_df[\"Cons_no\"] = train_str_df[\"Full_chain\"].str.count(\"C\")\n",
    "#         train_str_df[\"Tail_cons_no\"] = train_str_df[\"Trimmed_chain\"].str.count(\"C\")\n",
    "#         train_str_df[\"Tail_vowel_no\"] = train_str_df[\"Trimmed_chain\"].str.count(\"V\")\n",
    "#         train_str_df[\"No_splits\"] = train_str_df[\"Morph_split\"].str.len()\n",
    "#         train_str_df[\"YW_count\"] = train_str_df[\"Word\"].str.count(\"[yw]\")\n",
    "#         train_str_df[\"Tail_YW_count\"] = train_str_df[\"Morph_split\"].apply(\n",
    "#             lambda ms: sum(m.count(\"y\") + m.count(\"w\") for m in ms[1:])\n",
    "#         )\n",
    "    \n",
    "#     # Train or load model\n",
    "#     model, vocab, out = run_segmentation_with_privK(\n",
    "#         df=train_str_df,\n",
    "#         provided_suffix_list=suffix_list,\n",
    "#         use_suffix_list=False,\n",
    "#         unk_penalty=-15.0,\n",
    "#         epochs=15,\n",
    "#         use_prior=True,\n",
    "#         lambda_prior=lambda_prior,\n",
    "#         lambda_k=lambda_k,\n",
    "#         hparams=best_hparams,\n",
    "#         synthetic_choice=synthetic_choice\n",
    "#     )\n",
    "    \n",
    "#     # Evaluate without rejection filter\n",
    "#     results_base = evaluate_on_gold_df(\n",
    "#         acc_df, model, vocab, out,\n",
    "#         max_token_len=4,\n",
    "#         use_tuned_thr=True,\n",
    "#         show_sample=0\n",
    "#     )\n",
    "    \n",
    "#     # Evaluate with rejection filter\n",
    "#     results_filtered = evaluate_and_ignore_rejected(\n",
    "#         acc_df, model, vocab, out,\n",
    "#         allowed_suffixes=suffix_list,\n",
    "#         max_token_len=4,\n",
    "#         use_tuned_thr=True,\n",
    "#         show_sample=0\n",
    "#     )\n",
    "    \n",
    "#     return {\n",
    "#         \"base_exact_match\": results_base[\"exact_match_rate\"],\n",
    "#         \"base_f1\": results_base[\"micro_f1\"],\n",
    "#         \"filtered_exact_match\": results_filtered[\"exact_match_rate\"],\n",
    "#         \"filtered_f1\": results_filtered[\"micro_f1\"],\n",
    "#         \"filter_precision\": results_filtered[\"filter_precision\"],\n",
    "#         \"false_rejection_rate\": results_filtered[\"false_rejection_rate\"],\n",
    "#         \"rejection_count\": results_filtered[\"rejection_count\"],\n",
    "#         \"false_rejection_count\": results_filtered[\"false_rejection_count\"]\n",
    "#     }\n",
    "\n",
    "# # Load base gold data for training\n",
    "# print(\"Loading base gold data...\")\n",
    "# gold_df_base = pd.read_parquet(os.path.join(DATA_FOLDER, \"Sue_kalt.parquet\"))\n",
    "# gold_df_base['Word'] = gold_df_base['word']\n",
    "# gold_df_base['morph'] = gold_df_base['morph'].str.replace('-', ' ')\n",
    "# gold_df_base['Morph_split_str'] = gold_df_base['morph']\n",
    "# gold_df_base['Morph_split'] = gold_df_base['morph'].str.split(' ')\n",
    "# gold_df_base = gold_df_base[['Word', 'Morph_split', 'Morph_split_str']]\n",
    "# gold_df_base.drop_duplicates(subset='Word', keep='first', inplace=True)\n",
    "# gold_df_base.dropna(subset=['Word'], inplace=True)\n",
    "\n",
    "# # Process base data\n",
    "# gold_df_base[\"Char_split\"] = gold_df_base[\"Morph_split\"].apply(tokenize_morphemes)\n",
    "# gold_df_base[\"CV_split\"] = gold_df_base[\"Char_split\"].apply(morphs_to_cv)\n",
    "\n",
    "# str_df_base = pd.DataFrame()\n",
    "# str_df_base[\"Full_chain\"] = gold_df_base[\"CV_split\"].apply(cv_to_string)\n",
    "# str_df_base[\"Trimmed_chain\"] = str_df_base[\"Full_chain\"].apply(\n",
    "#     lambda x: x.split(\"-\", 1)[1] if \"-\" in x else np.nan\n",
    "# )\n",
    "# str_df_base[\"Word\"] = gold_df_base[\"Word\"]\n",
    "# str_df_base[\"Char_split\"] = gold_df_base[\"Char_split\"]\n",
    "# str_df_base[\"Morph_split\"] = gold_df_base[\"Morph_split\"]\n",
    "# str_df_base = str_df_base.dropna(subset=[\"Trimmed_chain\"]).reset_index(drop=True)\n",
    "\n",
    "# str_df_base[\"Word_len\"] = str_df_base[\"Word\"].str.len()\n",
    "# str_df_base[\"Vowel_no\"] = str_df_base[\"Full_chain\"].str.count(\"V\")\n",
    "# str_df_base[\"Cons_no\"] = str_df_base[\"Full_chain\"].str.count(\"C\")\n",
    "# str_df_base[\"Tail_cons_no\"] = str_df_base[\"Trimmed_chain\"].str.count(\"C\")\n",
    "# str_df_base[\"Tail_vowel_no\"] = str_df_base[\"Trimmed_chain\"].str.count(\"V\")\n",
    "# str_df_base[\"No_splits\"] = str_df_base[\"Morph_split\"].str.len()\n",
    "# str_df_base[\"YW_count\"] = str_df_base[\"Word\"].str.count(\"[yw]\")\n",
    "# str_df_base[\"Tail_YW_count\"] = str_df_base[\"Morph_split\"].apply(\n",
    "#     lambda ms: sum(m.count(\"y\") + m.count(\"w\") for m in ms[1:])\n",
    "# )\n",
    "\n",
    "# # Define all configurations to test\n",
    "# configs = [\n",
    "#     (\"none\", None, None),\n",
    "#     (\"gpt4o\", \"first\", 100),\n",
    "#     (\"gpt5mini\", \"first\", 100),\n",
    "#     (\"gpt4o\", \"first\", 200),\n",
    "#     (\"gpt5mini\", \"first\", 200),\n",
    "#     (\"gpt4o\", \"first\", 300),\n",
    "#     (\"gpt5mini\", \"first\", 300),\n",
    "#     (\"gpt4o\", \"random\", 100),\n",
    "#     (\"gpt5mini\", \"random\", 100),\n",
    "#     (\"gpt4o\", \"random\", 200),\n",
    "#     (\"gpt5mini\", \"random\", 200),\n",
    "#     (\"gpt4o\", \"random\", 300),\n",
    "#     (\"gpt5mini\", \"random\", 300),\n",
    "# ]\n",
    "\n",
    "# # Run evaluations for all configurations\n",
    "# results_list = []\n",
    "# for synthetic_choice, word_selection, n_words in configs:\n",
    "#     try:\n",
    "#         result = evaluate_augmentation_config(\n",
    "#             synthetic_choice=synthetic_choice,\n",
    "#             word_selection=word_selection,\n",
    "#             n_words=n_words,\n",
    "#             str_df_base=str_df_base,\n",
    "#             acc_df=acc_df,\n",
    "#             suffix_list=suffix_list,\n",
    "#             best_hparams=best,\n",
    "#             lambda_prior=0.15289202508573396,\n",
    "#             lambda_k=0.2\n",
    "#         )\n",
    "        \n",
    "#         if result is not None:\n",
    "#             # Create config name\n",
    "#             if synthetic_choice == \"none\":\n",
    "#                 config_name = \"No augmentation\"\n",
    "#             else:\n",
    "#                 config_name = f\"{synthetic_choice} {word_selection} {n_words}\"\n",
    "            \n",
    "#             results_list.append({\n",
    "#                 \"Configuration\": config_name,\n",
    "#                 \"Base Exact Match\": f\"{result['base_exact_match']:.3f}\",\n",
    "#                 \"Base F1\": f\"{result['base_f1']:.3f}\",\n",
    "#                 \"Filtered Exact Match\": f\"{result['filtered_exact_match']:.3f}\",\n",
    "#                 \"Filtered F1\": f\"{result['filtered_f1']:.3f}\",\n",
    "#                 \"Filter Precision\": f\"{result['filter_precision']:.1%}\",\n",
    "#                 \"False Rejection Rate\": f\"{result['false_rejection_rate']:.1%}\",\n",
    "#                 \"Rejections\": result['rejection_count'],\n",
    "#                 \"False Rejections\": result['false_rejection_count']\n",
    "#             })\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error evaluating {synthetic_choice} {word_selection} {n_words}: {e}\")\n",
    "#         import traceback\n",
    "#         traceback.print_exc()\n",
    "\n",
    "# # Create and display the table\n",
    "# if results_list:\n",
    "#     results_df = pd.DataFrame(results_list)\n",
    "#     print(\"\\n\" + \"=\"*80)\n",
    "#     print(\"SYNTHETIC DATA AUGMENTATION COMPARISON TABLE\")\n",
    "#     print(\"=\"*80)\n",
    "#     display(results_df)\n",
    "    \n",
    "#     # Also save to CSV\n",
    "#     output_file = os.path.join(DATA_FOLDER, \"augmentation_comparison_table.csv\")\n",
    "#     results_df.to_csv(output_file, index=False)\n",
    "#     print(f\"\\nTable saved to: {output_file}\")\n",
    "# else:\n",
    "#     print(\"No results to display.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd169c86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
