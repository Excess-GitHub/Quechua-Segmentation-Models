{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac9a685",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SEGMENTER-MORFESSOR: BILSTM WITH MORFESSOR ENSEMBLE PRIORS\n",
    "===========================================================\n",
    "\n",
    "This notebook implements a grapheme-level BiLSTM model for morphological segmentation\n",
    "of Quechua words that uses Morfessor ensemble models as an additional feature channel.\n",
    "Unlike segmenter.ipynb which only uses BiLSTM, this notebook integrates unsupervised\n",
    "Morfessor segmentations as boundary probability features.\n",
    "\n",
    "Key Features:\n",
    "- Grapheme-level tokenization (recognizes Quechua multigraphs like \"ch\", \"ll\", \"rr\")\n",
    "- BiLSTM architecture with additional Morfessor feature channel\n",
    "- Morfessor ensemble: trains N differently-seeded Morfessor Baseline models\n",
    "- Boundary probabilities from Morfessor ensemble are used as features alongside embeddings\n",
    "- Binary classification: predicts boundary (1) or no boundary (0) at each grapheme position\n",
    "- Comprehensive evaluation metrics (precision, recall, F1, exact match, split-count accuracy)\n",
    "- Model checkpointing to avoid redundant training (saves both BiLSTM and Morfessor models)\n",
    "\n",
    "Key Differences from segmenter.ipynb:\n",
    "- Adds Morfessor ensemble boundary probabilities as an additional feature channel\n",
    "- The BiLSTM model concatenates embeddings with Morfessor features before LSTM processing\n",
    "- Morfessor models are trained unsupervised on word surface forms\n",
    "- Both Morfessor ensemble and BiLSTM models are saved/loaded for reproducibility\n",
    "\n",
    "All data is read from the 'data' folder and models are saved to the 'models_segmenter-morfessor' folder.\n",
    "\"\"\"\n",
    "\n",
    "import ast\n",
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4be2176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# DATA FOLDER CONFIGURATION\n",
    "# =========================\n",
    "# All data files should be read from and saved to the data folder\n",
    "DATA_FOLDER = \"data\"\n",
    "\n",
    "# Model folder named after this notebook\n",
    "MODEL_NAME = \"segmenter-morfessor\"\n",
    "MODELS_FOLDER = f\"models_{MODEL_NAME}\"\n",
    "\n",
    "# Create folders if they don't exist\n",
    "os.makedirs(DATA_FOLDER, exist_ok=True)\n",
    "os.makedirs(MODELS_FOLDER, exist_ok=True)\n",
    "\n",
    "# =========================\n",
    "# LOAD GOLD STANDARD DATA\n",
    "# =========================\n",
    "# The gold standard dataset contains high-quality morphological segmentations\n",
    "# This is the base training data for the BiLSTM+Morfessor model\n",
    "print(\"Loading gold standard data...\")\n",
    "gold_df = pd.read_parquet(os.path.join(DATA_FOLDER, \"Sue_kalt.parquet\"))\n",
    "gold_df['Word'] = gold_df['word']\n",
    "gold_df['morph'] = gold_df['morph'].str.replace('-', ' ')  # Normalize separators\n",
    "gold_df['Morph_split_str'] = gold_df['morph']  # String version\n",
    "gold_df['Morph_split'] = gold_df['morph'].str.split(' ')  # List version\n",
    "gold_df = gold_df[['Word', 'Morph_split', 'Morph_split_str']]\n",
    "gold_df = gold_df.drop_duplicates(subset=['Word']).reset_index(drop=True)\n",
    "gold_df = gold_df.dropna(subset=['Word'])\n",
    "print(f\"Loaded {len(gold_df):,} gold standard examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32002a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79117e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2e2953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%  (put this near your imports)\n",
    "import unicodedata, regex as re\n",
    "import string\n",
    "\n",
    "# >>> CHANGED: unify apostrophes to a single codepoint for ejectives, etc.\n",
    "APOSTROPHE_CHARS = {\"'\", \"’\", \"ʼ\", \"‛\", \"`\"}\n",
    "STD_APOS = \"\\u02BC\"  # ʼ\n",
    "\n",
    "# build a translation table that deletes punctuation\n",
    "_EXTRA_PUNCT = \"±，“”‘’\"   # add any more special symbols you want stripped\n",
    "_DELETE = str.maketrans(\"\", \"\", string.punctuation + _EXTRA_PUNCT)\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    # NFC compose; lowercase; unify apostrophes\n",
    "    s = unicodedata.normalize(\"NFC\", str(s)).lower()\n",
    "    s = \"\".join(STD_APOS if ch in APOSTROPHE_CHARS else ch for ch in s)\n",
    "    # remove punctuation (ASCII + extras) and strip whitespace\n",
    "    s = s.translate(_DELETE).strip()\n",
    "    return s\n",
    "\n",
    "# >>> CHANGED: Quechua multigraph inventory (extend if your corpus has more)\n",
    "QUECHUA_MULTIGRAPHS = [\n",
    "    \"ch\"+STD_APOS, \"k\"+STD_APOS, \"p\"+STD_APOS, \"q\"+STD_APOS, \"t\"+STD_APOS,  # ejectives (optional)\n",
    "    \"ch\", \"ph\", \"qh\", \"kh\", \"ll\", \"rr\", \"sh\",\n",
    "]\n",
    "MG_SET = set(QUECHUA_MULTIGRAPHS)\n",
    "MAX_MG = max((len(mg) for mg in QUECHUA_MULTIGRAPHS), default=1)\n",
    "\n",
    "def to_graphemes_quechua(s: str) -> list[str]:\n",
    "    \"\"\"Greedy longest-match multigraph fusion; fallback to Unicode grapheme clusters (\\X).\"\"\"\n",
    "    s = normalize_text(s)\n",
    "    tokens, i, n = [], 0, len(s)\n",
    "    while i < n:\n",
    "        match = None\n",
    "        for L in range(MAX_MG, 1, -1):\n",
    "            if i + L <= n:\n",
    "                cand = s[i:i+L]\n",
    "                if cand in MG_SET:\n",
    "                    match = cand\n",
    "                    break\n",
    "        if match:\n",
    "            tokens.append(match)\n",
    "            i += len(match)\n",
    "        else:\n",
    "            m = re.match(r\"\\X\", s[i:])  # single Unicode grapheme cluster\n",
    "            g = m.group(0)\n",
    "            tokens.append(g)\n",
    "            i += len(g)\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dcfa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# GRAPHEME TOKENIZATION\n",
    "# =========================\n",
    "# Convert words and morphemes to grapheme sequences\n",
    "# This recognizes Quechua multigraphs (e.g., \"ch\", \"ll\", \"rr\", \"sh\") as single tokens\n",
    "# The tokenization is used for both input to the model and for computing boundary labels\n",
    "\n",
    "gold_df['token_seq'] = gold_df['Word'].apply(lambda w: to_graphemes_quechua(w))\n",
    "gold_df['morph_token_splits'] = gold_df['Morph_split'].apply(\n",
    "    lambda var: [to_graphemes_quechua(m) for m in var]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d57829",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b95d7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_boundary_labels_tokens(tokens: list[str], morph_tokens: list[list[str]]) -> list[int]:\n",
    "    labels = [0] * len(tokens)\n",
    "    idx = 0\n",
    "    # all but last morpheme end in a boundary\n",
    "    for mt in morph_tokens[:-1]:\n",
    "        idx += len(mt)\n",
    "        if 0 < idx <= len(tokens):\n",
    "            labels[idx-1] = 1\n",
    "    return labels\n",
    "\n",
    "gold_df['boundary_labels'] = gold_df.apply(\n",
    "    lambda row: get_boundary_labels_tokens(row['token_seq'], row['morph_token_splits']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# (Optional diagnostics)\n",
    "gold_df['num_morphemes'] = gold_df['Morph_split'].apply(len)\n",
    "gold_df['word_len_tokens'] = gold_df['token_seq'].apply(len)\n",
    "\n",
    "gold_df['char_seq'] = gold_df['token_seq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff66add6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# MORFESSOR ENSEMBLE BOUNDARY FEATURIZER\n",
    "# =========================\n",
    "# Uses N differently-seeded Morfessor Baseline models; each gives a hard segmentation.\n",
    "# Boundary *probability* at position t is the fraction of models that place a boundary at t.\n",
    "# Maps boundaries to your tokenization from `to_graphemes_quechua(...)`.\n",
    "# \n",
    "# The Morfessor ensemble provides unsupervised boundary probabilities that serve as\n",
    "# an additional feature channel for the BiLSTM model, helping it learn better\n",
    "# segmentation patterns by leveraging unsupervised morphological analysis.\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple\n",
    "import random\n",
    "import morfessor\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "@dataclass\n",
    "class MorfessorConfig:\n",
    "    n_models: int = 5\n",
    "    seed_base: int = 123\n",
    "    corpus_min_count: int = 1   # keep all words\n",
    "    lowercase: bool = True\n",
    "\n",
    "class MorfessorBoundaryFeaturizer:\n",
    "    def __init__(self, cfg: MorfessorConfig):\n",
    "        self.cfg = cfg\n",
    "        self.models = []\n",
    "        self._fitted = False\n",
    "\n",
    "    def _build_model(self, seed: int):\n",
    "        # Baseline trainer\n",
    "        io = morfessor.MorfessorIO()\n",
    "        model = morfessor.BaselineModel()\n",
    "        # baseline config tweaks (optional): cost weights etc.\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        return model\n",
    "\n",
    "    def fit(self, words: List[str]):\n",
    "        # Train N models with different random shuffles / seeds\n",
    "        words = [w.lower() if self.cfg.lowercase else w for w in words]\n",
    "        uniq = list(set(words))\n",
    "        rng = np.random.default_rng(self.cfg.seed_base)\n",
    "        self.models = []\n",
    "\n",
    "        for i in range(self.cfg.n_models):\n",
    "            seed = self.cfg.seed_base + i\n",
    "            model = self._build_model(seed)\n",
    "            # shuffle corpus\n",
    "            shuffled = uniq.copy()\n",
    "            rng.shuffle(shuffled)\n",
    "            # train: Morfessor expects (frequency, word) tuples\n",
    "            # use integer counts; also drop the count_modifier so ints stay ints\n",
    "            train_data = [(1, w) for w in shuffled]  # uniform integer frequency\n",
    "            model.load_data(train_data)\n",
    "            model.train_batch()  # default algorithm='recursive' is fine\n",
    "            # (optional) you can also try: model.train_batch(algorithm='viterbi')\n",
    "            self.models.append(model)\n",
    "\n",
    "        self._fitted = True\n",
    "\n",
    "    @staticmethod\n",
    "    def _boundaries_from_segments(word: str, segments: List[str]) -> List[int]:\n",
    "        \"\"\"Return hard boundary vector over raw characters: 1 at segment ends (except final).\"\"\"\n",
    "        # mark char positions at the end of each segment\n",
    "        b = [0] * len(word)\n",
    "        pos = 0\n",
    "        for seg_i, seg in enumerate(segments[:-1]):  # all but last seg end\n",
    "            pos += len(seg)\n",
    "            if 0 <= pos - 1 < len(word):\n",
    "                b[pos - 1] = 1\n",
    "        return b\n",
    "\n",
    "    @staticmethod\n",
    "    def _char_to_token_boundaries(word: str, tokens: List[str], char_bound: List[int]) -> List[float]:\n",
    "        \"\"\"\n",
    "        Map char-level boundaries to your tokenization.\n",
    "        We reconstruct the normalized word from tokens to ensure lengths match.\n",
    "        \"\"\"\n",
    "        # Rebuild the normalized string from tokens; this is what your model actually sees.\n",
    "        rebuilt = \"\".join(tokens)\n",
    "        if len(rebuilt) != len(char_bound):\n",
    "            # Fallback: if the original word length differs, recompute hard boundaries on `rebuilt`\n",
    "            # by resegmenting `rebuilt` so positions align.\n",
    "            # In practice, you can also just pad/truncate; here we do a safe clamp.\n",
    "            char_bound = (char_bound[:len(rebuilt)] + [0]*(len(rebuilt) - len(char_bound)))[:len(rebuilt)]\n",
    "\n",
    "        token_ends = []\n",
    "        p = 0\n",
    "        for tok in tokens:\n",
    "            p += len(tok)\n",
    "            token_ends.append(p - 1)  # last char index for this token\n",
    "\n",
    "        probs = [0.0] * len(tokens)\n",
    "        for t_i, char_end in enumerate(token_ends[:-1]):  # all but last token may host a boundary\n",
    "            probs[t_i] = float(char_bound[char_end])\n",
    "        return probs\n",
    "\n",
    "    def boundary_probs_for_word(self, word: str, tokens: List[str]) -> List[float]:\n",
    "        assert self._fitted, \"Call fit() first.\"\n",
    "        word_ = word.lower() if self.cfg.lowercase else word\n",
    "\n",
    "        # collect hard boundary vectors from each model, map to token space, then average\n",
    "        per_model_probs = []\n",
    "        for m in self.models:\n",
    "            segs, _ = m.viterbi_segment(word_)\n",
    "            char_b = self._boundaries_from_segments(word_, segs)\n",
    "            per_model_probs.append(self._char_to_token_boundaries(word_, tokens, char_b))\n",
    "\n",
    "        # average across ensemble\n",
    "        # (lengths match tokens; if any model failed weirdly, fall back to zeros)\n",
    "        if not per_model_probs:\n",
    "            return [0.0] * len(tokens)\n",
    "        probs = np.mean(np.array(per_model_probs), axis=0).tolist()\n",
    "        return probs\n",
    "\n",
    "    def boundary_probs_for_words(self, words: List[str], tok_lists: List[List[str]]) -> List[List[float]]:\n",
    "        return [self.boundary_probs_for_word(w, toks) for w, toks in zip(words, tok_lists)]\n",
    "\n",
    "# ====== DATASET WITH MORFESSOR CHANNEL ======\n",
    "class CharBoundaryDatasetMorf(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, morf_featurizer: MorfessorBoundaryFeaturizer, stoi):\n",
    "        self.words = df[\"Word\"].tolist()\n",
    "        self.x = df[\"char_seq\"].tolist()             # tokens from your pipeline\n",
    "        self.y = df[\"boundary_labels\"].tolist()\n",
    "        # precompute morfessor boundary probabilities per word\n",
    "        self.morf = morf_featurizer.boundary_probs_for_words(\n",
    "            self.words, self.x\n",
    "        )\n",
    "\n",
    "        self.stoi = stoi\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # returns tokens, labels, morf_probs\n",
    "        return self.x[idx], self.y[idx], self.morf[idx]\n",
    "\n",
    "def pad_batch_with_morf(batch, pad_id=0):\n",
    "    # batch: List[(List[str], List[int], List[float])]\n",
    "    seqs, labels, morf = zip(*batch)\n",
    "\n",
    "    x_ids = [[stoi.get(t, stoi[\"<UNK>\"]) for t in s] for s in seqs]\n",
    "    y_ids = [lab for lab in labels]\n",
    "    m_probs = [mp for mp in morf]\n",
    "\n",
    "    lengths = [len(x) for x in x_ids]\n",
    "    maxlen = max(lengths)\n",
    "\n",
    "    x_pad = [xi + [pad_id]*(maxlen - len(xi)) for xi in x_ids]\n",
    "    y_pad = [yi + [0]*(maxlen - len(yi)) for yi in y_ids]        # masked as 0\n",
    "    m_pad = [mi + [0.0]*(maxlen - len(mi)) for mi in m_probs]\n",
    "    mask  = [[1]*len(xi) + [0]*(maxlen - len(xi)) for xi in x_ids]\n",
    "\n",
    "    return (\n",
    "        torch.LongTensor(x_pad),          # x\n",
    "        torch.FloatTensor(y_pad),         # targets\n",
    "        torch.BoolTensor(mask),           # mask\n",
    "        torch.LongTensor(lengths),        # lengths\n",
    "        torch.FloatTensor(m_pad).unsqueeze(-1),  # morf channel: (B,T,1)\n",
    "    )\n",
    "\n",
    "# ====== BiLSTM with extra feature channel ======\n",
    "class BiLSTMBoundaryWithMorf(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_dim: int = 16, hidden_size: int = 16,\n",
    "                 num_layers: int = 1, dropout: float = 0.1, freeze_emb: bool = False,\n",
    "                 extra_feat_dim: int = 1):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        if freeze_emb:\n",
    "            for p in self.emb.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        self.in_dim = emb_dim + extra_feat_dim\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.in_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(hidden_size * 2, 1)\n",
    "\n",
    "    def forward(self, x_ids, lengths, extra_feats):\n",
    "        # x_ids: (B,T) Long\n",
    "        # extra_feats: (B,T,extra_feat_dim) float\n",
    "        emb = self.emb(x_ids)                     # (B,T,E)\n",
    "        x_in = torch.cat([emb, extra_feats], -1)  # (B,T,E+F)\n",
    "\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            x_in, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        packed_out, _ = self.lstm(packed)\n",
    "        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)\n",
    "        out = self.dropout(out)\n",
    "        logits = self.out(out).squeeze(-1)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3262b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ---- If you haven't already: ensure you have `gold_df` with columns:\n",
    "# 'char_seq' (List[str]) and 'boundary_labels' (List[int]) ----\n",
    "# e.g., produced by your earlier cells: gold_df['char_seq'] ... boundary labels ...  :contentReference[oaicite:1]{index=1}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# ==== 1) Build character vocab (embedding-friendly, not one-hot) ====\n",
    "# %%  (unchanged API, but works on token lists)\n",
    "PAD, UNK = \"<PAD>\", \"<UNK>\"\n",
    "\n",
    "def build_vocab(seqs: List[List[str]]):\n",
    "    toks = {t for seq in seqs for t in seq}\n",
    "    itos = [PAD, UNK] + sorted(toks)\n",
    "    stoi = {t:i for i,t in enumerate(itos)}\n",
    "    return stoi, itos\n",
    "\n",
    "stoi, itos = build_vocab(gold_df[\"char_seq\"].tolist())\n",
    "\n",
    "def encode(seq: List[str]) -> List[int]:\n",
    "    return [stoi.get(t, stoi[UNK]) for t in seq]\n",
    "\n",
    "def encode_labels(labels: List[int]) -> List[int]:\n",
    "    return labels\n",
    "\n",
    "# =========================\n",
    "# MORFESSOR SAVING/LOADING FUNCTIONS\n",
    "# =========================\n",
    "# Functions to save and load Morfessor ensemble models to avoid retraining\n",
    "\n",
    "def generate_morfessor_id(n_models, seed_base, lowercase):\n",
    "    \"\"\"\n",
    "    Generate a unique identifier for a Morfessor ensemble based on its configuration.\n",
    "    \n",
    "    Args:\n",
    "        n_models: Number of Morfessor models in the ensemble\n",
    "        seed_base: Base seed for random initialization\n",
    "        lowercase: Whether to lowercase words\n",
    "    \n",
    "    Returns:\n",
    "        A string identifier (hash) for the Morfessor ensemble\n",
    "    \"\"\"\n",
    "    params_dict = {\n",
    "        'n_models': n_models,\n",
    "        'seed_base': seed_base,\n",
    "        'lowercase': lowercase\n",
    "    }\n",
    "    params_str = json.dumps(params_dict, sort_keys=True)\n",
    "    morfessor_id = hashlib.md5(params_str.encode()).hexdigest()[:16]\n",
    "    return morfessor_id\n",
    "\n",
    "def save_morfessor_ensemble(morf_featurizer, morfessor_id, models_folder=MODELS_FOLDER):\n",
    "    \"\"\"\n",
    "    Save Morfessor ensemble models to the models folder.\n",
    "    \n",
    "    Args:\n",
    "        morf_featurizer: Trained MorfessorBoundaryFeaturizer\n",
    "        morfessor_id: Unique identifier for this Morfessor ensemble\n",
    "        models_folder: Folder to save models in\n",
    "    \"\"\"\n",
    "    morfessor_dir = os.path.join(models_folder, f\"morfessor_{morfessor_id}\")\n",
    "    os.makedirs(morfessor_dir, exist_ok=True)\n",
    "    \n",
    "    # Save each Morfessor model in the ensemble\n",
    "    for i, model in enumerate(morf_featurizer.models):\n",
    "        model_path = os.path.join(morfessor_dir, f\"morfessor_model_{i}.pkl\")\n",
    "        with open(model_path, 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "    \n",
    "    # Save configuration\n",
    "    config_path = os.path.join(morfessor_dir, \"morfessor_config.json\")\n",
    "    with open(config_path, \"w\") as f:\n",
    "        json.dump({\n",
    "            'n_models': morf_featurizer.cfg.n_models,\n",
    "            'seed_base': morf_featurizer.cfg.seed_base,\n",
    "            'lowercase': morf_featurizer.cfg.lowercase,\n",
    "            'morfessor_id': morfessor_id\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    print(f\"Morfessor ensemble saved to {morfessor_dir}\")\n",
    "    return morfessor_dir\n",
    "\n",
    "def load_morfessor_ensemble(morfessor_id, models_folder=MODELS_FOLDER):\n",
    "    \"\"\"\n",
    "    Load Morfessor ensemble models from the models folder.\n",
    "    \n",
    "    Args:\n",
    "        morfessor_id: Unique identifier for the Morfessor ensemble\n",
    "        models_folder: Folder where models are saved\n",
    "    \n",
    "    Returns:\n",
    "        MorfessorBoundaryFeaturizer instance or None if not found\n",
    "    \"\"\"\n",
    "    morfessor_dir = os.path.join(models_folder, f\"morfessor_{morfessor_id}\")\n",
    "    config_path = os.path.join(morfessor_dir, \"morfessor_config.json\")\n",
    "    \n",
    "    if not os.path.exists(config_path):\n",
    "        return None\n",
    "    \n",
    "    # Load configuration\n",
    "    with open(config_path, \"r\") as f:\n",
    "        config_data = json.load(f)\n",
    "    \n",
    "    # Recreate config and featurizer\n",
    "    m_cfg = MorfessorConfig(\n",
    "        n_models=config_data['n_models'],\n",
    "        seed_base=config_data['seed_base'],\n",
    "        lowercase=config_data['lowercase']\n",
    "    )\n",
    "    morf = MorfessorBoundaryFeaturizer(m_cfg)\n",
    "    \n",
    "    # Load each model in the ensemble\n",
    "    morf.models = []\n",
    "    for i in range(m_cfg.n_models):\n",
    "        model_path = os.path.join(morfessor_dir, f\"morfessor_model_{i}.pkl\")\n",
    "        if os.path.exists(model_path):\n",
    "            with open(model_path, 'rb') as f:\n",
    "                morf.models.append(pickle.load(f))\n",
    "        else:\n",
    "            print(f\"Warning: Morfessor model {i} not found at {model_path}\")\n",
    "            return None\n",
    "    \n",
    "    morf._fitted = True\n",
    "    print(f\"Morfessor ensemble loaded from {morfessor_dir}\")\n",
    "    return morf\n",
    "\n",
    "# =========================\n",
    "# FIT OR LOAD MORFESSOR ENSEMBLE\n",
    "# =========================\n",
    "# Train Morfessor ensemble on training words (unsupervised)\n",
    "# Or load existing ensemble if already trained with same configuration\n",
    "\n",
    "# Morfessor configuration\n",
    "m_cfg = MorfessorConfig(n_models=5, seed_base=123, lowercase=True)\n",
    "\n",
    "# Generate Morfessor ensemble identifier\n",
    "morfessor_id = generate_morfessor_id(m_cfg.n_models, m_cfg.seed_base, m_cfg.lowercase)\n",
    "\n",
    "# Try to load existing Morfessor ensemble\n",
    "print(f\"Checking for existing Morfessor ensemble with ID: {morfessor_id}\")\n",
    "morf = load_morfessor_ensemble(morfessor_id, models_folder=MODELS_FOLDER)\n",
    "\n",
    "if morf is None:\n",
    "    print(f\"No existing Morfessor ensemble found. Training new ensemble...\")\n",
    "    morf = MorfessorBoundaryFeaturizer(m_cfg)\n",
    "    morf.fit(gold_df[\"Word\"].tolist())  # Unsupervised; uses only surface forms\n",
    "    # Save the trained ensemble\n",
    "    save_morfessor_ensemble(morf, morfessor_id, models_folder=MODELS_FOLDER)\n",
    "    print(f\"Morfessor ensemble training complete! Saved with ID: {morfessor_id}\")\n",
    "else:\n",
    "    print(f\"Using existing Morfessor ensemble (ID: {morfessor_id})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455819c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# LOAD TEST DATA\n",
    "# =========================\n",
    "# Load the test/accuracy evaluation dataset\n",
    "print(\"Loading test data...\")\n",
    "acc_df = pd.read_parquet(os.path.join(DATA_FOLDER, \"cleaned_data_df.parquet\"))\n",
    "print(f\"Loaded {len(acc_df):,} test examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c06e128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# TRAIN/VALIDATION SPLIT\n",
    "# =========================\n",
    "# Split data into 90% training and 10% validation\n",
    "# Create datasets with Morfessor features precomputed\n",
    "rng = np.random.default_rng(42)\n",
    "indices = np.arange(len(gold_df))\n",
    "rng.shuffle(indices)\n",
    "split = int(0.9*len(indices))\n",
    "train_idx, val_idx = indices[:split], indices[split:]\n",
    "\n",
    "train_df = gold_df.iloc[train_idx].reset_index(drop=True)\n",
    "val_df   = gold_df.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "print(f\"Training samples: {len(train_df):,}\")\n",
    "print(f\"Validation samples: {len(val_df):,}\")\n",
    "\n",
    "# Create datasets with Morfessor boundary probabilities precomputed\n",
    "train_ds = CharBoundaryDatasetMorf(train_df, morf, stoi)\n",
    "val_ds   = CharBoundaryDatasetMorf(val_df,   morf, stoi)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          collate_fn=pad_batch_with_morf)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          collate_fn=pad_batch_with_morf)\n",
    "\n",
    "# ==== 4) Loss (masked BCEWithLogits) & Optimizer ====\n",
    "# %%  (compute once on train_df)\n",
    "def count_pos_neg(df_):\n",
    "    pos = sum(sum(lbls) for lbls in df_['boundary_labels'])\n",
    "    total = sum(len(seq) for seq in df_['char_seq'])\n",
    "    neg = total - pos\n",
    "    return pos, neg\n",
    "\n",
    "pos, neg = count_pos_neg(gold_df)\n",
    "pos_weight_value = float(neg) / max(float(pos), 1.0)\n",
    "\n",
    "def masked_bce_loss(logits, targets, mask):\n",
    "    loss_fn = nn.BCEWithLogitsLoss(reduction=\"none\",\n",
    "                                   pos_weight=torch.tensor(pos_weight_value, device=logits.device))\n",
    "    loss_per_token = loss_fn(logits, targets) * mask.float()\n",
    "    denom = mask.float().sum().clamp_min(1.0)\n",
    "    return loss_per_token.sum() / denom\n",
    "\n",
    "# ==== 5) Metrics ====\n",
    "def sigmoid(x): return 1 / (1 + torch.exp(-x))\n",
    "\n",
    "def boundary_f1(logits, targets, mask, threshold=0.5):\n",
    "    with torch.no_grad():\n",
    "        probs = torch.sigmoid(logits)\n",
    "        preds = (probs >= threshold).long()\n",
    "        t = targets.long()\n",
    "        m = mask.long()\n",
    "\n",
    "        tp = ((preds == 1) & (t == 1) & (m == 1)).sum().item()\n",
    "        fp = ((preds == 1) & (t == 0) & (m == 1)).sum().item()\n",
    "        fn = ((preds == 0) & (t == 1) & (m == 1)).sum().item()\n",
    "\n",
    "        prec = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        rec  = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1   = 2*prec*rec / (prec + rec) if (prec + rec) > 0 else 0.0\n",
    "        return prec, rec, f1\n",
    "\n",
    "def predict_boundaries_with_morf(words: List[str], model, stoi, morf_featurizer,\n",
    "                                 threshold=0.5, device=device):\n",
    "    model.eval()\n",
    "    token_lists = [to_graphemes_quechua(w) for w in words]\n",
    "\n",
    "    # ids + lengths + mask\n",
    "    x_ids = [[stoi.get(t, stoi[\"<UNK>\"]) for t in toks] for toks in token_lists]\n",
    "    lengths = [len(x) for x in x_ids]\n",
    "    maxlen = max(lengths) if lengths else 0\n",
    "    pad_id = stoi[\"<PAD>\"]\n",
    "    x_pad = [xi + [pad_id]*(maxlen - len(xi)) for xi in x_ids]\n",
    "    mask = [[1]*len(xi) + [0]*(maxlen - len(xi)) for xi in x_ids]\n",
    "\n",
    "    # Morfessor probs per word → pad\n",
    "    morf_probs = morf_featurizer.boundary_probs_for_words(words, token_lists)\n",
    "    morf_pad = [mp + [0.0]*(maxlen - len(mp)) for mp in morf_probs]\n",
    "    morf_feat = torch.FloatTensor(morf_pad).unsqueeze(-1)\n",
    "\n",
    "    x = torch.LongTensor(x_pad).to(device)\n",
    "    lengths_t = torch.LongTensor(lengths).to(device)\n",
    "    mask_t = torch.BoolTensor(mask).to(device)\n",
    "    morf_feat = morf_feat.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(x, lengths_t, morf_feat)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        preds = (probs >= threshold) & mask_t\n",
    "\n",
    "    out = []\n",
    "    for i, L in enumerate(lengths):\n",
    "        out.append(preds[i, :L].int().tolist())\n",
    "    return out\n",
    "\n",
    "def apply_boundaries_tokens(tokens: list[str], boundary_labels: List[int]) -> List[str]:\n",
    "    \"\"\"Reconstruct morphemes from token list and boundary labels, returning strings.\"\"\"\n",
    "    segs, start = [], 0\n",
    "    for i, b in enumerate(boundary_labels):\n",
    "        if b == 1:\n",
    "            segs.append(\"\".join(tokens[start:i+1]))\n",
    "            start = i+1\n",
    "    if start < len(tokens):\n",
    "        segs.append(\"\".join(tokens[start:]))\n",
    "    return segs\n",
    "\n",
    "def normalize_gold_variants(gold_variants):\n",
    "    \"\"\"\n",
    "    Convert gold_variants to a list format, handling numpy arrays and nested structures.\n",
    "    \"\"\"\n",
    "    if gold_variants is None:\n",
    "        return []\n",
    "    \n",
    "    # If it's a numpy array, convert to list\n",
    "    if isinstance(gold_variants, np.ndarray):\n",
    "        gold_variants = gold_variants.tolist()\n",
    "    \n",
    "    # If it's already a list, ensure nested elements are also lists (not numpy arrays)\n",
    "    if isinstance(gold_variants, list):\n",
    "        normalized = []\n",
    "        for variant in gold_variants:\n",
    "            if isinstance(variant, np.ndarray):\n",
    "                normalized.append(variant.tolist())\n",
    "            elif isinstance(variant, list):\n",
    "                # Recursively normalize nested lists\n",
    "                normalized.append([item.tolist() if isinstance(item, np.ndarray) else item for item in variant])\n",
    "            else:\n",
    "                normalized.append(variant)\n",
    "        return normalized\n",
    "    \n",
    "    return []\n",
    "\n",
    "def evaluate_accuracy_morf(df, model, stoi, morf_featurizer, device=\"cpu\", threshold=0.5):\n",
    "    \"\"\"\n",
    "    Accuracy = proportion of words where predicted segmentation == any gold variant,\n",
    "    using the Morfessor feature channel at inference time.\n",
    "    \"\"\"\n",
    "    all_words = df[\"Word\"].tolist()\n",
    "    all_gold  = df[\"Gold\"].tolist()\n",
    "\n",
    "    all_boundaries = predict_boundaries_with_morf(\n",
    "        all_words, model, stoi, morf_featurizer, threshold=threshold, device=device\n",
    "    )\n",
    "\n",
    "    correct = 0\n",
    "    for word, gold_variants, boundary_labels in zip(all_words, all_gold, all_boundaries):\n",
    "        # Normalize gold variants\n",
    "        gold_variants = normalize_gold_variants(gold_variants)\n",
    "        toks = to_graphemes_quechua(word)\n",
    "        predicted = apply_boundaries_tokens(toks, boundary_labels)\n",
    "        if any(predicted == variant for variant in gold_variants):\n",
    "            correct += 1\n",
    "    return correct / len(all_words) if all_words else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3c8af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# BILSTM MODEL CHECKPOINTING FUNCTIONS\n",
    "# =========================\n",
    "# Functions to save and load BiLSTM models to avoid redundant training\n",
    "\n",
    "def generate_model_id(vocab_size, emb_dim, hidden_size, num_layers, dropout, \n",
    "                      freeze_emb, extra_feat_dim, lr, weight_decay, morfessor_id, epochs):\n",
    "    \"\"\"\n",
    "    Generate a unique identifier for a BiLSTM model based on its hyperparameters.\n",
    "    \n",
    "    Args:\n",
    "        vocab_size: Size of the vocabulary\n",
    "        emb_dim: Embedding dimension\n",
    "        hidden_size: LSTM hidden size\n",
    "        num_layers: Number of LSTM layers\n",
    "        dropout: Dropout rate\n",
    "        freeze_emb: Whether embeddings are frozen\n",
    "        extra_feat_dim: Dimension of extra features (Morfessor channel)\n",
    "        lr: Learning rate\n",
    "        weight_decay: Weight decay\n",
    "        morfessor_id: ID of the Morfessor ensemble used\n",
    "        epochs: Number of training epochs\n",
    "    \n",
    "    Returns:\n",
    "        A string identifier (hash) for the model\n",
    "    \"\"\"\n",
    "    params_dict = {\n",
    "        'vocab_size': vocab_size,\n",
    "        'emb_dim': emb_dim,\n",
    "        'hidden_size': hidden_size,\n",
    "        'num_layers': num_layers,\n",
    "        'dropout': dropout,\n",
    "        'freeze_emb': freeze_emb,\n",
    "        'extra_feat_dim': extra_feat_dim,\n",
    "        'lr': lr,\n",
    "        'weight_decay': weight_decay,\n",
    "        'morfessor_id': morfessor_id,\n",
    "        'epochs': epochs\n",
    "    }\n",
    "    params_str = json.dumps(params_dict, sort_keys=True)\n",
    "    model_id = hashlib.md5(params_str.encode()).hexdigest()[:16]\n",
    "    return model_id\n",
    "\n",
    "def save_model_checkpoint(model, stoi, itos, model_id, models_folder=MODELS_FOLDER, \n",
    "                         suffix=\"\", best_metric_value=None):\n",
    "    \"\"\"\n",
    "    Save BiLSTM model checkpoint to the models folder.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained BiLSTM model\n",
    "        stoi: String-to-index vocabulary mapping\n",
    "        itos: Index-to-string vocabulary mapping\n",
    "        model_id: Unique identifier for this model\n",
    "        models_folder: Folder to save models in\n",
    "        suffix: Optional suffix for the checkpoint filename (e.g., \"_best_f1\", \"_best_acc\")\n",
    "        best_metric_value: Optional best metric value to save\n",
    "    \"\"\"\n",
    "    model_dir = os.path.join(models_folder, f\"model_{model_id}\")\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    checkpoint_path = os.path.join(model_dir, f\"bilstm_morfessor{suffix}.pt\")\n",
    "    checkpoint = {\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"stoi\": stoi,\n",
    "        \"itos\": itos,\n",
    "        \"model_id\": model_id,\n",
    "        \"morfessor_id\": morfessor_id\n",
    "    }\n",
    "    if best_metric_value is not None:\n",
    "        checkpoint[\"best_metric_value\"] = best_metric_value\n",
    "    \n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    print(f\"Model checkpoint saved to {checkpoint_path}\")\n",
    "    return checkpoint_path\n",
    "\n",
    "def load_model_checkpoint(model_id, models_folder=MODELS_FOLDER, suffix=\"\"):\n",
    "    \"\"\"\n",
    "    Load BiLSTM model checkpoint from the models folder.\n",
    "    \n",
    "    Args:\n",
    "        model_id: Unique identifier for the model\n",
    "        models_folder: Folder where models are saved\n",
    "        suffix: Optional suffix for the checkpoint filename\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with model_state, stoi, itos, and other saved data, or None if not found\n",
    "    \"\"\"\n",
    "    model_dir = os.path.join(models_folder, f\"model_{model_id}\")\n",
    "    checkpoint_path = os.path.join(model_dir, f\"bilstm_morfessor{suffix}.pt\")\n",
    "    \n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        return None\n",
    "    \n",
    "    # Load checkpoint with weights_only=False to allow numpy objects and other trusted data\n",
    "    # These are our own saved checkpoints, so they're trusted\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=\"cpu\", weights_only=False)\n",
    "    print(f\"Model checkpoint loaded from {checkpoint_path}\")\n",
    "    return checkpoint\n",
    "\n",
    "# =========================\n",
    "# MODEL CONFIGURATION\n",
    "# =========================\n",
    "# Define BiLSTM model hyperparameters\n",
    "# These parameters determine the model architecture and training behavior\n",
    "\n",
    "# Model architecture parameters\n",
    "VOCAB_SIZE = len(itos)\n",
    "EMB_DIM = 64\n",
    "HIDDEN_SIZE = 128\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.3\n",
    "FREEZE_EMB = True  # Set False to fine-tune embeddings\n",
    "EXTRA_FEAT_DIM = 1  # Morfessor feature channel dimension\n",
    "\n",
    "# Training parameters\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "EPOCHS = 20\n",
    "\n",
    "# Generate model ID based on hyperparameters\n",
    "model_id = generate_model_id(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    emb_dim=EMB_DIM,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT,\n",
    "    freeze_emb=FREEZE_EMB,\n",
    "    extra_feat_dim=EXTRA_FEAT_DIM,\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    morfessor_id=morfessor_id,\n",
    "    epochs=EPOCHS\n",
    ")\n",
    "\n",
    "print(f\"Model ID: {model_id}\")\n",
    "print(f\"Using Morfessor ensemble ID: {morfessor_id}\")\n",
    "\n",
    "# Check if model already exists\n",
    "print(f\"Checking for existing model with ID: {model_id}\")\n",
    "checkpoint = load_model_checkpoint(model_id, models_folder=MODELS_FOLDER, suffix=\"_best_f1\")\n",
    "\n",
    "if checkpoint is not None:\n",
    "    print(f\"Found existing model! Loading checkpoint...\")\n",
    "    # Model will be loaded in the training cell\n",
    "    model_loaded = True\n",
    "    saved_stoi = checkpoint[\"stoi\"]\n",
    "    saved_itos = checkpoint[\"itos\"]\n",
    "else:\n",
    "    print(f\"No existing model found. Will train new model.\")\n",
    "    model_loaded = False\n",
    "    saved_stoi = None\n",
    "    saved_itos = None\n",
    "\n",
    "# =========================\n",
    "# CREATE MODEL AND OPTIMIZER\n",
    "# =========================\n",
    "# Initialize BiLSTM model with Morfessor feature channel\n",
    "model = BiLSTMBoundaryWithMorf(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    emb_dim=EMB_DIM,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT,\n",
    "    freeze_emb=FREEZE_EMB,\n",
    "    extra_feat_dim=EXTRA_FEAT_DIM\n",
    ").to(device)\n",
    "\n",
    "# If model was loaded, restore its state\n",
    "if model_loaded:\n",
    "    model.load_state_dict(checkpoint[\"model_state\"])\n",
    "    print(\"Model state restored from checkpoint\")\n",
    "    # Use saved vocabularies if they match\n",
    "    if saved_stoi == stoi and saved_itos == itos:\n",
    "        print(\"Vocabulary matches saved checkpoint\")\n",
    "    else:\n",
    "        print(\"Warning: Vocabulary mismatch with saved checkpoint. Using current vocabulary.\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81b990b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# K-FOLD CROSS-VALIDATION FUNCTION\n",
    "# ===================================================================\n",
    "# This function performs k-fold cross-validation on the training data\n",
    "# It splits the data into k folds and trains/evaluates on each fold\n",
    "# For each fold, it trains a Morfessor ensemble and a BiLSTM model\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def run_kfold_cross_validation(\n",
    "    df,\n",
    "    n_folds=5,\n",
    "    emb_dim=64,\n",
    "    hidden_size=128,\n",
    "    num_layers=2,\n",
    "    dropout=0.3,\n",
    "    freeze_emb=True,\n",
    "    extra_feat_dim=1,\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-4,\n",
    "    morfessor_n_models=5,\n",
    "    morfessor_seed_base=123,\n",
    "    morfessor_lowercase=True,\n",
    "    random_state=42,\n",
    "    device=device\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform k-fold cross-validation on the training data with Morfessor ensemble features.\n",
    "    \n",
    "    Args:\n",
    "        df: Training DataFrame with 'char_seq', 'boundary_labels', and 'Word' columns\n",
    "        n_folds: Number of folds for cross-validation (default: 5)\n",
    "        emb_dim: Embedding dimension\n",
    "        hidden_size: LSTM hidden size\n",
    "        num_layers: Number of LSTM layers\n",
    "        dropout: Dropout rate\n",
    "        freeze_emb: Whether to freeze embeddings\n",
    "        extra_feat_dim: Dimension of extra features (Morfessor channel)\n",
    "        epochs: Number of training epochs per fold\n",
    "        batch_size: Training batch size\n",
    "        lr: Learning rate\n",
    "        weight_decay: Weight decay for optimizer\n",
    "        morfessor_n_models: Number of Morfessor models in ensemble\n",
    "        morfessor_seed_base: Base seed for Morfessor ensemble\n",
    "        morfessor_lowercase: Whether to lowercase words for Morfessor\n",
    "        random_state: Random seed for reproducibility\n",
    "        device: Device to run training on\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing:\n",
    "        - fold_results: List of results for each fold\n",
    "        - mean_metrics: Average metrics across all folds\n",
    "        - std_metrics: Standard deviation of metrics across folds\n",
    "        - best_fold_idx: Index of the fold with best F1 score\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"K-FOLD CROSS-VALIDATION (k={n_folds}) WITH MORFESSOR ENSEMBLE\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Create k-fold splitter\n",
    "    kfold = KFold(n_splits=n_folds, shuffle=True, random_state=random_state)\n",
    "    indices = np.arange(len(df))\n",
    "    \n",
    "    fold_results = []\n",
    "    all_metrics = {\n",
    "        'boundary_precision': [],\n",
    "        'boundary_recall': [],\n",
    "        'boundary_f1': [],\n",
    "        'val_loss': []\n",
    "    }\n",
    "    \n",
    "    # Train and evaluate on each fold\n",
    "    for fold_idx, (train_indices, val_indices) in enumerate(kfold.split(indices), 1):\n",
    "        print(f\"\\n{'─'*80}\")\n",
    "        print(f\"FOLD {fold_idx}/{n_folds}\")\n",
    "        print(f\"{'─'*80}\")\n",
    "        print(f\"Train samples: {len(train_indices)}, Validation samples: {len(val_indices)}\")\n",
    "        \n",
    "        # Split data into train and validation\n",
    "        train_df_fold = df.iloc[train_indices].reset_index(drop=True)\n",
    "        val_df_fold = df.iloc[val_indices].reset_index(drop=True)\n",
    "        \n",
    "        # Build vocabulary from training fold only\n",
    "        stoi_fold, itos_fold = build_vocab(train_df_fold[\"char_seq\"].tolist())\n",
    "        vocab_size = len(itos_fold)\n",
    "        \n",
    "        # Train Morfessor ensemble on training fold only\n",
    "        print(f\"  Training Morfessor ensemble on fold {fold_idx}...\")\n",
    "        m_cfg_fold = MorfessorConfig(\n",
    "            n_models=morfessor_n_models,\n",
    "            seed_base=morfessor_seed_base + fold_idx,  # Different seed per fold\n",
    "            lowercase=morfessor_lowercase\n",
    "        )\n",
    "        morf_fold = MorfessorBoundaryFeaturizer(m_cfg_fold)\n",
    "        morf_fold.fit(train_df_fold[\"Word\"].tolist())  # Train on training fold words only\n",
    "        print(f\"  Morfessor ensemble trained (n_models={morfessor_n_models})\")\n",
    "        \n",
    "        # Compute pos_weight for loss function from training fold\n",
    "        pos_fold, neg_fold = count_pos_neg(train_df_fold)\n",
    "        pos_weight_value_fold = float(neg_fold) / max(float(pos_fold), 1.0)\n",
    "        \n",
    "        # Create fold-specific collate function that uses the correct stoi\n",
    "        def pad_batch_with_morf_fold(batch, pad_id=0):\n",
    "            # batch: List[(List[str], List[int], List[float])]\n",
    "            seqs, labels, morf = zip(*batch)\n",
    "            x_ids = [[stoi_fold.get(t, stoi_fold[\"<UNK>\"]) for t in s] for s in seqs]\n",
    "            y_ids = [lab for lab in labels]\n",
    "            m_probs = [mp for mp in morf]\n",
    "            lengths = [len(x) for x in x_ids]\n",
    "            maxlen = max(lengths)\n",
    "            x_pad = [xi + [pad_id]*(maxlen - len(xi)) for xi in x_ids]\n",
    "            y_pad = [yi + [0]*(maxlen - len(yi)) for yi in y_ids]\n",
    "            m_pad = [mi + [0.0]*(maxlen - len(mi)) for mi in m_probs]\n",
    "            mask  = [[1]*len(xi) + [0]*(maxlen - len(xi)) for xi in x_ids]\n",
    "            return (\n",
    "                torch.LongTensor(x_pad),\n",
    "                torch.FloatTensor(y_pad),\n",
    "                torch.BoolTensor(mask),\n",
    "                torch.LongTensor(lengths),\n",
    "                torch.FloatTensor(m_pad).unsqueeze(-1),\n",
    "            )\n",
    "        \n",
    "        # Create datasets with Morfessor features\n",
    "        train_ds_fold = CharBoundaryDatasetMorf(train_df_fold, morf_fold, stoi_fold)\n",
    "        val_ds_fold = CharBoundaryDatasetMorf(val_df_fold, morf_fold, stoi_fold)\n",
    "        train_loader_fold = DataLoader(\n",
    "            train_ds_fold, batch_size=batch_size, shuffle=True,\n",
    "            collate_fn=pad_batch_with_morf_fold\n",
    "        )\n",
    "        val_loader_fold = DataLoader(\n",
    "            val_ds_fold, batch_size=batch_size, shuffle=False,\n",
    "            collate_fn=pad_batch_with_morf_fold\n",
    "        )\n",
    "        \n",
    "        # Create model\n",
    "        model_fold = BiLSTMBoundaryWithMorf(\n",
    "            vocab_size=vocab_size,\n",
    "            emb_dim=emb_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            freeze_emb=freeze_emb,\n",
    "            extra_feat_dim=extra_feat_dim\n",
    "        ).to(device)\n",
    "        \n",
    "        # Create optimizer\n",
    "        optimizer_fold = torch.optim.AdamW(model_fold.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        \n",
    "        # Define loss function for this fold\n",
    "        def masked_bce_loss_fold(logits, targets, mask):\n",
    "            loss_fn = nn.BCEWithLogitsLoss(\n",
    "                reduction=\"none\",\n",
    "                pos_weight=torch.tensor(pos_weight_value_fold, device=logits.device)\n",
    "            )\n",
    "            loss_per_token = loss_fn(logits, targets) * mask.float()\n",
    "            denom = mask.float().sum().clamp_min(1.0)\n",
    "            return loss_per_token.sum() / denom\n",
    "        \n",
    "        # Training loop\n",
    "        best_val_f1 = 0.0\n",
    "        best_val_prec = 0.0\n",
    "        best_val_rec = 0.0\n",
    "        best_val_loss = float('inf')\n",
    "        best_epoch = 0\n",
    "        \n",
    "        for epoch in range(1, epochs+1):\n",
    "            # Training phase\n",
    "            model_fold.train()\n",
    "            total_loss = 0.0\n",
    "            total_tokens = 0\n",
    "            for x, y, mask, lengths, morf_feat in train_loader_fold:\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                mask = mask.to(device)\n",
    "                lengths = lengths.to(device)\n",
    "                morf_feat = morf_feat.to(device)\n",
    "                \n",
    "                logits = model_fold(x, lengths, morf_feat)\n",
    "                loss = masked_bce_loss_fold(logits, y, mask)\n",
    "                \n",
    "                optimizer_fold.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(model_fold.parameters(), 1.0)\n",
    "                optimizer_fold.step()\n",
    "                \n",
    "                total_loss += loss.item() * mask.sum().item()\n",
    "                total_tokens += mask.sum().item()\n",
    "            \n",
    "            train_loss = total_loss / max(total_tokens, 1)\n",
    "            \n",
    "            # Validation phase\n",
    "            model_fold.eval()\n",
    "            val_loss, val_tokens = 0.0, 0\n",
    "            all_prec, all_rec, all_f1 = [], [], []\n",
    "            with torch.no_grad():\n",
    "                for x, y, mask, lengths, morf_feat in val_loader_fold:\n",
    "                    x = x.to(device)\n",
    "                    y = y.to(device)\n",
    "                    mask = mask.to(device)\n",
    "                    lengths = lengths.to(device)\n",
    "                    morf_feat = morf_feat.to(device)\n",
    "                    \n",
    "                    logits = model_fold(x, lengths, morf_feat)\n",
    "                    loss = masked_bce_loss_fold(logits, y, mask)\n",
    "                    val_loss += loss.item() * mask.sum().item()\n",
    "                    val_tokens += mask.sum().item()\n",
    "                    \n",
    "                    p, r, f = boundary_f1(logits, y, mask, threshold=0.5)\n",
    "                    all_prec.append(p)\n",
    "                    all_rec.append(r)\n",
    "                    all_f1.append(f)\n",
    "            \n",
    "            val_loss = val_loss / max(val_tokens, 1)\n",
    "            prec = np.mean(all_prec) if all_prec else 0.0\n",
    "            rec = np.mean(all_rec) if all_rec else 0.0\n",
    "            f1 = np.mean(all_f1) if all_f1 else 0.0\n",
    "            \n",
    "            print(f\"  Epoch {epoch:02d} | train_loss={train_loss:.4f}  val_loss={val_loss:.4f}  P={prec:.3f} R={rec:.3f} F1={f1:.3f}\")\n",
    "            \n",
    "            # Track best validation performance\n",
    "            if f1 > best_val_f1 or (np.isclose(f1, best_val_f1) and val_loss < best_val_loss):\n",
    "                best_val_f1 = f1\n",
    "                best_val_prec = prec\n",
    "                best_val_rec = rec\n",
    "                best_val_loss = val_loss\n",
    "                best_epoch = epoch\n",
    "        \n",
    "        print(f\"\\n  Best epoch: {best_epoch}\")\n",
    "        print(f\"  Best validation: P={best_val_prec:.3f} R={best_val_rec:.3f} F1={best_val_f1:.3f} Loss={best_val_loss:.4f}\")\n",
    "        \n",
    "        # Store fold results\n",
    "        fold_result = {\n",
    "            'fold': fold_idx,\n",
    "            'boundary_precision': best_val_prec,\n",
    "            'boundary_recall': best_val_rec,\n",
    "            'boundary_f1': best_val_f1,\n",
    "            'val_loss': best_val_loss,\n",
    "            'best_epoch': best_epoch\n",
    "        }\n",
    "        fold_results.append(fold_result)\n",
    "        \n",
    "        # Collect metrics for averaging\n",
    "        all_metrics['boundary_precision'].append(best_val_prec)\n",
    "        all_metrics['boundary_recall'].append(best_val_rec)\n",
    "        all_metrics['boundary_f1'].append(best_val_f1)\n",
    "        all_metrics['val_loss'].append(best_val_loss)\n",
    "    \n",
    "    # Calculate mean and std across folds\n",
    "    mean_metrics = {\n",
    "        'boundary_precision': np.mean(all_metrics['boundary_precision']),\n",
    "        'boundary_recall': np.mean(all_metrics['boundary_recall']),\n",
    "        'boundary_f1': np.mean(all_metrics['boundary_f1']),\n",
    "        'val_loss': np.mean(all_metrics['val_loss'])\n",
    "    }\n",
    "    \n",
    "    std_metrics = {\n",
    "        'boundary_precision': np.std(all_metrics['boundary_precision']),\n",
    "        'boundary_recall': np.std(all_metrics['boundary_recall']),\n",
    "        'boundary_f1': np.std(all_metrics['boundary_f1']),\n",
    "        'val_loss': np.std(all_metrics['val_loss'])\n",
    "    }\n",
    "    \n",
    "    # Find best fold (highest F1 score)\n",
    "    best_fold_idx = max(range(len(fold_results)), key=lambda i: fold_results[i]['boundary_f1'])\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"K-FOLD CROSS-VALIDATION SUMMARY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"\\nPer-fold results:\")\n",
    "    for result in fold_results:\n",
    "        print(f\"  Fold {result['fold']}: \"\n",
    "              f\"P={result['boundary_precision']:.3f}, \"\n",
    "              f\"R={result['boundary_recall']:.3f}, \"\n",
    "              f\"F1={result['boundary_f1']:.3f}, \"\n",
    "              f\"Loss={result['val_loss']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nMean ± Std across {n_folds} folds:\")\n",
    "    print(f\"  Boundary Precision: {mean_metrics['boundary_precision']:.3f} ± {std_metrics['boundary_precision']:.3f}\")\n",
    "    print(f\"  Boundary Recall:    {mean_metrics['boundary_recall']:.3f} ± {std_metrics['boundary_recall']:.3f}\")\n",
    "    print(f\"  Boundary F1:        {mean_metrics['boundary_f1']:.3f} ± {std_metrics['boundary_f1']:.3f}\")\n",
    "    print(f\"  Validation Loss:   {mean_metrics['val_loss']:.4f} ± {std_metrics['val_loss']:.4f}\")\n",
    "    print(f\"\\nBest fold: Fold {fold_results[best_fold_idx]['fold']} \"\n",
    "          f\"(F1: {fold_results[best_fold_idx]['boundary_f1']:.3f})\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    return {\n",
    "        'fold_results': fold_results,\n",
    "        'mean_metrics': mean_metrics,\n",
    "        'std_metrics': std_metrics,\n",
    "        'best_fold_idx': best_fold_idx,\n",
    "        'all_metrics': all_metrics\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136c1273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# K-FOLD CROSS-VALIDATION DEMONSTRATION\n",
    "# ===================================================================\n",
    "# This cell demonstrates how to use k-fold cross-validation to evaluate\n",
    "# model performance more robustly by training on multiple train/val splits\n",
    "# Each fold trains its own Morfessor ensemble and BiLSTM model\n",
    "\n",
    "# Run 5-fold cross-validation on the training data\n",
    "kfold_results = run_kfold_cross_validation(\n",
    "    df=gold_df,  # Use the full gold_df for cross-validation\n",
    "    n_folds=5,  # Number of folds\n",
    "    emb_dim=EMB_DIM,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT,\n",
    "    freeze_emb=FREEZE_EMB,\n",
    "    extra_feat_dim=EXTRA_FEAT_DIM,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    morfessor_n_models=m_cfg.n_models,\n",
    "    morfessor_seed_base=m_cfg.seed_base,\n",
    "    morfessor_lowercase=m_cfg.lowercase,\n",
    "    random_state=42,  # Use fixed seed for reproducibility\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# The results dictionary contains:\n",
    "# - fold_results: List of results for each fold\n",
    "# - mean_metrics: Average metrics across all folds\n",
    "# - std_metrics: Standard deviation of metrics across folds\n",
    "# - best_fold_idx: Index of the fold with best F1 score\n",
    "# - all_metrics: Raw metrics from all folds\n",
    "\n",
    "print(\"\\nK-fold cross-validation completed!\")\n",
    "print(f\"Average boundary F1: {kfold_results['mean_metrics']['boundary_f1']:.3f} ± {kfold_results['std_metrics']['boundary_f1']:.3f}\")\n",
    "print(f\"Average boundary precision: {kfold_results['mean_metrics']['boundary_precision']:.3f} ± {kfold_results['std_metrics']['boundary_precision']:.3f}\")\n",
    "print(f\"Average boundary recall: {kfold_results['mean_metrics']['boundary_recall']:.3f} ± {kfold_results['std_metrics']['boundary_recall']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15bf594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# TRAINING LOOP\n",
    "# =========================\n",
    "# Train the BiLSTM model with Morfessor features\n",
    "# Skip training if model was already loaded from checkpoint\n",
    "\n",
    "if model_loaded:\n",
    "    print(\"Model already trained and loaded from checkpoint. Skipping training.\")\n",
    "    print(\"To retrain, delete the model checkpoint or change hyperparameters.\")\n",
    "    # Load best metrics from checkpoint if available\n",
    "    best_val_f1 = checkpoint.get(\"best_metric_value\", 0.0)\n",
    "    best_val_acc = checkpoint.get(\"best_metric_value\", 0.0)\n",
    "    print(f\"Best F1 from checkpoint: {best_val_f1:.4f}\")\n",
    "    print(f\"Best Accuracy from checkpoint: {best_val_acc:.4f}\")\n",
    "else:\n",
    "    print(\"Starting training...\")\n",
    "    best_val_f1  = 0.0\n",
    "    best_val_acc = 0.0\n",
    "\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        # ===== Training Phase =====\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        total_tokens = 0\n",
    "        for x, y, mask, lengths, morf_feat in train_loader:\n",
    "            x = x.to(device); y = y.to(device); mask = mask.to(device)\n",
    "            lengths = lengths.to(device); morf_feat = morf_feat.to(device)\n",
    "\n",
    "            # Forward pass: model predicts boundary probabilities\n",
    "            logits = model(x, lengths, morf_feat)\n",
    "            loss = masked_bce_loss(logits, y, mask)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * mask.sum().item()\n",
    "            total_tokens += mask.sum().item()\n",
    "\n",
    "        train_loss = total_loss / max(total_tokens, 1)\n",
    "\n",
    "        # ===== Validation Phase =====\n",
    "        # Evaluate on validation set (F1 on token-level boundaries)\n",
    "        model.eval()\n",
    "        val_loss, val_tokens = 0.0, 0\n",
    "        all_prec, all_rec, all_f1 = [], [], []\n",
    "        with torch.no_grad():\n",
    "            for x, y, mask, lengths, morf_feat in val_loader:\n",
    "                x = x.to(device); y = y.to(device); mask = mask.to(device)\n",
    "                lengths = lengths.to(device); morf_feat = morf_feat.to(device)\n",
    "\n",
    "                logits = model(x, lengths, morf_feat)\n",
    "                loss = masked_bce_loss(logits, y, mask)\n",
    "\n",
    "                val_loss += loss.item() * mask.sum().item()\n",
    "                val_tokens += mask.sum().item()\n",
    "\n",
    "                # Compute token-level boundary metrics\n",
    "                p, r, f = boundary_f1(logits, y, mask, threshold=0.5)\n",
    "                all_prec.append(p); all_rec.append(r); all_f1.append(f)\n",
    "\n",
    "        val_loss = val_loss / max(val_tokens, 1)\n",
    "        prec = np.mean(all_prec) if all_prec else 0.0\n",
    "        rec  = np.mean(all_rec) if all_rec else 0.0\n",
    "        f1   = np.mean(all_f1) if all_f1 else 0.0\n",
    "\n",
    "        # ===== Whole-Word Segmentation Accuracy =====\n",
    "        # Evaluate on test set using Morfessor-aware prediction\n",
    "        acc = evaluate_accuracy_morf(acc_df, model, stoi, morf, device=device, threshold=0.5)\n",
    "\n",
    "        print(f\"Epoch {epoch:02d} | train_loss={train_loss:.4f}  val_loss={val_loss:.4f}  \"\n",
    "              f\"P={prec:.3f} R={rec:.3f} F1={f1:.3f}  Acc={acc:.3f}\")\n",
    "\n",
    "        # ===== Save Best Models =====\n",
    "        # Save checkpoint for best F1 score\n",
    "        if f1 > best_val_f1:\n",
    "            best_val_f1 = f1\n",
    "            save_model_checkpoint(\n",
    "                model, stoi, itos, model_id, \n",
    "                models_folder=MODELS_FOLDER, \n",
    "                suffix=\"_best_f1\",\n",
    "                best_metric_value=best_val_f1\n",
    "            )\n",
    "            print(\"  ↳ saved checkpoint by F1 (best so far)\")\n",
    "\n",
    "        # Save checkpoint for best Accuracy\n",
    "        if acc > best_val_acc:\n",
    "            best_val_acc = acc\n",
    "            save_model_checkpoint(\n",
    "                model, stoi, itos, model_id,\n",
    "                models_folder=MODELS_FOLDER,\n",
    "                suffix=\"_best_acc\",\n",
    "                best_metric_value=best_val_acc\n",
    "            )\n",
    "            print(\"  ↳ saved checkpoint by Accuracy (best so far)\")\n",
    "\n",
    "    print(f\"\\nTraining complete!\")\n",
    "    print(f\"Best validation F1: {best_val_f1:.4f}\")\n",
    "    print(f\"Best validation Accuracy: {best_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348fca95",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_words = [\"rikuchkani\", \"pikunas\", \"ñichkanchus\"]\n",
    "pred_b = predict_boundaries_with_morf(test_words, model, stoi, morf, threshold=0.5)\n",
    "for w, b in zip(test_words, pred_b):\n",
    "    toks = to_graphemes_quechua(w)\n",
    "    print(w, b, \"->\", apply_boundaries_tokens(toks, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca78ab0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# LOAD TRAINED MODEL CHECKPOINT\n",
    "# =========================\n",
    "# Load the best model checkpoint using the new checkpointing system\n",
    "\n",
    "print(f\"Loading model with ID: {model_id}\")\n",
    "\n",
    "# Try loading best accuracy checkpoint first, then fall back to best F1\n",
    "ckpt = load_model_checkpoint(model_id, models_folder=MODELS_FOLDER, suffix=\"_best_acc\")\n",
    "\n",
    "if ckpt is None:\n",
    "    raise FileNotFoundError(\n",
    "        f\"Model checkpoint not found for model_id: {eval_model_id}\\n\"\n",
    "        f\"Please ensure the model has been trained first, or check that the hyperparameters match.\"\n",
    "    )\n",
    "\n",
    "stoi, itos = ckpt[\"stoi\"], ckpt[\"itos\"]\n",
    "\n",
    "model.load_state_dict(ckpt[\"model_state\"])\n",
    "model.eval()\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# ==== 4) Evaluation ====\n",
    "# ==== 4) Evaluation (token-space boundary precision/recall/F1) ====\n",
    "\n",
    "def boundary_positions_from_labels(labels, L=None):\n",
    "    \"\"\"\n",
    "    Convert per-token boundary labels to boundary positions 0..L-2.\n",
    "    We ignore any 'boundary' at the last token index.\n",
    "    \"\"\"\n",
    "    if not labels:\n",
    "        return set()\n",
    "    if L is None:\n",
    "        L = len(labels)\n",
    "    upto = min(L - 1, len(labels))\n",
    "    return {i for i in range(upto) if labels[i] == 1}\n",
    "\n",
    "def boundary_positions_from_morpheme_tokens(morpheme_token_lists):\n",
    "    \"\"\"\n",
    "    Given a list of morphemes, each as a list of grapheme tokens,\n",
    "    return boundary positions (end-of-morpheme token indices) excluding the last morpheme.\n",
    "    \"\"\"\n",
    "    pos = set()\n",
    "    acc = 0\n",
    "    for k, toks in enumerate(morpheme_token_lists):\n",
    "        acc += len(toks)\n",
    "        if k < len(morpheme_token_lists) - 1:\n",
    "            pos.add(acc - 1)\n",
    "    return pos\n",
    "\n",
    "def prf_from_sets(pred_set, gold_set):\n",
    "    tp = len(pred_set & gold_set)\n",
    "    fp = len(pred_set - gold_set)\n",
    "    fn = len(gold_set - pred_set)\n",
    "\n",
    "    if tp + fp == 0:\n",
    "        precision = 1.0 if (tp + fn == 0) else 0.0\n",
    "    else:\n",
    "        precision = tp / (tp + fp)\n",
    "\n",
    "    if tp + fn == 0:\n",
    "        recall = 1.0 if (tp + fp == 0) else 0.0\n",
    "    else:\n",
    "        recall = tp / (tp + fn)\n",
    "\n",
    "    if precision + recall == 0:\n",
    "        f1 = 1.0 if (tp + fp + fn) == 0 else 0.0\n",
    "    else:\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    return tp, fp, fn, precision, recall, f1\n",
    "\n",
    "def best_variant_metrics_token_space(word_tokens, pred_boundary_labels, gold_variants):\n",
    "    \"\"\"\n",
    "    Compare predicted token-boundaries to each gold variant (converted to token boundaries).\n",
    "    Return metrics for the gold variant that maximizes F1.\n",
    "    \"\"\"\n",
    "    pred_b = boundary_positions_from_labels(pred_boundary_labels, L=len(word_tokens))\n",
    "\n",
    "    best = None\n",
    "    for variant in gold_variants:\n",
    "        # Tokenize each morpheme with the same grapheme tokenizer\n",
    "        variant_token_lists = [to_graphemes_quechua(m) for m in variant]\n",
    "        gold_b = boundary_positions_from_morpheme_tokens(variant_token_lists)\n",
    "        tp, fp, fn, P, R, F1 = prf_from_sets(pred_b, gold_b)\n",
    "        key = (F1, tp, -fn, -fp)  # tie-breakers\n",
    "        if (best is None) or (key > best[0]):\n",
    "            best = (key, gold_b, tp, fp, fn, P, R, F1)\n",
    "\n",
    "    if best is None:\n",
    "        gold_b = set()\n",
    "        tp, fp, fn, P, R, F1 = prf_from_sets(pred_b, gold_b)\n",
    "        return pred_b, gold_b, tp, fp, fn, P, R, F1\n",
    "\n",
    "    _, gold_b, tp, fp, fn, P, R, F1 = best\n",
    "    return pred_b, gold_b, tp, fp, fn, P, R, F1\n",
    "\n",
    "def is_correct_prediction(predicted, gold_variants):\n",
    "    return any(predicted == variant for variant in gold_variants)\n",
    "\n",
    "def split_count_metrics(predicted_segments, gold_variants):\n",
    "    \"\"\"\n",
    "    Returns a dictionary with boolean flags:\n",
    "    - exact: same split count as any gold variant\n",
    "    - plus1: one more split than any gold variant\n",
    "    - minus1: one less split than any gold variant\n",
    "    - plusminus1: within ±1 split difference of any gold variant\n",
    "    \"\"\"\n",
    "    pred_count = len(predicted_segments)\n",
    "    gold_counts = [len(gold) for gold in gold_variants]\n",
    "\n",
    "    exact = any(pred_count == g for g in gold_counts)\n",
    "    plus1 = any(pred_count == g + 1 for g in gold_counts)\n",
    "    minus1 = any(pred_count == g - 1 for g in gold_counts)\n",
    "    plusminus1 = any(abs(pred_count - g) <= 1 for g in gold_counts)\n",
    "\n",
    "    return {\n",
    "        \"Exact\": exact,\n",
    "        \"+1\": plus1,\n",
    "        \"-1\": minus1,\n",
    "        \"±1\": plusminus1\n",
    "    }\n",
    "\n",
    "def normalize_gold_variants(gold_variants):\n",
    "    \"\"\"\n",
    "    Convert gold_variants to a list format, handling numpy arrays and nested structures.\n",
    "    \"\"\"\n",
    "    if gold_variants is None:\n",
    "        return []\n",
    "    \n",
    "    # If it's a numpy array, convert to list\n",
    "    if isinstance(gold_variants, np.ndarray):\n",
    "        gold_variants = gold_variants.tolist()\n",
    "    \n",
    "    # If it's already a list, ensure nested elements are also lists (not numpy arrays)\n",
    "    if isinstance(gold_variants, list):\n",
    "        normalized = []\n",
    "        for variant in gold_variants:\n",
    "            if isinstance(variant, np.ndarray):\n",
    "                normalized.append(variant.tolist())\n",
    "            elif isinstance(variant, list):\n",
    "                # Recursively normalize nested lists\n",
    "                normalized.append([item.tolist() if isinstance(item, np.ndarray) else item for item in variant])\n",
    "            else:\n",
    "                normalized.append(variant)\n",
    "        return normalized\n",
    "    \n",
    "    return []\n",
    "\n",
    "\n",
    "# ----- Batch predict -----\n",
    "all_words = acc_df[\"Word\"].tolist()\n",
    "all_gold = acc_df[\"Gold\"]\n",
    "\n",
    "all_boundaries = predict_boundaries_with_morf(\n",
    "    all_words, model, stoi, morf, threshold=0.5, device=device\n",
    ")\n",
    "\n",
    "records = []\n",
    "micro_tp = micro_fp = micro_fn = 0\n",
    "macro_Ps, macro_Rs, macro_F1s = [], [], []\n",
    "exact_flags = []\n",
    "split_exact_flags = []\n",
    "split_plus1_flags = []\n",
    "split_minus1_flags = []\n",
    "split_pm1_flags = []\n",
    "overlap_flags = []\n",
    "\n",
    "for word, gold_variants, boundary_labels in zip(all_words, all_gold, all_boundaries):\n",
    "    # Normalize gold variants\n",
    "    gold_variants = normalize_gold_variants(gold_variants)\n",
    "    \n",
    "    # Tokenize word into graphemes\n",
    "    toks = to_graphemes_quechua(word)\n",
    "\n",
    "    # Build predicted segmentation from token-level labels\n",
    "    predicted_segments = apply_boundaries_tokens(toks, boundary_labels)\n",
    "\n",
    "    # Exact-match accuracy\n",
    "    correct_exact = is_correct_prediction(predicted_segments, gold_variants)\n",
    "\n",
    "    # (2) Split count metrics\n",
    "    split_metrics = split_count_metrics(predicted_segments, gold_variants)\n",
    "\n",
    "    # (3) Overlap\n",
    "    overlap = correct_exact and split_metrics[\"Exact\"]\n",
    "\n",
    "    # Boundary metrics in token space (pick best gold per word)\n",
    "    pred_b, gold_b_chosen, tp, fp, fn, P, R, F1 = best_variant_metrics_token_space(\n",
    "        toks, boundary_labels, gold_variants\n",
    "    )\n",
    "\n",
    "    records.append({\n",
    "        \"Word\": word,\n",
    "        \"Prediction\": predicted_segments,\n",
    "        \"Gold\": gold_variants,\n",
    "        \"PredBoundaries(tok_idx)\": sorted(pred_b),\n",
    "        \"GoldBoundaries(Chosen tok_idx)\": sorted(gold_b_chosen),\n",
    "        \"TP\": tp, \"FP\": fp, \"FN\": fn,\n",
    "        \"P_word\": P, \"R_word\": R, \"F1_word\": F1,\n",
    "        \"CorrectExactSeg\": correct_exact,\n",
    "        \"CorrectSplitCount\": split_metrics[\"Exact\"],\n",
    "        \"SplitCount+1\": split_metrics[\"+1\"],\n",
    "        \"SplitCount-1\": split_metrics[\"-1\"],\n",
    "        \"SplitCount±1\": split_metrics[\"±1\"],\n",
    "        \"OverlapExactAndSplit\": overlap\n",
    "    })\n",
    "\n",
    "    micro_tp += tp\n",
    "    micro_fp += fp\n",
    "    micro_fn += fn\n",
    "    macro_Ps.append(P)\n",
    "    macro_Rs.append(R)\n",
    "    macro_F1s.append(F1)\n",
    "    exact_flags.append(correct_exact)\n",
    "    split_exact_flags.append(split_metrics[\"Exact\"])\n",
    "    split_plus1_flags.append(split_metrics[\"+1\"])\n",
    "    split_minus1_flags.append(split_metrics[\"-1\"])\n",
    "    split_pm1_flags.append(split_metrics[\"±1\"])\n",
    "    overlap_flags.append(overlap)\n",
    "\n",
    "results_df = pd.DataFrame(records)\n",
    "\n",
    "# Exact segmentation accuracy\n",
    "accuracy = results_df[\"CorrectExactSeg\"].mean()\n",
    "\n",
    "# Micro (global) metrics\n",
    "if micro_tp + micro_fp == 0:\n",
    "    P_micro = 1.0 if micro_tp + micro_fn == 0 else 0.0\n",
    "else:\n",
    "    P_micro = micro_tp / (micro_tp + micro_fp)\n",
    "\n",
    "if micro_tp + micro_fn == 0:\n",
    "    R_micro = 1.0 if micro_tp + micro_fp == 0 else 0.0\n",
    "else:\n",
    "    R_micro = micro_tp / (micro_tp + micro_fn)\n",
    "\n",
    "if P_micro + R_micro == 0:\n",
    "    F1_micro = 1.0 if (micro_tp + micro_fp + micro_fn) == 0 else 0.0\n",
    "else:\n",
    "    F1_micro = 2 * P_micro * R_micro / (P_micro + R_micro)\n",
    "\n",
    "# Macro (average of per-word scores)\n",
    "P_macro = float(pd.Series(macro_Ps).mean()) if macro_Ps else 0.0\n",
    "R_macro = float(pd.Series(macro_Rs).mean()) if macro_Rs else 0.0\n",
    "F1_macro = float(pd.Series(macro_F1s).mean()) if macro_F1s else 0.0\n",
    "\n",
    "exact_accuracy = np.mean(exact_flags)\n",
    "split_exact_acc = np.mean(split_exact_flags)\n",
    "split_plus1_acc = np.mean(split_plus1_flags)\n",
    "split_minus1_acc = np.mean(split_minus1_flags)\n",
    "split_pm1_acc = np.mean(split_pm1_flags)\n",
    "overlap_accuracy = np.mean(overlap_flags)\n",
    "\n",
    "print(\"=== Segmentation and Split Count Metrics ===\")\n",
    "print(f\"Exact segmentation accuracy:  {exact_accuracy:.4f}\")\n",
    "print(f\"Split-count (Exact):          {split_exact_acc:.4f}\")\n",
    "print(f\"Split-count (+1):             {split_plus1_acc:.4f}\")\n",
    "print(f\"Split-count (−1):             {split_minus1_acc:.4f}\")\n",
    "print(f\"Split-count (±1):             {split_pm1_acc:.4f}\")\n",
    "print(f\"Overlap (Exact ∩ Split):      {overlap_accuracy:.4f}\")\n",
    "\n",
    "print(\"Boundary metrics (token space):\")\n",
    "print(f\"  Micro  - P: {P_micro:.4f}  R: {R_micro:.4f}  F1: {F1_micro:.4f}\")\n",
    "print(f\"  Macro  - P: {P_macro:.4f}  R: {R_macro:.4f}  F1: {F1_macro:.4f}\")\n",
    "\n",
    "# Save enriched results\n",
    "results_output_path = os.path.join(DATA_FOLDER, \"bilstm_morfessor_eval_results.csv\")\n",
    "results_df.to_csv(results_output_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd44cd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09375a29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
