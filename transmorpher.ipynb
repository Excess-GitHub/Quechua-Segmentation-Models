{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c8caba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TRANSMORPHER: TRANSFORMER-BASED MORPHOLOGY PARSER\n",
    "==================================================\n",
    "\n",
    "This notebook implements a Transformer encoder-decoder model for morphological segmentation\n",
    "of Quechua words. Unlike segmenter.ipynb which uses BiLSTM for boundary prediction, this\n",
    "notebook uses a sequence-to-sequence Transformer architecture that directly generates\n",
    "segmented words with '+' separators.\n",
    "\n",
    "Key Features:\n",
    "- Transformer encoder-decoder architecture (sequence-to-sequence)\n",
    "- Character-level input/output tokenization\n",
    "- Direct generation of segmented words (e.g., \"pikunas\" -> \"pi+kuna+s\")\n",
    "- Data analysis section: correlation analysis, outlier detection, regression models\n",
    "- Model checkpointing to avoid redundant training\n",
    "- Comprehensive evaluation metrics (precision, recall, F1, exact match, split-count accuracy)\n",
    "\n",
    "Key Differences from segmenter.ipynb:\n",
    "- Uses Transformer architecture instead of BiLSTM\n",
    "- Generates segmented strings directly (with '+' separators) instead of predicting boundaries\n",
    "- Sequence-to-sequence approach rather than sequence labeling\n",
    "- Includes exploratory data analysis (correlation, regression, outlier detection)\n",
    "\n",
    "All data is read from the 'data' folder and models are saved to the 'models_transmorpher' folder.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import ast\n",
    "import json\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from matplotlib import cm\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.linear_model import LinearRegression \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e818ce08",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae7390e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# DATA FOLDER CONFIGURATION\n",
    "# =========================\n",
    "# All data files should be read from and saved to the data folder\n",
    "DATA_FOLDER = \"data\"\n",
    "\n",
    "# Model folder named after this notebook\n",
    "MODEL_NAME = \"transmorpher\"\n",
    "MODELS_FOLDER = f\"models_{MODEL_NAME}\"\n",
    "\n",
    "# Create folders if they don't exist\n",
    "os.makedirs(DATA_FOLDER, exist_ok=True)\n",
    "os.makedirs(MODELS_FOLDER, exist_ok=True)\n",
    "\n",
    "# =========================\n",
    "# LOAD GOLD STANDARD DATA\n",
    "# =========================\n",
    "# The gold standard dataset contains high-quality morphological segmentations\n",
    "# This is the base training data for the Transformer model\n",
    "print(\"Loading gold standard data...\")\n",
    "gold_df = pd.read_parquet(os.path.join(DATA_FOLDER, \"Sue_kalt.parquet\"))\n",
    "gold_df['Word'] = gold_df['word']\n",
    "gold_df['morph'] = gold_df['morph'].str.replace('-', ' ')  # Normalize separators\n",
    "gold_df['Morph_split_str'] = gold_df['morph']  # String version\n",
    "gold_df['Morph_split'] = gold_df['morph'].str.split(' ')  # List version\n",
    "gold_df = gold_df[['Word', 'Morph_split', 'Morph_split_str']]\n",
    "gold_df.drop_duplicates(subset='Word', keep='first', inplace=True)\n",
    "gold_df.dropna(subset=['Word'], inplace=True)\n",
    "print(f\"Loaded {len(gold_df):,} gold standard examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51e078f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cb07d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a219a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_df['num_morphemes'] = gold_df['Morph_split'].apply(len)\n",
    "gold_df['word_len'] = gold_df['Word'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c831bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0766e3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8957e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_data = gold_df.groupby(['word_len', 'num_morphemes']).size().unstack(fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e21e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(heatmap_data, annot=True, fmt='d', cmap='YlGnBu')\n",
    "plt.title('Heatmap of Word Length vs. Morpheme Count')\n",
    "plt.xlabel('Number of Morphemes')\n",
    "plt.ylabel('Word Length (Characters)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dcf624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the two series\n",
    "x = gold_df['word_len']\n",
    "y = gold_df['num_morphemes']\n",
    "\n",
    "# Pearson (linear)\n",
    "pearson_corr, pearson_p = pearsonr(x, y)\n",
    "\n",
    "# Spearman (rank-based)\n",
    "spearman_corr, spearman_p = spearmanr(x, y)\n",
    "\n",
    "print(f\"Pearson correlation: {pearson_corr:.3f} (p={pearson_p:.3e})\")\n",
    "print(f\"Spearman correlation: {spearman_corr:.3f} (p={spearman_p:.3e})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e1f7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_df1 = gold_df.copy()\n",
    "print(\"Original Size: \", gold_df1.shape)\n",
    "\n",
    "X = gold_df1[['word_len']]\n",
    "y = gold_df1['num_morphemes']\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "gold_df1['predicted'] = model.predict(X)\n",
    "gold_df1['residual'] = gold_df1['num_morphemes'] - gold_df1['predicted']\n",
    "\n",
    "std_residual = gold_df1['residual'].std()\n",
    "filtered_df = gold_df1[np.abs(gold_df1['residual']) <= std_residual]\n",
    "print(\"Cleaned Size: \", filtered_df.shape)\n",
    "\n",
    "X_filtered = filtered_df[['word_len']]\n",
    "y_filtered = filtered_df['num_morphemes']\n",
    "\n",
    "model_filtered = LinearRegression()\n",
    "model_filtered.fit(X_filtered, y_filtered)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=filtered_df, x='word_len', y='num_morphemes', alpha=0.5, label='Filtered Data')\n",
    "plt.plot(\n",
    "    X_filtered, \n",
    "    model_filtered.predict(X_filtered), \n",
    "    color='red', \n",
    "    linewidth=2, \n",
    "    label='Regression Line'\n",
    ")\n",
    "plt.title('Optimized Linear Regression: Word Length vs Morpheme Count')\n",
    "plt.xlabel('Word Length (Characters)')\n",
    "plt.ylabel('Number of Morphemes')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc72906",
   "metadata": {},
   "outputs": [],
   "source": [
    "slope = model.coef_[0]\n",
    "intercept = model.intercept_\n",
    "print(f\"Pre_Refined Regression equation: num_morphemes ≈ {slope:.2f} × word_len + {intercept:.2f}\")\n",
    "\n",
    "slope = model_filtered.coef_[0]\n",
    "intercept = model_filtered.intercept_\n",
    "print(f\"Refined Regression equation: num_morphemes ≈ {slope:.2f} × word_len + {intercept:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4a7e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_full = r2_score(y, gold_df1['predicted'])\n",
    "print(f\"R2 (Original): {r2_full:.3f}\")\n",
    "\n",
    "y_pred_filtered = model_filtered.predict(X_filtered)\n",
    "r2_filtered = r2_score(y_filtered, y_pred_filtered)\n",
    "print(f\"R2 (Filtered): {r2_filtered:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535f5940",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_df2 = gold_df.copy()\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(gold_df2[['word_len']], gold_df2['num_morphemes'])\n",
    "\n",
    "gold_df2['predicted_rf'] = rf.predict(gold_df2[['word_len']])\n",
    "gold_df2['residual_rf'] = gold_df2['num_morphemes'] - gold_df2['predicted_rf']\n",
    "\n",
    "mse_full = mean_squared_error(gold_df2['num_morphemes'], gold_df2['predicted_rf'])\n",
    "mae_full = mean_absolute_error(gold_df2['num_morphemes'], gold_df2['predicted_rf'])\n",
    "r2_full = r2_score(gold_df2['num_morphemes'], gold_df2['predicted_rf'])\n",
    "\n",
    "std_residual = gold_df2['residual_rf'].std()\n",
    "filtered_df_rf = gold_df2[np.abs(gold_df2['residual_rf']) <= std_residual].copy()\n",
    "\n",
    "rf_filtered = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "X_filtered = filtered_df_rf[['word_len']]\n",
    "y_filtered = filtered_df_rf['num_morphemes']\n",
    "rf_filtered.fit(X_filtered, y_filtered)\n",
    "\n",
    "filtered_df_rf['predicted_rf'] = rf_filtered.predict(X_filtered)\n",
    "r2_filtered = r2_score(y_filtered, filtered_df_rf['predicted_rf'])\n",
    "mse_filtered = mean_squared_error(y_filtered, filtered_df_rf['predicted_rf'])\n",
    "mae_filtered = mean_absolute_error(y_filtered, filtered_df_rf['predicted_rf'])\n",
    "\n",
    "print(\"Random Forest Evaluation (Before Outlier Removal):\")\n",
    "print(f\"MSE: {mse_full:.3f}\")\n",
    "print(f\"MAE: {mae_full:.3f}\")\n",
    "print(f\"R²:  {r2_full:.3f}\")\n",
    "\n",
    "print(\"Random Forest Evaluation (After Outlier Removal):\")\n",
    "print(f\"MSE: {mse_filtered:.3f}\")\n",
    "print(f\"MAE: {mae_filtered:.3f}\")\n",
    "print(f\"R²:  {r2_filtered:.3f}\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=filtered_df_rf['word_len'], y=filtered_df_rf['num_morphemes'], alpha=0.5, label='Filtered Data')\n",
    "sns.lineplot(x=filtered_df_rf['word_len'], y=filtered_df_rf['predicted_rf'], color='red', label='RF Prediction (Filtered)')\n",
    "plt.xlabel('Word Length (Characters)')\n",
    "plt.ylabel('Number of Morphemes')\n",
    "plt.title('Random Forest Regression (Filtered)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc87067",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(degree=2)\n",
    "X_poly = poly.fit_transform(gold_df2[['word_len']])\n",
    "model_poly = LinearRegression().fit(X_poly, gold_df2['num_morphemes'])\n",
    "preds_poly = model_poly.predict(X_poly)\n",
    "r2_before = r2_score(gold_df2['num_morphemes'], preds_poly)\n",
    "print(f\"Polynomial Regression R2 (Before Filtering): {r2_before:.3f}\")\n",
    "\n",
    "residuals_poly = gold_df2['num_morphemes'] - preds_poly\n",
    "std_resid_poly = residuals_poly.std()\n",
    "mask = np.abs(residuals_poly) <= std_resid_poly\n",
    "filtered_df_poly = gold_df2[mask].copy()\n",
    "\n",
    "X_filtered_poly = poly.fit_transform(filtered_df_poly[['word_len']])\n",
    "model_poly_filtered = LinearRegression().fit(X_filtered_poly, filtered_df_poly['num_morphemes'])\n",
    "preds_filtered = model_poly_filtered.predict(X_filtered_poly)\n",
    "r2_after = r2_score(filtered_df_poly['num_morphemes'], preds_filtered)\n",
    "print(f\"Polynomial Regression R2 (After Filtering):  {r2_after:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c01465",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_outliers = gold_df1[np.abs(gold_df1['residual']) > std_residual]\n",
    "rf_outliers = gold_df2[np.abs(gold_df2['residual_rf']) > std_residual]\n",
    "poly_outliers = gold_df2[np.abs(residuals_poly) > std_resid_poly]\n",
    "\n",
    "all_outliers = pd.concat([linear_outliers, rf_outliers, poly_outliers])\n",
    "all_outliers = all_outliers[['word_len', 'num_morphemes']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e3681b",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_data = gold_df.groupby(['word_len', 'num_morphemes']).size().unstack(fill_value=0)\n",
    "\n",
    "outlier_coords = all_outliers[['word_len', 'num_morphemes']]\n",
    "outlier_coords = outlier_coords[\n",
    "    (outlier_coords['word_len'].isin(heatmap_data.index)) & \n",
    "    (outlier_coords['num_morphemes'].isin(heatmap_data.columns))\n",
    "]\n",
    "outlier_coords = outlier_coords.copy()\n",
    "outlier_coords['freq'] = outlier_coords.apply(\n",
    "    lambda row: heatmap_data.at[row['word_len'], row['num_morphemes']], axis=1\n",
    ")\n",
    "\n",
    "norm = plt.Normalize(vmin=heatmap_data.values.min(), vmax=heatmap_data.values.max())\n",
    "colors = cm.Purples_r(norm(outlier_coords['freq'].values))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.heatmap(heatmap_data, annot=True, fmt='d', cmap='Reds', cbar_kws={'label': 'Frequency'})\n",
    "plt.title('Heatmap of Word Length vs. Morpheme Count with Outliers')\n",
    "plt.xlabel('Number of Morphemes')\n",
    "plt.ylabel('Word Length (Characters)')\n",
    "\n",
    "for j, (_, row) in enumerate(outlier_coords.iterrows()):\n",
    "    plt.scatter(\n",
    "        x=row['num_morphemes'] + 0.5,\n",
    "        y=row['word_len'] + 0.5,\n",
    "        color=colors[j],\n",
    "        s=100,\n",
    "        marker='X',\n",
    "        linewidths=2,\n",
    "        label='Outlier' if j == 0 else \"\"\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695e5421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# PREPARE SEGMENTATION FORMAT\n",
    "# =========================\n",
    "# Convert morpheme splits to segmentation format with '+' separators\n",
    "# This format is used for the Transformer model (sequence-to-sequence)\n",
    "# Example: \"pi kuna s\" -> \"pi+kuna+s\"\n",
    "gold_df['segmentation'] = gold_df['Morph_split_str'].str.replace(' ', '+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5ea0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b19e335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# BUILD VOCABULARIES\n",
    "# =========================\n",
    "# Create character-level vocabularies for input (words) and output (segmented words)\n",
    "# Input vocab: characters in words\n",
    "# Output vocab: characters in segmented words (includes '+' separator)\n",
    "special_tokens = ['<pad>', '<s>', '</s>']  # Padding, start, and end tokens\n",
    "input_chars = sorted({ch for word in gold_df['Word'] for ch in word})\n",
    "output_chars = sorted({ch for seg in gold_df['segmentation'] for ch in seg})\n",
    "input_vocab = special_tokens + input_chars\n",
    "output_vocab = special_tokens + output_chars\n",
    "input2idx = {ch: idx for idx, ch in enumerate(input_vocab)}\n",
    "output2idx = {ch: idx for idx, ch in enumerate(output_vocab)}\n",
    "PAD_IN, START_IN, END_IN = input2idx['<pad>'], input2idx['<s>'], input2idx['</s>']\n",
    "PAD_OUT, START_OUT, END_OUT = output2idx['<pad>'], output2idx['<s>'], output2idx['</s>']\n",
    "\n",
    "# =========================\n",
    "# DATASET CLASS\n",
    "# =========================\n",
    "# PyTorch Dataset for Transformer sequence-to-sequence training\n",
    "# Each sample contains a word and its segmented version\n",
    "class QuechuaSegDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for Quechua morphological segmentation.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with 'Word' and 'segmentation' columns\n",
    "        input2idx: Dictionary mapping input characters to indices\n",
    "        output2idx: Dictionary mapping output characters to indices\n",
    "        max_input_len: Maximum input sequence length (auto-computed if None)\n",
    "        max_output_len: Maximum output sequence length (auto-computed if None)\n",
    "    \"\"\"\n",
    "    def __init__(self, df, input2idx, output2idx, max_input_len=None, max_output_len=None):\n",
    "        self.words = df['Word'].tolist()\n",
    "        self.segs = df['segmentation'].tolist()\n",
    "        self.input2idx = input2idx\n",
    "        self.output2idx = output2idx\n",
    "        # Add 2 for <s> and </s> tokens\n",
    "        self.max_input_len = max_input_len or max(len(w) for w in self.words) + 2\n",
    "        self.max_output_len = max_output_len or max(len(s) for s in self.segs) + 2\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.words)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        word = self.words[idx]\n",
    "        seg = self.segs[idx]\n",
    "        # Encode source: <s> + chars + </s>, pad to max_input_len\n",
    "        src = [self.input2idx.get(ch, PAD_IN) for ch in word]\n",
    "        src = [START_IN] + src + [END_IN]\n",
    "        src += [PAD_IN] * (self.max_input_len - len(src))\n",
    "        # Encode target: <s> + segmented chars + </s>, pad to max_output_len\n",
    "        tgt = [START_OUT] + [self.output2idx[ch] for ch in seg] + [END_OUT]\n",
    "        tgt += [PAD_OUT] * (self.max_output_len - len(tgt))\n",
    "        return torch.tensor(src), torch.tensor(tgt)\n",
    "\n",
    "# =========================\n",
    "# POSITIONAL ENCODING\n",
    "# =========================\n",
    "# Adds positional information to embeddings (required for Transformers)\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Sinusoidal positional encoding for Transformer models.\n",
    "    Adds position-dependent information to token embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(pos * div)  # Even dimensions: sin\n",
    "        pe[:, 1::2] = torch.cos(pos * div)  # Odd dimensions: cos\n",
    "        pe = pe.unsqueeze(1)  # Shape: (max_len, 1, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "    def forward(self, x):\n",
    "        # x shape: (seq_len, batch, d_model)\n",
    "        return x + self.pe[:x.size(0)]\n",
    "\n",
    "# =========================\n",
    "# TRANSFORMER MODEL ARCHITECTURE\n",
    "# =========================\n",
    "# Encoder-decoder Transformer for sequence-to-sequence morphological segmentation\n",
    "class MorphSegModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer encoder-decoder model for morphological segmentation.\n",
    "    \n",
    "    Architecture:\n",
    "    1. Encoder: Processes input word character-by-character\n",
    "    2. Decoder: Generates segmented word character-by-character (with '+' separators)\n",
    "    3. Output projection: Maps decoder hidden states to output vocabulary\n",
    "    \n",
    "    Args:\n",
    "        in_vocab: Size of input vocabulary (characters in words)\n",
    "        out_vocab: Size of output vocabulary (characters in segmented words, including '+')\n",
    "        d_model: Model dimension (embedding size)\n",
    "        ff: Feed-forward network dimension\n",
    "        heads: Number of attention heads\n",
    "        layers: Number of encoder/decoder layers\n",
    "        drop: Dropout rate\n",
    "    \"\"\"\n",
    "    def __init__(self, in_vocab, out_vocab, d_model=64, ff=128, heads=2, layers=1, drop=0.0):\n",
    "        super().__init__()\n",
    "        # Encoder: processes input word\n",
    "        self.enc_embed = nn.Embedding(in_vocab, d_model, padding_idx=PAD_IN)\n",
    "        self.pos_enc = PositionalEncoding(d_model)\n",
    "        enc_layer = nn.TransformerEncoderLayer(d_model, heads, ff, drop, batch_first=False)\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, layers)\n",
    "\n",
    "        # Decoder: generates segmented word\n",
    "        self.dec_embed = nn.Embedding(out_vocab, d_model, padding_idx=PAD_OUT)\n",
    "        self.pos_dec = PositionalEncoding(d_model)\n",
    "        dec_layer = nn.TransformerDecoderLayer(d_model, heads, ff, drop, batch_first=False)\n",
    "        self.decoder = nn.TransformerDecoder(dec_layer, layers)\n",
    "\n",
    "        # Output projection: maps to output vocabulary\n",
    "        self.out_proj = nn.Linear(d_model, out_vocab)\n",
    "\n",
    "    def forward(self, src, tgt, tgt_mask=None, src_key_padding_mask=None, tgt_key_padding_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass through the Transformer.\n",
    "        \n",
    "        Args:\n",
    "            src: Input word tokens (seq_len, batch)\n",
    "            tgt: Target segmented word tokens (seq_len, batch)\n",
    "            tgt_mask: Causal mask for decoder (prevents looking ahead)\n",
    "            src_key_padding_mask: Mask for padding in source\n",
    "            tgt_key_padding_mask: Mask for padding in target\n",
    "        \n",
    "        Returns:\n",
    "            Logits over output vocabulary (seq_len, batch, out_vocab)\n",
    "        \"\"\"\n",
    "        # Encode source word\n",
    "        e_src = self.pos_enc(self.enc_embed(src))\n",
    "        memory = self.encoder(e_src, src_key_padding_mask=src_key_padding_mask)\n",
    "        \n",
    "        # Decode segmented word\n",
    "        e_tgt = self.pos_dec(self.dec_embed(tgt))\n",
    "        out = self.decoder(e_tgt, memory, tgt_mask=tgt_mask,\n",
    "                           memory_key_padding_mask=src_key_padding_mask,\n",
    "                           tgt_key_padding_mask=tgt_key_padding_mask)\n",
    "        \n",
    "        # Project to output vocabulary\n",
    "        return self.out_proj(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7fcd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# DATASET SPLIT AND DATALOADERS\n",
    "# =========================\n",
    "# Split data into 80% training and 20% validation\n",
    "# Create DataLoaders for batching\n",
    "dataset = QuechuaSegDataset(gold_df, input2idx, output2idx)\n",
    "n_train = int(0.8 * len(dataset))\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [n_train, len(dataset) - n_train])\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "print(f\"Training samples: {len(train_dataset):,}\")\n",
    "print(f\"Validation samples: {len(val_dataset):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7ea2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# MODEL CHECKPOINTING FUNCTIONS\n",
    "# =========================\n",
    "# Functions to save and load trained models to avoid retraining\n",
    "\n",
    "def generate_model_id(d_model, ff, heads, layers, drop, epochs, batch_size, lr, in_vocab_size, out_vocab_size):\n",
    "    \"\"\"\n",
    "    Generate a unique identifier for a model based on its training parameters.\n",
    "    \n",
    "    Args:\n",
    "        All training hyperparameters and vocabulary sizes\n",
    "    \n",
    "    Returns:\n",
    "        A string identifier (hash) for the model\n",
    "    \"\"\"\n",
    "    params_dict = {\n",
    "        'd_model': d_model,\n",
    "        'ff': ff,\n",
    "        'heads': heads,\n",
    "        'layers': layers,\n",
    "        'drop': drop,\n",
    "        'epochs': epochs,\n",
    "        'batch_size': batch_size,\n",
    "        'lr': lr,\n",
    "        'in_vocab_size': in_vocab_size,\n",
    "        'out_vocab_size': out_vocab_size\n",
    "    }\n",
    "    params_str = json.dumps(params_dict, sort_keys=True)\n",
    "    model_id = hashlib.md5(params_str.encode()).hexdigest()[:16]\n",
    "    return model_id\n",
    "\n",
    "def save_model_checkpoint(model, input2idx, output2idx, model_id, models_folder=MODELS_FOLDER):\n",
    "    \"\"\"\n",
    "    Save model checkpoint to the models folder.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained MorphSegModel\n",
    "        input2idx: Input vocabulary mapping\n",
    "        output2idx: Output vocabulary mapping\n",
    "        model_id: Unique identifier for this model\n",
    "        models_folder: Folder to save models in\n",
    "    \"\"\"\n",
    "    model_dir = os.path.join(models_folder, model_id)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    checkpoint_path = os.path.join(model_dir, \"transformer_morph_seg.pt\")\n",
    "    torch.save({\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"input2idx\": input2idx,\n",
    "        \"output2idx\": output2idx\n",
    "    }, checkpoint_path)\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata_path = os.path.join(model_dir, \"metadata.json\")\n",
    "    with open(metadata_path, \"w\") as f:\n",
    "        json.dump({\n",
    "            'model_id': model_id,\n",
    "            'in_vocab_size': len(input2idx),\n",
    "            'out_vocab_size': len(output2idx),\n",
    "            'model_name': MODEL_NAME\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    print(f\"Model checkpoint saved to {model_dir}\")\n",
    "    return model_dir\n",
    "\n",
    "def load_model_checkpoint(model_id, models_folder=MODELS_FOLDER):\n",
    "    \"\"\"\n",
    "    Load model checkpoint from the models folder.\n",
    "    \n",
    "    Args:\n",
    "        model_id: Unique identifier for the model\n",
    "        models_folder: Folder where models are saved\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with 'model_state', 'input2idx', 'output2idx', 'checkpoint_path', 'model_dir' or None if not found\n",
    "    \"\"\"\n",
    "    model_dir = os.path.join(models_folder, model_id)\n",
    "    checkpoint_path = os.path.join(model_dir, \"transformer_morph_seg.pt\")\n",
    "    \n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        return None\n",
    "    \n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    print(f\"Model checkpoint loaded from {model_dir}\")\n",
    "    return {\n",
    "        'model_state': checkpoint['model_state'],\n",
    "        'input2idx': checkpoint['input2idx'],\n",
    "        'output2idx': checkpoint['output2idx'],\n",
    "        'checkpoint_path': checkpoint_path,\n",
    "        'model_dir': model_dir\n",
    "    }\n",
    "\n",
    "# =========================\n",
    "# MODEL INITIALIZATION\n",
    "# =========================\n",
    "# Initialize model, loss function, and optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Model hyperparameters\n",
    "D_MODEL = 64\n",
    "FF = 128\n",
    "HEADS = 2\n",
    "LAYERS = 1\n",
    "DROP = 0.0\n",
    "EPOCHS = 15\n",
    "BATCH_SIZE = 16\n",
    "LR = 1e-4\n",
    "\n",
    "# Generate model identifier\n",
    "model_id = generate_model_id(D_MODEL, FF, HEADS, LAYERS, DROP, EPOCHS, BATCH_SIZE, LR, \n",
    "                              len(input_vocab), len(output_vocab))\n",
    "\n",
    "# Try to load existing model\n",
    "print(f\"Checking for existing model with ID: {model_id}\")\n",
    "loaded = load_model_checkpoint(model_id, models_folder=MODELS_FOLDER)\n",
    "\n",
    "if loaded is not None:\n",
    "    print(f\"Found existing model! Loading from {loaded['model_dir']}\")\n",
    "    input2idx = loaded['input2idx']\n",
    "    output2idx = loaded['output2idx']\n",
    "    # Recompute special token indices from loaded vocabularies\n",
    "    PAD_IN, START_IN, END_IN = input2idx['<pad>'], input2idx['<s>'], input2idx['</s>']\n",
    "    PAD_OUT, START_OUT, END_OUT = output2idx['<pad>'], output2idx['<s>'], output2idx['</s>']\n",
    "    model = MorphSegModel(len(input2idx), len(output2idx), d_model=D_MODEL, ff=FF, \n",
    "                          heads=HEADS, layers=LAYERS, drop=DROP).to(device)\n",
    "    model.load_state_dict(loaded['model_state'])\n",
    "    model.eval()\n",
    "    print(\"Model loaded successfully. Skipping training.\")\n",
    "else:\n",
    "    print(f\"No existing model found. Training new model...\")\n",
    "    model = MorphSegModel(len(input_vocab), len(output_vocab), d_model=D_MODEL, ff=FF, \n",
    "                          heads=HEADS, layers=LAYERS, drop=DROP).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=PAD_OUT)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d332d874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# TRAINING LOOP\n",
    "# =========================\n",
    "# Train the Transformer model for specified number of epochs\n",
    "# Only runs if no existing model was found\n",
    "\n",
    "if loaded is None:\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for src_batch, tgt_batch in train_loader:\n",
    "            # 1) Move batch to device\n",
    "            src_batch = src_batch.to(device)           # (batch, src_len)\n",
    "            tgt_batch = tgt_batch.to(device)           # (batch, tgt_len)\n",
    "\n",
    "            # 2) Build padding masks (True for padding tokens)\n",
    "            src_pad_mask = (src_batch == PAD_IN)       # (batch, src_len)\n",
    "            tgt_pad_mask = (tgt_batch == PAD_OUT)     # (batch, tgt_len)\n",
    "\n",
    "            # 3) Transpose to (seq_len, batch) as expected by nn.Transformer\n",
    "            src = src_batch.transpose(0, 1)            # (src_len, batch)\n",
    "            tgt = tgt_batch.transpose(0, 1)            # (tgt_len, batch)\n",
    "            tgt_input = tgt[:-1, :]                    # Input to decoder (shifted)\n",
    "            tgt_output = tgt[1:, :]                    # Target output (shifted)\n",
    "\n",
    "            # 4) Causal mask for decoder (prevents looking at future tokens)\n",
    "            tgt_mask = nn.Transformer.generate_square_subsequent_mask(\n",
    "                           tgt_input.size(0)\n",
    "                       ).to(device)\n",
    "\n",
    "            # 5) Forward pass: predict next token in segmented sequence\n",
    "            logits = model(\n",
    "                src,\n",
    "                tgt_input,\n",
    "                tgt_mask=tgt_mask,\n",
    "                src_key_padding_mask=src_pad_mask,\n",
    "                tgt_key_padding_mask=tgt_pad_mask[:, :-1]\n",
    "            )\n",
    "            \n",
    "            # 6) Compute loss (cross-entropy over output vocabulary)\n",
    "            loss = criterion(\n",
    "                logits.reshape(-1, logits.size(-1)),  # Flatten to (seq*batch, vocab)\n",
    "                tgt_output.reshape(-1)                # Flatten to (seq*batch,)\n",
    "            )\n",
    "\n",
    "            # 7) Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch:02d} — Train Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Save model after training\n",
    "    save_model_checkpoint(model, input2idx, output2idx, model_id, models_folder=MODELS_FOLDER)\n",
    "    print(f\"\\nTraining complete! Model saved with ID: {model_id}\")\n",
    "else:\n",
    "    print(\"Using loaded model. Training skipped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ec4ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# K-FOLD CROSS-VALIDATION FUNCTION\n",
    "# ===================================================================\n",
    "# This function performs k-fold cross-validation on the training data\n",
    "# It splits the data into k folds and trains/evaluates on each fold\n",
    "# For each fold, it trains a Transformer encoder-decoder model\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def run_kfold_cross_validation(\n",
    "    df,\n",
    "    n_folds=5,\n",
    "    d_model=64,\n",
    "    ff=128,\n",
    "    heads=2,\n",
    "    layers=1,\n",
    "    drop=0.0,\n",
    "    epochs=15,\n",
    "    batch_size=16,\n",
    "    lr=1e-4,\n",
    "    random_state=42,\n",
    "    device=device\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform k-fold cross-validation on the training data with Transformer model.\n",
    "    \n",
    "    Args:\n",
    "        df: Training DataFrame with 'Word' and 'segmentation' columns\n",
    "        n_folds: Number of folds for cross-validation (default: 5)\n",
    "        d_model: Model dimension (embedding size)\n",
    "        ff: Feed-forward network dimension\n",
    "        heads: Number of attention heads\n",
    "        layers: Number of encoder/decoder layers\n",
    "        drop: Dropout rate\n",
    "        epochs: Number of training epochs per fold\n",
    "        batch_size: Training batch size\n",
    "        lr: Learning rate\n",
    "        random_state: Random seed for reproducibility\n",
    "        device: Device to run training on\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing:\n",
    "        - fold_results: List of results for each fold\n",
    "        - mean_metrics: Average metrics across all folds\n",
    "        - std_metrics: Standard deviation of metrics across folds\n",
    "        - best_fold_idx: Index of the fold with best validation loss\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"K-FOLD CROSS-VALIDATION (k={n_folds}) WITH TRANSFORMER\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Create k-fold splitter\n",
    "    kfold = KFold(n_splits=n_folds, shuffle=True, random_state=random_state)\n",
    "    indices = np.arange(len(df))\n",
    "    \n",
    "    fold_results = []\n",
    "    all_metrics = {\n",
    "        'val_loss': [],\n",
    "        'exact_match': []\n",
    "    }\n",
    "    \n",
    "    # Train and evaluate on each fold\n",
    "    for fold_idx, (train_indices, val_indices) in enumerate(kfold.split(indices), 1):\n",
    "        print(f\"\\n{'─'*80}\")\n",
    "        print(f\"FOLD {fold_idx}/{n_folds}\")\n",
    "        print(f\"{'─'*80}\")\n",
    "        print(f\"Train samples: {len(train_indices)}, Validation samples: {len(val_indices)}\")\n",
    "        \n",
    "        # Split data into train and validation\n",
    "        train_df_fold = df.iloc[train_indices].reset_index(drop=True)\n",
    "        val_df_fold = df.iloc[val_indices].reset_index(drop=True)\n",
    "        \n",
    "        # Build vocabularies from training fold only\n",
    "        print(f\"  Building vocabularies from training fold...\")\n",
    "        special_tokens = ['<pad>', '<s>', '</s>']\n",
    "        input_chars_fold = sorted({ch for word in train_df_fold['Word'] for ch in word})\n",
    "        output_chars_fold = sorted({ch for seg in train_df_fold['segmentation'] for ch in seg})\n",
    "        input_vocab_fold = special_tokens + input_chars_fold\n",
    "        output_vocab_fold = special_tokens + output_chars_fold\n",
    "        input2idx_fold = {ch: idx for idx, ch in enumerate(input_vocab_fold)}\n",
    "        output2idx_fold = {ch: idx for idx, ch in enumerate(output_vocab_fold)}\n",
    "        PAD_IN_FOLD = input2idx_fold['<pad>']\n",
    "        START_IN_FOLD = input2idx_fold['<s>']\n",
    "        END_IN_FOLD = input2idx_fold['</s>']\n",
    "        PAD_OUT_FOLD = output2idx_fold['<pad>']\n",
    "        START_OUT_FOLD = output2idx_fold['<s>']\n",
    "        END_OUT_FOLD = output2idx_fold['</s>']\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset_fold = QuechuaSegDataset(train_df_fold, input2idx_fold, output2idx_fold)\n",
    "        val_dataset_fold = QuechuaSegDataset(val_df_fold, input2idx_fold, output2idx_fold)\n",
    "        train_loader_fold = DataLoader(train_dataset_fold, batch_size=batch_size, shuffle=True)\n",
    "        val_loader_fold = DataLoader(val_dataset_fold, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        # Create model\n",
    "        model_fold = MorphSegModel(\n",
    "            len(input_vocab_fold), \n",
    "            len(output_vocab_fold), \n",
    "            d_model=d_model, \n",
    "            ff=ff, \n",
    "            heads=heads, \n",
    "            layers=layers, \n",
    "            drop=drop\n",
    "        ).to(device)\n",
    "        \n",
    "        # Create loss function and optimizer\n",
    "        criterion_fold = nn.CrossEntropyLoss(ignore_index=PAD_OUT_FOLD)\n",
    "        optimizer_fold = optim.Adam(model_fold.parameters(), lr=lr, betas=(0.9, 0.98), eps=1e-9)\n",
    "        \n",
    "        # Training loop\n",
    "        best_val_loss = float('inf')\n",
    "        best_epoch = 0\n",
    "        best_exact_match = 0.0\n",
    "        \n",
    "        for epoch in range(1, epochs+1):\n",
    "            # Training phase\n",
    "            model_fold.train()\n",
    "            total_loss = 0.0\n",
    "            for src_batch, tgt_batch in train_loader_fold:\n",
    "                src_batch = src_batch.to(device)\n",
    "                tgt_batch = tgt_batch.to(device)\n",
    "                \n",
    "                src_pad_mask = (src_batch == PAD_IN_FOLD)\n",
    "                tgt_pad_mask = (tgt_batch == PAD_OUT_FOLD)\n",
    "                \n",
    "                src = src_batch.transpose(0, 1)\n",
    "                tgt = tgt_batch.transpose(0, 1)\n",
    "                tgt_input = tgt[:-1, :]\n",
    "                tgt_output = tgt[1:, :]\n",
    "                \n",
    "                tgt_mask = nn.Transformer.generate_square_subsequent_mask(\n",
    "                    tgt_input.size(0)\n",
    "                ).to(device)\n",
    "                \n",
    "                logits = model_fold(\n",
    "                    src,\n",
    "                    tgt_input,\n",
    "                    tgt_mask=tgt_mask,\n",
    "                    src_key_padding_mask=src_pad_mask,\n",
    "                    tgt_key_padding_mask=tgt_pad_mask[:, :-1]\n",
    "                )\n",
    "                \n",
    "                loss = criterion_fold(\n",
    "                    logits.reshape(-1, logits.size(-1)),\n",
    "                    tgt_output.reshape(-1)\n",
    "                )\n",
    "                \n",
    "                optimizer_fold.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer_fold.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            train_loss = total_loss / len(train_loader_fold)\n",
    "            \n",
    "            # Validation phase\n",
    "            model_fold.eval()\n",
    "            val_loss = 0.0\n",
    "            exact_matches = 0\n",
    "            total_val = 0\n",
    "            \n",
    "            # Build reverse vocabulary for inference\n",
    "            idx2output_fold = {idx: ch for ch, idx in output2idx_fold.items()}\n",
    "            \n",
    "            # Define fold-specific segment_word function\n",
    "            def segment_word_fold(word, model, in2idx, out2idx, idx2out, max_len=50):\n",
    "                \"\"\"Fold-specific version of segment_word using fold-specific constants.\"\"\"\n",
    "                model.eval()\n",
    "                src_idx = [START_IN_FOLD] + [in2idx.get(ch, PAD_IN_FOLD) for ch in word] + [END_IN_FOLD]\n",
    "                src = torch.tensor(src_idx).unsqueeze(1).to(device)\n",
    "                src_pad = (src.squeeze(1) == PAD_IN_FOLD).unsqueeze(0)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    mem = model.pos_enc(model.enc_embed(src))\n",
    "                    mem = model.encoder(mem, src_key_padding_mask=src_pad)\n",
    "                \n",
    "                out_ids = [START_OUT_FOLD]\n",
    "                for step in range(max_len):\n",
    "                    tgt = torch.tensor(out_ids).unsqueeze(1).to(device)\n",
    "                    mask = nn.Transformer.generate_square_subsequent_mask(len(out_ids)).to(device)\n",
    "                    with torch.no_grad():\n",
    "                        dec_out = model.pos_dec(model.dec_embed(tgt))\n",
    "                        dec = model.decoder(dec_out, mem, tgt_mask=mask, memory_key_padding_mask=src_pad)\n",
    "                        logits = model.out_proj(dec)\n",
    "                    \n",
    "                    probs = torch.softmax(logits[-1, 0], dim=0)\n",
    "                    nxt = logits[-1, 0].argmax().item()\n",
    "                    \n",
    "                    current_output = ''.join(idx2out[i] for i in out_ids[1:])\n",
    "                    current_chars = len(current_output.replace('+', ''))\n",
    "                    input_chars = len(word)\n",
    "                    min_expected_chars = input_chars\n",
    "                    \n",
    "                    if nxt == END_OUT_FOLD:\n",
    "                        end_prob = probs[END_OUT_FOLD].item()\n",
    "                        if current_chars >= min_expected_chars or end_prob > 0.8:\n",
    "                            break\n",
    "                        else:\n",
    "                            top_probs, top_indices = torch.topk(probs, k=3)\n",
    "                            for idx in top_indices:\n",
    "                                if idx.item() != END_OUT_FOLD:\n",
    "                                    nxt = idx.item()\n",
    "                                    break\n",
    "                    \n",
    "                    out_ids.append(nxt)\n",
    "                \n",
    "                result = ''.join(idx2out[i] for i in out_ids[1:])\n",
    "                return result\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # Compute validation loss\n",
    "                for src_batch, tgt_batch in val_loader_fold:\n",
    "                    src_batch = src_batch.to(device)\n",
    "                    tgt_batch = tgt_batch.to(device)\n",
    "                    \n",
    "                    src_pad_mask = (src_batch == PAD_IN_FOLD)\n",
    "                    tgt_pad_mask = (tgt_batch == PAD_OUT_FOLD)\n",
    "                    \n",
    "                    src = src_batch.transpose(0, 1)\n",
    "                    tgt = tgt_batch.transpose(0, 1)\n",
    "                    tgt_input = tgt[:-1, :]\n",
    "                    tgt_output = tgt[1:, :]\n",
    "                    \n",
    "                    tgt_mask = nn.Transformer.generate_square_subsequent_mask(\n",
    "                        tgt_input.size(0)\n",
    "                    ).to(device)\n",
    "                    \n",
    "                    logits = model_fold(\n",
    "                        src,\n",
    "                        tgt_input,\n",
    "                        tgt_mask=tgt_mask,\n",
    "                        src_key_padding_mask=src_pad_mask,\n",
    "                        tgt_key_padding_mask=tgt_pad_mask[:, :-1]\n",
    "                    )\n",
    "                    \n",
    "                    loss = criterion_fold(\n",
    "                        logits.reshape(-1, logits.size(-1)),\n",
    "                        tgt_output.reshape(-1)\n",
    "                    )\n",
    "                    val_loss += loss.item()\n",
    "                \n",
    "                # Compute exact match accuracy on validation set\n",
    "                for i in range(len(val_dataset_fold)):\n",
    "                    word = val_df_fold.iloc[i]['Word']\n",
    "                    gold_seg = val_df_fold.iloc[i]['segmentation']\n",
    "                    \n",
    "                    # Predict segmentation using fold-specific function\n",
    "                    seg_str = segment_word_fold(\n",
    "                        word, model_fold, input2idx_fold, output2idx_fold, \n",
    "                        idx2output_fold, max_len=50\n",
    "                    )\n",
    "                    \n",
    "                    if seg_str == gold_seg:\n",
    "                        exact_matches += 1\n",
    "                    total_val += 1\n",
    "            \n",
    "            val_loss = val_loss / len(val_loader_fold)\n",
    "            exact_match_rate = exact_matches / total_val if total_val > 0 else 0.0\n",
    "            \n",
    "            print(f\"  Epoch {epoch:02d} | train_loss={train_loss:.4f}  val_loss={val_loss:.4f}  exact_match={exact_match_rate:.3f}\")\n",
    "            \n",
    "            # Track best validation performance\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_exact_match = exact_match_rate\n",
    "                best_epoch = epoch\n",
    "        \n",
    "        print(f\"\\n  Best epoch: {best_epoch}\")\n",
    "        print(f\"  Best validation: Loss={best_val_loss:.4f}  Exact Match={best_exact_match:.3f}\")\n",
    "        \n",
    "        # Store fold results\n",
    "        fold_result = {\n",
    "            'fold': fold_idx,\n",
    "            'val_loss': best_val_loss,\n",
    "            'exact_match': best_exact_match,\n",
    "            'best_epoch': best_epoch\n",
    "        }\n",
    "        fold_results.append(fold_result)\n",
    "        \n",
    "        # Collect metrics for averaging\n",
    "        all_metrics['val_loss'].append(best_val_loss)\n",
    "        all_metrics['exact_match'].append(best_exact_match)\n",
    "    \n",
    "    # Calculate mean and std across folds\n",
    "    mean_metrics = {\n",
    "        'val_loss': np.mean(all_metrics['val_loss']),\n",
    "        'exact_match': np.mean(all_metrics['exact_match'])\n",
    "    }\n",
    "    \n",
    "    std_metrics = {\n",
    "        'val_loss': np.std(all_metrics['val_loss']),\n",
    "        'exact_match': np.std(all_metrics['exact_match'])\n",
    "    }\n",
    "    \n",
    "    # Find best fold (lowest validation loss)\n",
    "    best_fold_idx = min(range(len(fold_results)), key=lambda i: fold_results[i]['val_loss'])\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"K-FOLD CROSS-VALIDATION SUMMARY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"\\nPer-fold results:\")\n",
    "    for result in fold_results:\n",
    "        print(f\"  Fold {result['fold']}: \"\n",
    "              f\"Loss={result['val_loss']:.4f}, \"\n",
    "              f\"Exact Match={result['exact_match']:.3f}\")\n",
    "    \n",
    "    print(f\"\\nMean ± Std across {n_folds} folds:\")\n",
    "    print(f\"  Validation Loss:   {mean_metrics['val_loss']:.4f} ± {std_metrics['val_loss']:.4f}\")\n",
    "    print(f\"  Exact Match Rate:  {mean_metrics['exact_match']:.3f} ± {std_metrics['exact_match']:.3f}\")\n",
    "    print(f\"\\nBest fold: Fold {fold_results[best_fold_idx]['fold']} \"\n",
    "          f\"(Loss: {fold_results[best_fold_idx]['val_loss']:.4f}, \"\n",
    "          f\"Exact Match: {fold_results[best_fold_idx]['exact_match']:.3f})\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    return {\n",
    "        'fold_results': fold_results,\n",
    "        'mean_metrics': mean_metrics,\n",
    "        'std_metrics': std_metrics,\n",
    "        'best_fold_idx': best_fold_idx,\n",
    "        'all_metrics': all_metrics\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02527f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# K-FOLD CROSS-VALIDATION DEMONSTRATION\n",
    "# ===================================================================\n",
    "# This cell demonstrates how to use k-fold cross-validation to evaluate\n",
    "# model performance more robustly by training on multiple train/val splits\n",
    "# Each fold trains its own Transformer encoder-decoder model\n",
    "\n",
    "# Run 5-fold cross-validation on the training data\n",
    "kfold_results = run_kfold_cross_validation(\n",
    "    df=gold_df,  # Use the full gold_df for cross-validation\n",
    "    n_folds=5,  # Number of folds\n",
    "    d_model=D_MODEL,\n",
    "    ff=FF,\n",
    "    heads=HEADS,\n",
    "    layers=LAYERS,\n",
    "    drop=DROP,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    lr=LR,\n",
    "    random_state=42,  # Use fixed seed for reproducibility\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# The results dictionary contains:\n",
    "# - fold_results: List of results for each fold\n",
    "# - mean_metrics: Average metrics across all folds\n",
    "# - std_metrics: Standard deviation of metrics across folds\n",
    "# - best_fold_idx: Index of the fold with best validation loss\n",
    "# - all_metrics: Raw metrics from all folds\n",
    "\n",
    "print(\"\\nK-fold cross-validation completed!\")\n",
    "print(f\"Average exact match rate: {kfold_results['mean_metrics']['exact_match']:.3f} ± {kfold_results['std_metrics']['exact_match']:.3f}\")\n",
    "print(f\"Average validation loss: {kfold_results['mean_metrics']['val_loss']:.4f} ± {kfold_results['std_metrics']['val_loss']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f47bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# INFERENCE FUNCTION\n",
    "# =========================\n",
    "# Function to segment a word using the trained Transformer model\n",
    "def segment_word(word, model, in2idx, out2idx, idx2out, max_len=50, debug=False):\n",
    "    \"\"\"\n",
    "    Segment a word using the trained Transformer model.\n",
    "    \n",
    "    Args:\n",
    "        word: Input word string to segment\n",
    "        model: Trained MorphSegModel\n",
    "        in2idx: Input vocabulary mapping\n",
    "        out2idx: Output vocabulary mapping\n",
    "        idx2out: Reverse output vocabulary mapping (index to character)\n",
    "        max_len: Maximum output sequence length\n",
    "        debug: If True, print debugging information about token generation\n",
    "    \n",
    "    Returns:\n",
    "        Segmented word string (e.g., \"pi+kuna+s\")\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    # Encode input word: <s> + chars + </s>\n",
    "    src_idx = [START_IN] + [in2idx.get(ch, PAD_IN) for ch in word] + [END_IN]\n",
    "    src = torch.tensor(src_idx).unsqueeze(1).to(device)  # (seq_len, 1)\n",
    "    src_pad = (src.squeeze(1) == PAD_IN).unsqueeze(0)    # (1, seq_len)\n",
    "    \n",
    "    # Encode source word\n",
    "    with torch.no_grad():\n",
    "        mem = model.pos_enc(model.enc_embed(src))\n",
    "        mem = model.encoder(mem, src_key_padding_mask=src_pad)\n",
    "    \n",
    "    # Decode segmented word autoregressively\n",
    "    out_ids = [START_OUT]\n",
    "    if debug:\n",
    "        print(f\"Segmenting '{word}':\")\n",
    "        print(f\"  Generated tokens so far: \", end=\"\")\n",
    "    \n",
    "    for step in range(max_len):\n",
    "        tgt = torch.tensor(out_ids).unsqueeze(1).to(device)  # (seq_len, 1)\n",
    "        mask = nn.Transformer.generate_square_subsequent_mask(len(out_ids)).to(device)\n",
    "        with torch.no_grad():\n",
    "            dec_out = model.pos_dec(model.dec_embed(tgt))\n",
    "            dec = model.decoder(dec_out, mem, tgt_mask=mask, memory_key_padding_mask=src_pad)\n",
    "            logits = model.out_proj(dec)\n",
    "        \n",
    "        # Get probabilities for top predictions\n",
    "        probs = torch.softmax(logits[-1, 0], dim=0)\n",
    "        top_probs, top_indices = torch.topk(probs, k=3)\n",
    "        \n",
    "        # Predict next token (greedy decoding)\n",
    "        nxt = logits[-1, 0].argmax().item()\n",
    "        \n",
    "        if debug:\n",
    "            top_chars = [idx2out[idx.item()] for idx in top_indices]\n",
    "            print(f\"\\n  Step {step+1}: top predictions = {list(zip(top_chars, top_probs.tolist()))}\")\n",
    "            print(f\"    -> Selected: '{idx2out[nxt]}' (prob={probs[nxt].item():.4f})\")\n",
    "        \n",
    "        # Smart stopping criterion: don't stop too early\n",
    "        # Calculate current output length (excluding '+' separators)\n",
    "        current_output = ''.join(idx2out[i] for i in out_ids[1:])\n",
    "        current_chars = len(current_output.replace('+', ''))\n",
    "        input_chars = len(word)\n",
    "        \n",
    "        # Don't stop if:\n",
    "        # 1. We haven't generated at least as many characters as the input (accounting for separators)\n",
    "        # 2. The probability of END_OUT is not very high (below 0.8) AND we're still short\n",
    "        min_expected_chars = input_chars  # Should generate at least as many chars as input\n",
    "        \n",
    "        if nxt == END_OUT:\n",
    "            # Only stop if we've generated enough characters OR END_OUT probability is very high\n",
    "            end_prob = probs[END_OUT].item()\n",
    "            if current_chars >= min_expected_chars or end_prob > 0.8:\n",
    "                if debug:\n",
    "                    print(f\"  Stopped at END_OUT token (chars: {current_chars}/{min_expected_chars}, prob: {end_prob:.4f})\")\n",
    "                break\n",
    "            else:\n",
    "                # Force continue: select the second-best token if it's not END_OUT\n",
    "                if debug:\n",
    "                    print(f\"  END_OUT predicted but too early (chars: {current_chars}/{min_expected_chars}), forcing continuation...\")\n",
    "                # Find the best non-END_OUT token\n",
    "                for idx in top_indices:\n",
    "                    if idx.item() != END_OUT:\n",
    "                        nxt = idx.item()\n",
    "                        if debug:\n",
    "                            print(f\"    -> Forced selection: '{idx2out[nxt]}' (prob={probs[nxt].item():.4f})\")\n",
    "                        break\n",
    "        \n",
    "        out_ids.append(nxt)\n",
    "        \n",
    "        if debug:\n",
    "            current_seg = ''.join(idx2out[i] for i in out_ids[1:])\n",
    "            print(f\"    Current segmentation: '{current_seg}'\")\n",
    "    \n",
    "    # Convert indices to characters (skip START_OUT token)\n",
    "    result = ''.join(idx2out[i] for i in out_ids[1:])\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"  Final result: '{result}'\")\n",
    "        print(f\"  Expected length check: input '{word}' ({len(word)} chars) -> output '{result}' ({len(result.replace('+', ''))} chars)\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25099d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# BUILD REVERSE VOCABULARY MAPPING\n",
    "# =========================\n",
    "# Create index-to-character mapping for output (needed for inference)\n",
    "idx2output = {idx: ch for ch, idx in output2idx.items()}\n",
    "\n",
    "# Example usage with debugging to see what's happening:\n",
    "print(\"Example segmentation with debugging:\")\n",
    "test_word = \"pikunas\"\n",
    "segmented = segment_word(test_word, model, input2idx, output2idx, idx2output, debug=True)\n",
    "print(f\"\\nFinal result: {test_word} -> {segmented}\")\n",
    "\n",
    "# Check what the gold standard segmentation should be\n",
    "if 'gold_df' in locals():\n",
    "    gold_seg = gold_df[gold_df['Word'] == test_word]\n",
    "    if len(gold_seg) > 0:\n",
    "        print(f\"\\nGold standard segmentation(s):\")\n",
    "        for idx, row in gold_seg.iterrows():\n",
    "            print(f\"  {row['Morph_split']} -> {row['segmentation']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1539a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# EVALUATION ON TEST SET\n",
    "# =========================\n",
    "# Load test data and evaluate the model with comprehensive metrics\n",
    "# Similar to segmenter.ipynb evaluation\n",
    "\n",
    "print(\"Loading test data...\")\n",
    "test_df = pd.read_parquet(os.path.join(DATA_FOLDER, \"cleaned_data_df.parquet\"))\n",
    "print(f\"Loaded {len(test_df):,} test examples\")\n",
    "\n",
    "# Build reverse vocabulary mapping\n",
    "idx2output = {idx: ch for ch, idx in output2idx.items()}\n",
    "\n",
    "# =========================\n",
    "# EVALUATION HELPER FUNCTIONS\n",
    "# =========================\n",
    "# Functions for evaluating segmentation accuracy\n",
    "\n",
    "def parse_segmentation(seg_str):\n",
    "    \"\"\"\n",
    "    Parse segmented string (e.g., \"pi+kuna+s\") into list of morphemes.\n",
    "    \n",
    "    Args:\n",
    "        seg_str: Segmented string with '+' separators\n",
    "    \n",
    "    Returns:\n",
    "        List of morpheme strings\n",
    "    \"\"\"\n",
    "    if not seg_str:\n",
    "        return []\n",
    "    return seg_str.split('+')\n",
    "\n",
    "def is_correct_prediction(predicted, gold_variants):\n",
    "    \"\"\"\n",
    "    Check if predicted segmentation exactly matches any gold variant.\n",
    "    \n",
    "    Args:\n",
    "        predicted: List of predicted morphemes\n",
    "        gold_variants: List of gold segmentation variants (each is a list of morphemes)\n",
    "    \n",
    "    Returns:\n",
    "        True if prediction matches any gold variant, False otherwise\n",
    "    \"\"\"\n",
    "    # Normalize gold_variants (handle numpy arrays, nested structures)\n",
    "    if gold_variants is None:\n",
    "        return False\n",
    "    \n",
    "    if isinstance(gold_variants, np.ndarray):\n",
    "        gold_variants = gold_variants.tolist()\n",
    "    \n",
    "    if isinstance(gold_variants, list):\n",
    "        normalized = []\n",
    "        for variant in gold_variants:\n",
    "            if isinstance(variant, np.ndarray):\n",
    "                normalized.append(variant.tolist())\n",
    "            elif isinstance(variant, list):\n",
    "                normalized.append([item.tolist() if isinstance(item, np.ndarray) else item for item in variant])\n",
    "            else:\n",
    "                normalized.append(variant)\n",
    "        gold_variants = normalized\n",
    "    \n",
    "    return any(predicted == variant for variant in gold_variants)\n",
    "\n",
    "def split_count_metrics(predicted_segments, gold_variants):\n",
    "    \"\"\"\n",
    "    Compute split-count accuracy variants:\n",
    "    - Exact: same number of morphemes as any gold variant\n",
    "    - +1: one more split than any gold variant\n",
    "    - -1: one fewer split than any gold variant\n",
    "    - ±1: difference ≤ 1 with any gold variant\n",
    "    \"\"\"\n",
    "    pred_count = len(predicted_segments)\n",
    "    \n",
    "    # Normalize gold_variants\n",
    "    if gold_variants is None:\n",
    "        return {\"Exact\": False, \"+1\": False, \"-1\": False, \"±1\": False}\n",
    "    \n",
    "    if isinstance(gold_variants, np.ndarray):\n",
    "        gold_variants = gold_variants.tolist()\n",
    "    \n",
    "    if isinstance(gold_variants, list):\n",
    "        normalized = []\n",
    "        for variant in gold_variants:\n",
    "            if isinstance(variant, np.ndarray):\n",
    "                normalized.append(variant.tolist())\n",
    "            elif isinstance(variant, list):\n",
    "                normalized.append([item.tolist() if isinstance(item, np.ndarray) else item for item in variant])\n",
    "            else:\n",
    "                normalized.append(variant)\n",
    "        gold_variants = normalized\n",
    "    \n",
    "    gold_counts = [len(gold) for gold in gold_variants]\n",
    "\n",
    "    exact = any(pred_count == g for g in gold_counts)\n",
    "    plus1 = any(pred_count == g + 1 for g in gold_counts)\n",
    "    minus1 = any(pred_count == g - 1 for g in gold_counts)\n",
    "    pm1 = any(abs(pred_count - g) <= 1 for g in gold_counts)\n",
    "\n",
    "    return {\"Exact\": exact, \"+1\": plus1, \"-1\": minus1, \"±1\": pm1}\n",
    "\n",
    "# =========================\n",
    "# EVALUATION LOOP\n",
    "# =========================\n",
    "# Predict segmentations for all test words and compute metrics\n",
    "\n",
    "records = []\n",
    "all_words = test_df[\"Word\"].tolist()\n",
    "\n",
    "print(\"Predicting segmentations for test words...\")\n",
    "for word in all_words:\n",
    "    # Predict segmentation\n",
    "    seg_str = segment_word(word, model, input2idx, output2idx, idx2output)\n",
    "    predicted_segments = parse_segmentation(seg_str)\n",
    "    \n",
    "    # Get gold variants\n",
    "    gold_variants = test_df[test_df[\"Word\"] == word][\"Gold\"].iloc[0] if len(test_df[test_df[\"Word\"] == word]) > 0 else []\n",
    "    \n",
    "    # Exact match accuracy\n",
    "    correct_exact = is_correct_prediction(predicted_segments, gold_variants)\n",
    "    \n",
    "    # Split-count metrics\n",
    "    split_metrics = split_count_metrics(predicted_segments, gold_variants)\n",
    "    \n",
    "    records.append({\n",
    "        \"Word\": word,\n",
    "        \"Prediction\": predicted_segments,\n",
    "        \"Gold\": gold_variants,\n",
    "        \"CorrectExactSeg\": correct_exact,\n",
    "        \"CorrectSplitCount\": split_metrics[\"Exact\"],\n",
    "        \"SplitCount+1\": split_metrics[\"+1\"],\n",
    "        \"SplitCount-1\": split_metrics[\"-1\"],\n",
    "        \"SplitCount±1\": split_metrics[\"±1\"],\n",
    "        \"OverlapExactAndSplit\": correct_exact and split_metrics[\"Exact\"]\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(records)\n",
    "\n",
    "# =========================\n",
    "# COMPUTE AGGREGATE METRICS\n",
    "# =========================\n",
    "# Calculate overall accuracy and split-count metrics\n",
    "\n",
    "accuracy = results_df[\"CorrectExactSeg\"].mean()\n",
    "split_exact_acc = results_df[\"CorrectSplitCount\"].mean()\n",
    "split_plus1_acc = results_df[\"SplitCount+1\"].mean()\n",
    "split_minus1_acc = results_df[\"SplitCount-1\"].mean()\n",
    "split_pm1_acc = results_df[\"SplitCount±1\"].mean()\n",
    "overlap_accuracy = results_df[\"OverlapExactAndSplit\"].mean()\n",
    "\n",
    "print(f\"\\n=== Evaluation Results ===\")\n",
    "print(f\"Exact segmentation accuracy: {accuracy:.4f}\")\n",
    "print(f\"\\n=== Split-count metrics ===\")\n",
    "print(f\"Split-count (Exact):          {split_exact_acc:.4f}\")\n",
    "print(f\"Split-count (+1):             {split_plus1_acc:.4f}\")\n",
    "print(f\"Split-count (−1):              {split_minus1_acc:.4f}\")\n",
    "print(f\"Split-count (±1):              {split_pm1_acc:.4f}\")\n",
    "print(f\"Overlap (Exact ∩ Split):      {overlap_accuracy:.4f}\")\n",
    "\n",
    "# =========================\n",
    "# SAVE EVALUATION RESULTS\n",
    "# =========================\n",
    "# Save evaluation results to the data folder with a descriptive filename\n",
    "results_output_path = os.path.join(DATA_FOLDER, \"transformer_eval_results.csv\")\n",
    "results_df.to_csv(results_output_path, index=False)\n",
    "print(f\"\\nEvaluation results saved to {results_output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c035ae8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# EXAMPLE USAGE\n",
    "# =========================\n",
    "# Test the model on a few example words\n",
    "test_words = [\"pikunas\", \"rikuchkani\", \"ñichkanchus\"]\n",
    "print(\"Example segmentations:\")\n",
    "for word in test_words:\n",
    "    segmented = segment_word(word, model, input2idx, output2idx, idx2output)\n",
    "    print(f\"  {word} -> {segmented}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8895024a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
