{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SEGMENTER-OLD: CHARACTER-LEVEL BILSTM MORPHOLOGY PARSER\n",
    "========================================================\n",
    "\n",
    "This notebook implements a character-level BiLSTM model for morphological segmentation\n",
    "of Quechua words. It predicts boundary positions at the character level, marking where\n",
    "morpheme boundaries occur within words.\n",
    "\n",
    "Key Features:\n",
    "- Character-level tokenization (each character is a token)\n",
    "- BiLSTM architecture for sequence labeling\n",
    "- Binary classification: predicts boundary (1) or no boundary (0) at each character position\n",
    "- Comprehensive evaluation metrics (precision, recall, F1, exact match, split-count accuracy)\n",
    "\n",
    "This is an older/alternative approach compared to the token-window based models in\n",
    "DT-LSTM-MarkovFilter.ipynb and Markov-LSTM-MarkovFilter.ipynb.\n",
    "\n",
    "All data is read from the 'data' folder and models are saved to the 'models_segmenter-old' folder.\n",
    "\"\"\"\n",
    "\n",
    "import ast\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading gold standard data...\n",
      "Loaded 6,896 gold standard examples\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# DATA FOLDER CONFIGURATION\n",
    "# =========================\n",
    "# All data files should be read from and saved to the data folder\n",
    "DATA_FOLDER = \"data\"\n",
    "\n",
    "# Model folder named after this notebook\n",
    "MODEL_NAME = \"segmenter-old\"\n",
    "MODELS_FOLDER = f\"models_{MODEL_NAME}\"\n",
    "\n",
    "# Create folders if they don't exist\n",
    "os.makedirs(DATA_FOLDER, exist_ok=True)\n",
    "os.makedirs(MODELS_FOLDER, exist_ok=True)\n",
    "\n",
    "# =========================\n",
    "# LOAD GOLD STANDARD DATA\n",
    "# =========================\n",
    "# The gold standard dataset contains high-quality morphological segmentations\n",
    "# This is the base training data for the character-level BiLSTM model\n",
    "print(\"Loading gold standard data...\")\n",
    "gold_df = pd.read_parquet(os.path.join(DATA_FOLDER, \"Sue_kalt.parquet\"))\n",
    "gold_df['Word'] = gold_df['word']\n",
    "gold_df['morph'] = gold_df['morph'].str.replace('-', ' ')  # Normalize separators\n",
    "gold_df['Morph_split_str'] = gold_df['morph']  # String version\n",
    "gold_df['Morph_split'] = gold_df['morph'].str.split(' ')  # List version\n",
    "gold_df = gold_df[['Word', 'Morph_split', 'Morph_split_str']]\n",
    "gold_df.drop_duplicates(subset='Word', keep='first', inplace=True)\n",
    "gold_df.dropna(subset=['Word'], inplace=True)\n",
    "print(f\"Loaded {len(gold_df):,} gold standard examples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Morph_split</th>\n",
       "      <th>Morph_split_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cementerioman</td>\n",
       "      <td>[cementerio, man]</td>\n",
       "      <td>cementerio man</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kawsachkananta</td>\n",
       "      <td>[kawsa, chka, na, n, ta]</td>\n",
       "      <td>kawsa chka na n ta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mañakunpis</td>\n",
       "      <td>[maña, ku, n, pis]</td>\n",
       "      <td>maña ku n pis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>imaynapichus</td>\n",
       "      <td>[imayna, pi, chus]</td>\n",
       "      <td>imayna pi chus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>qipiyuq</td>\n",
       "      <td>[qipi, yuq]</td>\n",
       "      <td>qipi yuq</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Word               Morph_split     Morph_split_str\n",
       "0   cementerioman         [cementerio, man]      cementerio man\n",
       "1  kawsachkananta  [kawsa, chka, na, n, ta]  kawsa chka na n ta\n",
       "2      mañakunpis        [maña, ku, n, pis]       maña ku n pis\n",
       "3    imaynapichus        [imayna, pi, chus]      imayna pi chus\n",
       "4         qipiyuq               [qipi, yuq]            qipi yuq"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6896, 3)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# FEATURE EXTRACTION\n",
    "# =========================\n",
    "# Extract basic features for analysis and potential use in the model\n",
    "gold_df['num_morphemes'] = gold_df['Morph_split'].apply(len)  # Number of morphemes per word\n",
    "gold_df['word_len'] = gold_df['Word'].apply(len)  # Character length of word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Morph_split</th>\n",
       "      <th>Morph_split_str</th>\n",
       "      <th>num_morphemes</th>\n",
       "      <th>word_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cementerioman</td>\n",
       "      <td>[cementerio, man]</td>\n",
       "      <td>cementerio man</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kawsachkananta</td>\n",
       "      <td>[kawsa, chka, na, n, ta]</td>\n",
       "      <td>kawsa chka na n ta</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mañakunpis</td>\n",
       "      <td>[maña, ku, n, pis]</td>\n",
       "      <td>maña ku n pis</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>imaynapichus</td>\n",
       "      <td>[imayna, pi, chus]</td>\n",
       "      <td>imayna pi chus</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>qipiyuq</td>\n",
       "      <td>[qipi, yuq]</td>\n",
       "      <td>qipi yuq</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Word               Morph_split     Morph_split_str  \\\n",
       "0   cementerioman         [cementerio, man]      cementerio man   \n",
       "1  kawsachkananta  [kawsa, chka, na, n, ta]  kawsa chka na n ta   \n",
       "2      mañakunpis        [maña, ku, n, pis]       maña ku n pis   \n",
       "3    imaynapichus        [imayna, pi, chus]      imayna pi chus   \n",
       "4         qipiyuq               [qipi, yuq]            qipi yuq   \n",
       "\n",
       "   num_morphemes  word_len  \n",
       "0              2        13  \n",
       "1              5        14  \n",
       "2              4        10  \n",
       "3              3        12  \n",
       "4              2         7  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# BOUNDARY LABEL GENERATION\n",
    "# =========================\n",
    "# Convert morpheme splits into character-level boundary labels\n",
    "# Labels mark the end position of each morpheme (except the last one)\n",
    "\n",
    "def get_boundary_labels(word, split):\n",
    "    \"\"\"\n",
    "    Generate binary boundary labels for a word given its morpheme split.\n",
    "    \n",
    "    Args:\n",
    "        word: The full word string\n",
    "        split: List of morphemes (e.g., ['kawsa', 'chka', 'na', 'n', 'ta'])\n",
    "    \n",
    "    Returns:\n",
    "        List of binary labels (0=no boundary, 1=boundary) for each character position\n",
    "        The label at position i indicates if there's a boundary after character i\n",
    "    \"\"\"\n",
    "    labels = [0] * len(word)\n",
    "    idx = 0\n",
    "    # Mark boundaries after each morpheme (except the last one)\n",
    "    for morpheme in split[:-1]: \n",
    "        idx += len(morpheme)\n",
    "        if idx < len(word):\n",
    "            labels[idx - 1] = 1  # Boundary at the end of this morpheme\n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# PREPARE TRAINING DATA\n",
    "# =========================\n",
    "# Convert words to character sequences and generate boundary labels\n",
    "# This prepares the data for the character-level BiLSTM model\n",
    "\n",
    "gold_df['char_seq'] = gold_df['Word'].apply(list)  # Convert word to list of characters\n",
    "gold_df['boundary_labels'] = gold_df.apply(\n",
    "    lambda row: get_boundary_labels(row['Word'], row['Morph_split']), axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Vocabulary size: 55 characters\n",
      "Training samples: 6,206\n",
      "Validation samples: 690\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# PYTORCH IMPORTS AND SETUP\n",
    "# =========================\n",
    "# Import libraries for neural network training and data handling\n",
    "\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Set device (GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# =========================\n",
    "# VOCABULARY CONSTRUCTION\n",
    "# =========================\n",
    "# Build character-level vocabulary for embedding layer\n",
    "# Each unique character gets an integer ID\n",
    "\n",
    "PAD, UNK = \"<PAD>\", \"<UNK>\"  # Special tokens for padding and unknown characters\n",
    "\n",
    "def build_vocab(seqs: List[List[str]]):\n",
    "    \"\"\"\n",
    "    Build vocabulary from character sequences.\n",
    "    \n",
    "    Args:\n",
    "        seqs: List of character sequences (each sequence is a list of characters)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (stoi, itos):\n",
    "        - stoi: Dictionary mapping character to integer ID\n",
    "        - itos: List mapping integer ID to character\n",
    "    \"\"\"\n",
    "    chars = {c for seq in seqs for c in seq}  # Collect all unique characters\n",
    "    itos = [PAD, UNK] + sorted(chars)  # Index-to-string: [PAD, UNK, 'a', 'b', ...]\n",
    "    stoi = {ch: i for i, ch in enumerate(itos)}  # String-to-index dictionary\n",
    "    return stoi, itos\n",
    "\n",
    "# Build vocabulary from all character sequences in the gold data\n",
    "stoi, itos = build_vocab(gold_df[\"char_seq\"].tolist())\n",
    "print(f\"Vocabulary size: {len(itos)} characters\")\n",
    "\n",
    "def encode(seq: List[str]) -> List[int]:\n",
    "    \"\"\"Convert character sequence to integer IDs.\"\"\"\n",
    "    return [stoi.get(c, stoi[UNK]) for c in seq]\n",
    "\n",
    "def encode_labels(labels: List[int]) -> List[int]:\n",
    "    \"\"\"Labels are already 0/1, so just return them as-is.\"\"\"\n",
    "    return labels\n",
    "\n",
    "# =========================\n",
    "# DATASET AND DATALOADER\n",
    "# =========================\n",
    "# PyTorch Dataset and DataLoader for batching and padding sequences\n",
    "\n",
    "class CharBoundaryDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for character-level boundary prediction.\n",
    "    Each sample contains a character sequence and its boundary labels.\n",
    "    \"\"\"\n",
    "    def __init__(self, df):\n",
    "        self.x = df[\"char_seq\"].tolist()  # Character sequences\n",
    "        self.y = df[\"boundary_labels\"].tolist()  # Boundary labels\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "def pad_batch(batch, pad_id=0):\n",
    "    \"\"\"\n",
    "    Collate function for DataLoader: pads sequences to the same length.\n",
    "    \n",
    "    Args:\n",
    "        batch: List of (character_sequence, boundary_labels) tuples\n",
    "        pad_id: ID to use for padding (default: 0, which is PAD token)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of tensors:\n",
    "        - x_pad: Padded character sequences (B, T)\n",
    "        - y_pad: Padded boundary labels (B, T)\n",
    "        - mask: Boolean mask indicating valid positions (B, T)\n",
    "        - lengths: Actual length of each sequence (B,)\n",
    "    \"\"\"\n",
    "    # batch: List[ (List[str], List[int]) ]\n",
    "    seqs, labels = zip(*batch)\n",
    "    x_ids = [encode(s) for s in seqs]  # Convert characters to IDs\n",
    "    y_ids = [encode_labels(y) for y in labels]  # Labels are already 0/1\n",
    "    lengths = [len(x) for x in x_ids]\n",
    "    maxlen = max(lengths)\n",
    "    \n",
    "    # Pad sequences and labels to maxlen\n",
    "    x_pad = [xi + [pad_id]*(maxlen - len(xi)) for xi in x_ids]\n",
    "    y_pad = [yi + [0]*(maxlen - len(yi)) for yi in y_ids]  # Pad labels as 0 (will be masked)\n",
    "    mask  = [[1]*len(xi) + [0]*(maxlen - len(xi)) for xi in x_ids]  # 1 for valid, 0 for padding\n",
    "    \n",
    "    return (\n",
    "        torch.LongTensor(x_pad),\n",
    "        torch.FloatTensor(y_pad),   # BCE expects float targets\n",
    "        torch.BoolTensor(mask),\n",
    "        torch.LongTensor(lengths),\n",
    "    )\n",
    "\n",
    "# =========================\n",
    "# TRAIN/VALIDATION SPLIT\n",
    "# =========================\n",
    "# Split data into 90% training and 10% validation\n",
    "rng = np.random.default_rng(42)  # Fixed seed for reproducibility\n",
    "indices = np.arange(len(gold_df))\n",
    "rng.shuffle(indices)\n",
    "split = int(0.9*len(indices))\n",
    "train_idx, val_idx = indices[:split], indices[split:]\n",
    "\n",
    "train_df = gold_df.iloc[train_idx].reset_index(drop=True)\n",
    "val_df   = gold_df.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "print(f\"Training samples: {len(train_df):,}\")\n",
    "print(f\"Validation samples: {len(val_df):,}\")\n",
    "\n",
    "train_ds = CharBoundaryDataset(train_df)\n",
    "val_ds   = CharBoundaryDataset(val_df)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=pad_batch)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, collate_fn=pad_batch)\n",
    "\n",
    "# =========================\n",
    "# BILSTM MODEL ARCHITECTURE\n",
    "# =========================\n",
    "# Character-level BiLSTM for boundary prediction\n",
    "\n",
    "class BiLSTMBoundary(nn.Module):\n",
    "    \"\"\"\n",
    "    Bidirectional LSTM model for character-level boundary prediction.\n",
    "    \n",
    "    Architecture:\n",
    "    1. Character embeddings (emb_dim dimensions)\n",
    "    2. Bidirectional LSTM (hidden_size per direction)\n",
    "    3. Dropout for regularization\n",
    "    4. Linear output layer (predicts boundary probability at each position)\n",
    "    \n",
    "    The model processes sequences character-by-character and outputs a logit\n",
    "    for each position indicating the probability of a boundary after that character.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size: int, emb_dim: int = 16, hidden_size: int = 16, num_layers: int = 1, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        # Character embedding layer\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        \n",
    "        # Bidirectional LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=emb_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=True,  # Process sequence in both directions\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Output layer: 2*hidden_size because bidirectional LSTM concatenates forward/backward\n",
    "        self.out = nn.Linear(hidden_size * 2, 1)  # Binary classification per time-step\n",
    "    \n",
    "    def forward(self, x, lengths):\n",
    "        \"\"\"\n",
    "        Forward pass through the model.\n",
    "        \n",
    "        Args:\n",
    "            x: Input character IDs (B, T) - Long tensor\n",
    "            lengths: Actual length of each sequence (B,) - Long tensor\n",
    "        \n",
    "        Returns:\n",
    "            logits: Boundary prediction logits (B, T) - Float tensor\n",
    "        \"\"\"\n",
    "        emb = self.emb(x)  # (B, T, E) - Embed characters\n",
    "        \n",
    "        # Pack sequences to ignore padding during LSTM processing\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(emb, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_out, _ = self.lstm(packed)\n",
    "        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)  # (B, T, 2H)\n",
    "        \n",
    "        out = self.dropout(out)\n",
    "        logits = self.out(out).squeeze(-1)  # (B, T) - One logit per character position\n",
    "        return logits\n",
    "\n",
    "# =========================\n",
    "# LOSS FUNCTION\n",
    "# =========================\n",
    "# Masked binary cross-entropy loss (ignores padding positions)\n",
    "\n",
    "def masked_bce_loss(logits, targets, mask):\n",
    "    \"\"\"\n",
    "    Compute masked binary cross-entropy loss.\n",
    "    Only computes loss on valid (non-padded) positions.\n",
    "    \n",
    "    Args:\n",
    "        logits: Model predictions (B, T)\n",
    "        targets: Ground truth labels (B, T)\n",
    "        mask: Boolean mask indicating valid positions (B, T)\n",
    "    \n",
    "    Returns:\n",
    "        Scalar loss value\n",
    "    \"\"\"\n",
    "    loss_fn = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "    loss_per_token = loss_fn(logits, targets)\n",
    "    loss_per_token = loss_per_token * mask.float()  # Zero out padding positions\n",
    "    denom = mask.float().sum().clamp_min(1.0)  # Total number of valid tokens\n",
    "    return loss_per_token.sum() / denom\n",
    "\n",
    "# =========================\n",
    "# EVALUATION METRICS\n",
    "# =========================\n",
    "# Functions to compute precision, recall, and F1 score for boundary prediction\n",
    "\n",
    "def boundary_f1(logits, targets, mask, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Compute precision, recall, and F1 score for boundary prediction.\n",
    "    \n",
    "    Args:\n",
    "        logits: Model predictions (B, T)\n",
    "        targets: Ground truth labels (B, T)\n",
    "        mask: Boolean mask indicating valid positions (B, T)\n",
    "        threshold: Probability threshold for binary classification (default: 0.5)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (precision, recall, f1_score)\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        probs = torch.sigmoid(logits)  # Convert logits to probabilities\n",
    "        preds = (probs >= threshold).long()  # Binary predictions\n",
    "        t = targets.long()\n",
    "        m = mask.long()\n",
    "\n",
    "        # Compute true positives, false positives, false negatives\n",
    "        tp = ((preds == 1) & (t == 1) & (m == 1)).sum().item()\n",
    "        fp = ((preds == 1) & (t == 0) & (m == 1)).sum().item()\n",
    "        fn = ((preds == 0) & (t == 1) & (m == 1)).sum().item()\n",
    "\n",
    "        # Compute metrics with safe division\n",
    "        prec = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        rec  = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1   = 2*prec*rec / (prec + rec) if (prec + rec) > 0 else 0.0\n",
    "        return prec, rec, f1\n",
    "\n",
    "# ==== 7) Inference: predict boundaries and reconstruct morphemes ====\n",
    "def predict_boundaries(words: List[str], model, stoi, threshold=0.5) -> List[List[int]]:\n",
    "    model.eval()\n",
    "    char_lists = [list(w) for w in words]\n",
    "    x_ids = [ [stoi.get(c, stoi[UNK]) for c in chars] for chars in char_lists ]\n",
    "    lengths = [len(x) for x in x_ids]\n",
    "    maxlen = max(lengths)\n",
    "    pad_id = stoi[PAD]\n",
    "\n",
    "    x_pad = [xi + [pad_id]*(maxlen - len(xi)) for xi in x_ids]\n",
    "    mask  = [[1]*len(xi) + [0]*(maxlen - len(xi)) for xi in x_ids]\n",
    "\n",
    "    x = torch.LongTensor(x_pad).to(device)\n",
    "    lengths_t = torch.LongTensor(lengths).to(device)\n",
    "    mask_t = torch.BoolTensor(mask).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(x, lengths_t)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        preds = (probs >= threshold) & mask_t\n",
    "    # trim pad and convert to 0/1\n",
    "    out = []\n",
    "    for i, L in enumerate(lengths):\n",
    "        out.append(preds[i, :L].int().tolist())\n",
    "    return out\n",
    "\n",
    "def apply_boundaries(word: str, boundary_labels: List[int]) -> List[str]:\n",
    "    # boundary_labels marks the *end* of a morpheme at that position (same convention as your helper)\n",
    "    segs = []\n",
    "    start = 0\n",
    "    for i, b in enumerate(boundary_labels, start=0):\n",
    "        if b == 1:\n",
    "            segs.append(word[start:i+1])\n",
    "            start = i+1\n",
    "    if start < len(word):\n",
    "        segs.append(word[start:])\n",
    "    return segs\n",
    "\n",
    "# Example usage:\n",
    "# test_words = [\"rikuchkani\", \"pikunas\", \"ñichkanchus\"]\n",
    "# pred_b = predict_boundaries(test_words, model, stoi, threshold=0.5)\n",
    "# for w, b in zip(test_words, pred_b):\n",
    "#     print(w, b, \"->\", apply_boundaries(w, b))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for existing model with ID: 6112ccdaef2e0c54\n",
      "Model checkpoint loaded from models_segmenter-old\\6112ccdaef2e0c54\n",
      "✅ Found existing model! Loading from models_segmenter-old\\6112ccdaef2e0c54\n",
      "Model loaded successfully. Skipping training.\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# MODEL CHECKPOINTING FUNCTIONS\n",
    "# =========================\n",
    "# Functions to save and load trained models to avoid retraining\n",
    "\n",
    "def generate_model_id(emb_dim, hidden_size, num_layers, dropout, epochs, batch_size, lr, weight_decay):\n",
    "    \"\"\"\n",
    "    Generate a unique identifier for a model based on its training parameters.\n",
    "    \n",
    "    Args:\n",
    "        All training hyperparameters\n",
    "    \n",
    "    Returns:\n",
    "        A string identifier (hash) for the model\n",
    "    \"\"\"\n",
    "    params_dict = {\n",
    "        'emb_dim': emb_dim,\n",
    "        'hidden_size': hidden_size,\n",
    "        'num_layers': num_layers,\n",
    "        'dropout': dropout,\n",
    "        'epochs': epochs,\n",
    "        'batch_size': batch_size,\n",
    "        'lr': lr,\n",
    "        'weight_decay': weight_decay,\n",
    "        'vocab_size': len(itos)\n",
    "    }\n",
    "    params_str = json.dumps(params_dict, sort_keys=True)\n",
    "    model_id = hashlib.md5(params_str.encode()).hexdigest()[:16]\n",
    "    return model_id\n",
    "\n",
    "def save_model_checkpoint(model, stoi, itos, model_id, models_folder=MODELS_FOLDER):\n",
    "    \"\"\"\n",
    "    Save model checkpoint to the models folder.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained BiLSTMBoundary model\n",
    "        stoi: String-to-index vocabulary dictionary\n",
    "        itos: Index-to-string vocabulary list\n",
    "        model_id: Unique identifier for this model\n",
    "        models_folder: Folder to save models in\n",
    "    \"\"\"\n",
    "    model_dir = os.path.join(models_folder, model_id)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    checkpoint_path = os.path.join(model_dir, \"bilstm_char_boundary.pt\")\n",
    "    torch.save({\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"stoi\": stoi,\n",
    "        \"itos\": itos\n",
    "    }, checkpoint_path)\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata_path = os.path.join(model_dir, \"metadata.json\")\n",
    "    with open(metadata_path, \"w\") as f:\n",
    "        json.dump({\n",
    "            'model_id': model_id,\n",
    "            'vocab_size': len(itos),\n",
    "            'model_name': MODEL_NAME\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    print(f\"Model checkpoint saved to {model_dir}\")\n",
    "    return model_dir\n",
    "\n",
    "def load_model_checkpoint(model_id, models_folder=MODELS_FOLDER):\n",
    "    \"\"\"\n",
    "    Load model checkpoint from the models folder.\n",
    "    \n",
    "    Args:\n",
    "        model_id: Unique identifier for the model\n",
    "        models_folder: Folder where models are saved\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with 'model_state', 'stoi', 'itos', 'checkpoint_path', 'model_dir' or None if not found\n",
    "    \"\"\"\n",
    "    model_dir = os.path.join(models_folder, model_id)\n",
    "    checkpoint_path = os.path.join(model_dir, \"bilstm_char_boundary.pt\")\n",
    "    \n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        return None\n",
    "    \n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    print(f\"Model checkpoint loaded from {model_dir}\")\n",
    "    return {\n",
    "        'model_state': checkpoint['model_state'],\n",
    "        'stoi': checkpoint['stoi'],\n",
    "        'itos': checkpoint['itos'],\n",
    "        'checkpoint_path': checkpoint_path,\n",
    "        'model_dir': model_dir\n",
    "    }\n",
    "\n",
    "# =========================\n",
    "# MODEL INITIALIZATION AND TRAINING\n",
    "# =========================\n",
    "# Initialize model and optimizer, then train (or load if already trained)\n",
    "\n",
    "# Model hyperparameters\n",
    "EMB_DIM = 16\n",
    "HIDDEN_SIZE = 32\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.3\n",
    "EPOCHS = 35\n",
    "BATCH_SIZE = 64\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "# Generate model identifier\n",
    "model_id = generate_model_id(EMB_DIM, HIDDEN_SIZE, NUM_LAYERS, DROPOUT, EPOCHS, BATCH_SIZE, LR, WEIGHT_DECAY)\n",
    "\n",
    "# Try to load existing model\n",
    "print(f\"Checking for existing model with ID: {model_id}\")\n",
    "loaded = load_model_checkpoint(model_id, models_folder=MODELS_FOLDER)\n",
    "\n",
    "if loaded is not None:\n",
    "    print(f\"✅ Found existing model! Loading from {loaded['model_dir']}\")\n",
    "    stoi = loaded['stoi']\n",
    "    itos = loaded['itos']\n",
    "    model = BiLSTMBoundary(vocab_size=len(itos), emb_dim=EMB_DIM, hidden_size=HIDDEN_SIZE, \n",
    "                           num_layers=NUM_LAYERS, dropout=DROPOUT).to(device)\n",
    "    model.load_state_dict(loaded['model_state'])\n",
    "    model.eval()\n",
    "    print(\"Model loaded successfully. Skipping training.\")\n",
    "else:\n",
    "    print(f\"No existing model found. Training new model...\")\n",
    "    \n",
    "    # Initialize model and optimizer\n",
    "    model = BiLSTMBoundary(vocab_size=len(itos), emb_dim=EMB_DIM, hidden_size=HIDDEN_SIZE, \n",
    "                          num_layers=NUM_LAYERS, dropout=DROPOUT).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    \n",
    "    # =========================\n",
    "    # TRAINING LOOP\n",
    "    # =========================\n",
    "    # Train the model for specified number of epochs\n",
    "    # Save checkpoint whenever validation F1 improves\n",
    "    \n",
    "    best_val_f1 = 0.0\n",
    "    \n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        total_tokens = 0\n",
    "        for x, y, mask, lengths in train_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            mask = mask.to(device)\n",
    "            lengths = lengths.to(device)\n",
    "\n",
    "            logits = model(x, lengths)\n",
    "            loss = masked_bce_loss(logits, y, mask)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * mask.sum().item()\n",
    "            total_tokens += mask.sum().item()\n",
    "\n",
    "        train_loss = total_loss / max(total_tokens, 1)\n",
    "\n",
    "        # ---- Validation phase\n",
    "        model.eval()\n",
    "        val_loss, val_tokens = 0.0, 0\n",
    "        all_prec, all_rec, all_f1 = [], [], []\n",
    "        with torch.no_grad():\n",
    "            for x, y, mask, lengths in val_loader:\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                mask = mask.to(device)\n",
    "                lengths = lengths.to(device)\n",
    "\n",
    "                logits = model(x, lengths)\n",
    "                loss = masked_bce_loss(logits, y, mask)\n",
    "                val_loss += loss.item() * mask.sum().item()\n",
    "                val_tokens += mask.sum().item()\n",
    "\n",
    "                p, r, f = boundary_f1(logits, y, mask, threshold=0.5)\n",
    "                all_prec.append(p); all_rec.append(r); all_f1.append(f)\n",
    "\n",
    "        val_loss = val_loss / max(val_tokens, 1)\n",
    "        prec = np.mean(all_prec) if all_prec else 0.0\n",
    "        rec  = np.mean(all_rec)  if all_rec  else 0.0\n",
    "        f1   = np.mean(all_f1)   if all_f1   else 0.0\n",
    "\n",
    "        print(f\"Epoch {epoch:02d} | train_loss={train_loss:.4f}  val_loss={val_loss:.4f}  P={prec:.3f} R={rec:.3f} F1={f1:.3f}\")\n",
    "\n",
    "        # Keep best model based on validation F1\n",
    "        if f1 > best_val_f1:\n",
    "            best_val_f1 = f1\n",
    "            save_model_checkpoint(model, stoi, itos, model_id, models_folder=MODELS_FOLDER)\n",
    "            print(\"  ↳ saved checkpoint (best F1 so far)\")\n",
    "    \n",
    "    print(f\"\\nTraining complete! Best validation F1: {best_val_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data...\n",
      "Loaded 913 test examples\n",
      "Model checkpoint loaded from models_segmenter-old\\6112ccdaef2e0c54\n",
      "Model loaded successfully for evaluation.\n",
      "Exact segmentation accuracy: 0.5268\n",
      "Boundary metrics:\n",
      "  Micro  - P: 0.7963  R: 0.8397  F1: 0.8174\n",
      "  Macro  - P: 0.8072  R: 0.8279  F1: 0.7993\n",
      "\n",
      "=== Split-count metrics ===\n",
      "Split-count (Exact):          0.6440\n",
      "Split-count (+1):             0.1906\n",
      "Split-count (−1):             0.1391\n",
      "Split-count (±1):             0.9726\n",
      "Overlap (Exact ∩ Split):      0.5268\n",
      "\n",
      "Evaluation results saved to data\\bilstm_eval_results.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "\n",
    "# =========================\n",
    "# LOAD TEST DATA\n",
    "# =========================\n",
    "# Load the test/accuracy evaluation dataset\n",
    "print(\"Loading test data...\")\n",
    "df = pd.read_parquet(os.path.join(DATA_FOLDER, \"cleaned_data_df.parquet\"))\n",
    "print(f\"Loaded {len(df):,} test examples\")\n",
    "\n",
    "# =========================\n",
    "# LOAD TRAINED MODEL\n",
    "# =========================\n",
    "# Load the best model checkpoint from the models folder\n",
    "# Use the same model ID that was generated during training\n",
    "\n",
    "# Model hyperparameters (must match training)\n",
    "EMB_DIM = 16\n",
    "HIDDEN_SIZE = 32\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.3\n",
    "EPOCHS = 35\n",
    "BATCH_SIZE = 64\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "# Generate the same model ID\n",
    "model_id = generate_model_id(EMB_DIM, HIDDEN_SIZE, NUM_LAYERS, DROPOUT, EPOCHS, BATCH_SIZE, LR, WEIGHT_DECAY)\n",
    "\n",
    "# Load checkpoint\n",
    "loaded = load_model_checkpoint(model_id, models_folder=MODELS_FOLDER)\n",
    "if loaded is None:\n",
    "    raise FileNotFoundError(f\"Model checkpoint not found. Please train the model first (model_id: {model_id})\")\n",
    "\n",
    "stoi, itos = loaded[\"stoi\"], loaded[\"itos\"]\n",
    "model = BiLSTMBoundary(vocab_size=len(itos), emb_dim=EMB_DIM, hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS)\n",
    "model.load_state_dict(loaded[\"model_state\"])\n",
    "model.eval()\n",
    "print(\"Model loaded successfully for evaluation.\")\n",
    "\n",
    "# =========================\n",
    "# EVALUATION HELPER FUNCTIONS\n",
    "# =========================\n",
    "# Functions for predicting boundaries and evaluating segmentation accuracy\n",
    "\n",
    "def predict_boundaries(words, model, stoi, threshold=0.5):\n",
    "    model.eval()\n",
    "    char_lists = [list(w) for w in words]\n",
    "    x_ids = [[stoi.get(c, stoi[\"<UNK>\"]) for c in chars] for chars in char_lists]\n",
    "    lengths = [len(x) for x in x_ids]\n",
    "    maxlen = max(lengths)\n",
    "    pad_id = stoi[\"<PAD>\"]\n",
    "\n",
    "    x_pad = [xi + [pad_id] * (maxlen - len(xi)) for xi in x_ids]\n",
    "    mask = [[1] * len(xi) + [0] * (maxlen - len(xi)) for xi in x_ids]\n",
    "\n",
    "    x = torch.LongTensor(x_pad)\n",
    "    lengths_t = torch.LongTensor(lengths)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(x, lengths_t)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        preds = (probs >= threshold)\n",
    "    out = []\n",
    "    for i, L in enumerate(lengths):\n",
    "        out.append(preds[i, :L].int().tolist())\n",
    "    return out\n",
    "\n",
    "def apply_boundaries(word, boundary_labels):\n",
    "    segs = []\n",
    "    start = 0\n",
    "    for i, b in enumerate(boundary_labels):\n",
    "        if b == 1:\n",
    "            segs.append(word[start:i+1])\n",
    "            start = i+1\n",
    "    if start < len(word):\n",
    "        segs.append(word[start:])\n",
    "    return segs\n",
    "\n",
    "\n",
    "# =========================\n",
    "# EVALUATION METRICS FUNCTIONS\n",
    "# =========================\n",
    "# Functions to compute various evaluation metrics for morphological segmentation\n",
    "\n",
    "def is_correct_prediction(predicted, gold_variants):\n",
    "    \"\"\"\n",
    "    Check if predicted segmentation exactly matches any gold variant.\n",
    "    \n",
    "    Args:\n",
    "        predicted: List of predicted morphemes\n",
    "        gold_variants: List of gold segmentation variants\n",
    "    \n",
    "    Returns:\n",
    "        True if prediction matches any gold variant, False otherwise\n",
    "    \"\"\"\n",
    "    return any(predicted == variant for variant in gold_variants)\n",
    "\n",
    "def boundary_positions_from_labels(labels):\n",
    "    \"\"\"\n",
    "    Convert per-char boundary labels (length L) into boundary positions\n",
    "    at indices 0..L-2 (between characters). We ignore any label at the last index.\n",
    "    \"\"\"\n",
    "    if not labels:\n",
    "        return set()\n",
    "    L = len(labels)\n",
    "    return {i for i in range(min(L-1, len(labels))) if labels[i] == 1}\n",
    "\n",
    "def boundary_positions_from_segments(segments):\n",
    "    \"\"\"\n",
    "    Convert a list of segments into boundary positions (end-of-segment indices)\n",
    "    excluding the final segment end.\n",
    "    \"\"\"\n",
    "    pos = set()\n",
    "    acc = 0\n",
    "    for k, seg in enumerate(segments):\n",
    "        acc += len(seg)\n",
    "        if k < len(segments) - 1:\n",
    "            pos.add(acc - 1)  # boundary after this segment at index acc-1\n",
    "    return pos\n",
    "\n",
    "def prf_from_sets(pred_set, gold_set):\n",
    "    tp = len(pred_set & gold_set)\n",
    "    fp = len(pred_set - gold_set)\n",
    "    fn = len(gold_set - pred_set)\n",
    "    # Precision, recall, F1 with safe 0/0 handling (define as 1.0 when both empty)\n",
    "    if tp + fp == 0:\n",
    "        precision = 1.0 if tp + fp + fn == 0 else 0.0\n",
    "    else:\n",
    "        precision = tp / (tp + fp)\n",
    "    if tp + fn == 0:\n",
    "        recall = 1.0 if tp + fp + fn == 0 else 0.0\n",
    "    else:\n",
    "        recall = tp / (tp + fn)\n",
    "    if precision + recall == 0:\n",
    "        f1 = 1.0 if tp + fp + fn == 0 else 0.0\n",
    "    else:\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "    return tp, fp, fn, precision, recall, f1\n",
    "\n",
    "def best_variant_metrics(pred_boundaries, gold_variants):\n",
    "    \"\"\"\n",
    "    Among multiple gold segmentations (variants), pick the one that maximizes F1.\n",
    "    Returns: (best_gold_boundaries, tp, fp, fn, P, R, F1)\n",
    "    \"\"\"\n",
    "    best = None\n",
    "    for variant in gold_variants:\n",
    "        gold_b = boundary_positions_from_segments(variant)\n",
    "        tp, fp, fn, P, R, F1 = prf_from_sets(pred_boundaries, gold_b)\n",
    "        key = (F1, tp, -fn, -fp)  # tie-breakers\n",
    "        if (best is None) or (key > best[0]):\n",
    "            best = (key, gold_b, tp, fp, fn, P, R, F1)\n",
    "    # If no variants (shouldn't happen after your cleaning), fall back to empty\n",
    "    if best is None:\n",
    "        gold_b = set()\n",
    "        tp, fp, fn, P, R, F1 = prf_from_sets(pred_boundaries, gold_b)\n",
    "        return gold_b, tp, fp, fn, P, R, F1\n",
    "    _, gold_b, tp, fp, fn, P, R, F1 = best\n",
    "    return gold_b, tp, fp, fn, P, R, F1\n",
    "\n",
    "def normalize_gold_variants(gold_variants):\n",
    "    \"\"\"\n",
    "    Convert gold_variants to a list format, handling numpy arrays and nested structures.\n",
    "    \"\"\"\n",
    "    if gold_variants is None:\n",
    "        return []\n",
    "    \n",
    "    # If it's a numpy array, convert to list\n",
    "    if isinstance(gold_variants, np.ndarray):\n",
    "        gold_variants = gold_variants.tolist()\n",
    "    \n",
    "    # If it's already a list, ensure nested elements are also lists (not numpy arrays)\n",
    "    if isinstance(gold_variants, list):\n",
    "        normalized = []\n",
    "        for variant in gold_variants:\n",
    "            if isinstance(variant, np.ndarray):\n",
    "                normalized.append(variant.tolist())\n",
    "            elif isinstance(variant, list):\n",
    "                # Recursively normalize nested lists\n",
    "                normalized.append([item.tolist() if isinstance(item, np.ndarray) else item for item in variant])\n",
    "            else:\n",
    "                normalized.append(variant)\n",
    "        return normalized\n",
    "    \n",
    "    return []\n",
    "\n",
    "# =========================\n",
    "# EVALUATION ON TEST SET\n",
    "# =========================\n",
    "# Predict boundaries for all test words and compute evaluation metrics\n",
    "\n",
    "# Batch predict boundaries for all test words\n",
    "all_words = df[\"Word\"].tolist()\n",
    "all_boundaries = predict_boundaries(all_words, model, stoi, threshold=0.5)\n",
    "\n",
    "# Initialize metrics accumulators\n",
    "results = []\n",
    "micro_tp = micro_fp = micro_fn = 0  # Micro-averaged metrics (global counts)\n",
    "macro_Ps, macro_Rs, macro_F1s = [], [], []  # Macro-averaged metrics (per-word averages)\n",
    "\n",
    "# Evaluate each word\n",
    "for word, gold_variants, boundary_labels in zip(all_words, df[\"Gold\"], all_boundaries):\n",
    "    # Normalize gold_variants (convert numpy arrays to lists)\n",
    "    gold_variants = normalize_gold_variants(gold_variants)\n",
    "\n",
    "    # Per-word predicted boundaries from labels (ignore last index)\n",
    "    pred_b = boundary_positions_from_labels(boundary_labels)\n",
    "    # Choose the best gold variant for boundary comparison\n",
    "    gold_b, tp, fp, fn, P, R, F1 = best_variant_metrics(pred_b, gold_variants)\n",
    "\n",
    "    # Also compute the segmentation strings (as you had) for the exact-match accuracy\n",
    "    predicted_segments = apply_boundaries(word, boundary_labels)\n",
    "    correct = is_correct_prediction(predicted_segments, gold_variants)\n",
    "\n",
    "    results.append({\n",
    "        \"Word\": word,\n",
    "        \"Prediction\": predicted_segments,\n",
    "        \"Gold\": gold_variants,\n",
    "        \"PredBoundaries\": sorted(pred_b),\n",
    "        \"GoldBoundaries(Chosen)\": sorted(gold_b),\n",
    "        \"TP\": tp, \"FP\": fp, \"FN\": fn,\n",
    "        \"P_word\": P, \"R_word\": R, \"F1_word\": F1,\n",
    "        \"CorrectExactSeg\": correct\n",
    "    })\n",
    "\n",
    "    micro_tp += tp\n",
    "    micro_fp += fp\n",
    "    micro_fn += fn\n",
    "    macro_Ps.append(P)\n",
    "    macro_Rs.append(R)\n",
    "    macro_F1s.append(F1)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Exact segmentation accuracy (same as before)\n",
    "accuracy = results_df[\"CorrectExactSeg\"].mean()\n",
    "\n",
    "# Micro metrics (global)\n",
    "if micro_tp + micro_fp == 0:\n",
    "    P_micro = 1.0 if micro_tp + micro_fn == 0 else 0.0\n",
    "else:\n",
    "    P_micro = micro_tp / (micro_tp + micro_fp)\n",
    "if micro_tp + micro_fn == 0:\n",
    "    R_micro = 1.0 if micro_tp + micro_fp == 0 else 0.0\n",
    "else:\n",
    "    R_micro = micro_tp / (micro_tp + micro_fn)\n",
    "if P_micro + R_micro == 0:\n",
    "    F1_micro = 1.0 if (micro_tp + micro_fp + micro_fn) == 0 else 0.0\n",
    "else:\n",
    "    F1_micro = 2 * P_micro * R_micro / (P_micro + R_micro)\n",
    "\n",
    "# Macro metrics (average of per-word scores)\n",
    "P_macro = float(pd.Series(macro_Ps).mean()) if macro_Ps else 0.0\n",
    "R_macro = float(pd.Series(macro_Rs).mean()) if macro_Rs else 0.0\n",
    "F1_macro = float(pd.Series(macro_F1s).mean()) if macro_F1s else 0.0\n",
    "\n",
    "print(f\"Exact segmentation accuracy: {accuracy:.4f}\")\n",
    "print(\"Boundary metrics:\")\n",
    "print(f\"  Micro  - P: {P_micro:.4f}  R: {R_micro:.4f}  F1: {F1_micro:.4f}\")\n",
    "print(f\"  Macro  - P: {P_macro:.4f}  R: {R_macro:.4f}  F1: {F1_macro:.4f}\")\n",
    "\n",
    "# =========================\n",
    "# SPLIT-COUNT ACCURACY METRICS\n",
    "# =========================\n",
    "# Additional metrics that measure how close the predicted number of morphemes\n",
    "# is to the gold standard, even if the exact segmentation differs\n",
    "\n",
    "def split_count_metrics(predicted_segments, gold_variants):\n",
    "    \"\"\"\n",
    "    Compute split-count accuracy variants:\n",
    "    - Exact: same number of morphemes as any gold variant\n",
    "    - +1: one more split than any gold variant\n",
    "    - -1: one fewer split than any gold variant\n",
    "    - ±1: difference ≤ 1 with any gold variant\n",
    "    \"\"\"\n",
    "    pred_count = len(predicted_segments)\n",
    "    gold_counts = [len(gold) for gold in gold_variants]\n",
    "\n",
    "    exact = any(pred_count == g for g in gold_counts)\n",
    "    plus1 = any(pred_count == g + 1 for g in gold_counts)\n",
    "    minus1 = any(pred_count == g - 1 for g in gold_counts)\n",
    "    pm1 = any(abs(pred_count - g) <= 1 for g in gold_counts)\n",
    "\n",
    "    return {\"Exact\": exact, \"+1\": plus1, \"-1\": minus1, \"±1\": pm1}\n",
    "\n",
    "\n",
    "# ---- Extend results with split metrics ----\n",
    "split_exact_flags = []\n",
    "split_plus1_flags = []\n",
    "split_minus1_flags = []\n",
    "split_pm1_flags = []\n",
    "overlap_flags = []\n",
    "\n",
    "for rec in results:\n",
    "    predicted_segments = rec[\"Prediction\"]\n",
    "    gold_variants = rec[\"Gold\"]\n",
    "\n",
    "    # Normalize gold_variants (convert numpy arrays to lists)\n",
    "    gold_variants = normalize_gold_variants(gold_variants)\n",
    "\n",
    "    split_metrics = split_count_metrics(predicted_segments, gold_variants)\n",
    "    rec[\"CorrectSplitCount\"] = split_metrics[\"Exact\"]\n",
    "    rec[\"SplitCount+1\"] = split_metrics[\"+1\"]\n",
    "    rec[\"SplitCount-1\"] = split_metrics[\"-1\"]\n",
    "    rec[\"SplitCount±1\"] = split_metrics[\"±1\"]\n",
    "\n",
    "    # Overlap between exact segmentation and correct split-count\n",
    "    overlap = rec[\"CorrectExactSeg\"] and split_metrics[\"Exact\"]\n",
    "    rec[\"OverlapExactAndSplit\"] = overlap\n",
    "\n",
    "    split_exact_flags.append(split_metrics[\"Exact\"])\n",
    "    split_plus1_flags.append(split_metrics[\"+1\"])\n",
    "    split_minus1_flags.append(split_metrics[\"-1\"])\n",
    "    split_pm1_flags.append(split_metrics[\"±1\"])\n",
    "    overlap_flags.append(overlap)\n",
    "\n",
    "\n",
    "# ---- Aggregate metrics ----\n",
    "split_exact_acc = np.mean(split_exact_flags)\n",
    "split_plus1_acc = np.mean(split_plus1_flags)\n",
    "split_minus1_acc = np.mean(split_minus1_flags)\n",
    "split_pm1_acc = np.mean(split_pm1_flags)\n",
    "overlap_accuracy = np.mean(overlap_flags)\n",
    "\n",
    "# ---- Print summary ----\n",
    "print(\"\\n=== Split-count metrics ===\")\n",
    "print(f\"Split-count (Exact):          {split_exact_acc:.4f}\")\n",
    "print(f\"Split-count (+1):             {split_plus1_acc:.4f}\")\n",
    "print(f\"Split-count (−1):             {split_minus1_acc:.4f}\")\n",
    "print(f\"Split-count (±1):             {split_pm1_acc:.4f}\")\n",
    "print(f\"Overlap (Exact ∩ Split):      {overlap_accuracy:.4f}\")\n",
    "\n",
    "# ---- Save updated results ----\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# SAVE EVALUATION RESULTS\n",
    "# =========================\n",
    "# Save evaluation results to the data folder with a descriptive filename\n",
    "results_output_path = os.path.join(DATA_FOLDER, \"bilstm_eval_results.csv\")\n",
    "results_df.to_csv(results_output_path, index=False)\n",
    "print(f\"\\nEvaluation results saved to {results_output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Gold</th>\n",
       "      <th>PredBoundaries</th>\n",
       "      <th>GoldBoundaries(Chosen)</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>P_word</th>\n",
       "      <th>R_word</th>\n",
       "      <th>F1_word</th>\n",
       "      <th>CorrectExactSeg</th>\n",
       "      <th>CorrectSplitCount</th>\n",
       "      <th>SplitCount+1</th>\n",
       "      <th>SplitCount-1</th>\n",
       "      <th>SplitCount±1</th>\n",
       "      <th>OverlapExactAndSplit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>unupas</td>\n",
       "      <td>[unupa, s]</td>\n",
       "      <td>[[unu, pas]]</td>\n",
       "      <td>[4]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>umankus</td>\n",
       "      <td>[uma, nku, s]</td>\n",
       "      <td>[[uma, nku, s]]</td>\n",
       "      <td>[2, 5]</td>\n",
       "      <td>[2, 5]</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hikurin</td>\n",
       "      <td>[hiku, ri, n]</td>\n",
       "      <td>[[hikuri, n]]</td>\n",
       "      <td>[3, 5]</td>\n",
       "      <td>[5]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sutipi</td>\n",
       "      <td>[suti, pi]</td>\n",
       "      <td>[[suti, pi]]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pikunas</td>\n",
       "      <td>[pi, kuna, s]</td>\n",
       "      <td>[[pi, kuna, s]]</td>\n",
       "      <td>[1, 5]</td>\n",
       "      <td>[1, 5]</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>atipaq</td>\n",
       "      <td>[ati, paq]</td>\n",
       "      <td>[[ati, paq], [ati, pa, q]]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tomani</td>\n",
       "      <td>[toma, ni]</td>\n",
       "      <td>[[toma, ni]]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rantiq</td>\n",
       "      <td>[ranti, q]</td>\n",
       "      <td>[[ranti, q]]</td>\n",
       "      <td>[4]</td>\n",
       "      <td>[4]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>imakunas</td>\n",
       "      <td>[ima, kuna, s]</td>\n",
       "      <td>[[ima, kuna, s]]</td>\n",
       "      <td>[2, 6]</td>\n",
       "      <td>[2, 6]</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>chiqaq</td>\n",
       "      <td>[chiqa, q]</td>\n",
       "      <td>[[chiqaq]]</td>\n",
       "      <td>[4]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>hampichu</td>\n",
       "      <td>[hampi, chu]</td>\n",
       "      <td>[[hampi, chu]]</td>\n",
       "      <td>[4]</td>\n",
       "      <td>[4]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>misata</td>\n",
       "      <td>[misa, ta]</td>\n",
       "      <td>[[misa, ta]]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>nakuy</td>\n",
       "      <td>[na, ku, y]</td>\n",
       "      <td>[[na, ku, y]]</td>\n",
       "      <td>[1, 3]</td>\n",
       "      <td>[1, 3]</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>hampi</td>\n",
       "      <td>[hampi]</td>\n",
       "      <td>[[hampi]]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>hampiy</td>\n",
       "      <td>[hampi, y]</td>\n",
       "      <td>[[hampi, y]]</td>\n",
       "      <td>[4]</td>\n",
       "      <td>[4]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>limun</td>\n",
       "      <td>[limu, n]</td>\n",
       "      <td>[[limun]]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>hinas</td>\n",
       "      <td>[hina, s]</td>\n",
       "      <td>[[hina, s]]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>atispa</td>\n",
       "      <td>[ati, spa]</td>\n",
       "      <td>[[ati, spa]]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>riman</td>\n",
       "      <td>[rima, n]</td>\n",
       "      <td>[[rima, n]]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>matin</td>\n",
       "      <td>[mati, n]</td>\n",
       "      <td>[[mati, n]]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>yachani</td>\n",
       "      <td>[yacha, ni]</td>\n",
       "      <td>[[yacha, ni]]</td>\n",
       "      <td>[4]</td>\n",
       "      <td>[4]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>chaypiqa</td>\n",
       "      <td>[chay, pi, qa]</td>\n",
       "      <td>[[chay, pi, qa]]</td>\n",
       "      <td>[3, 5]</td>\n",
       "      <td>[3, 5]</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>niqchu</td>\n",
       "      <td>[ni, q, chu]</td>\n",
       "      <td>[[ni, q, chu]]</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>napiqa</td>\n",
       "      <td>[na, pi, qa]</td>\n",
       "      <td>[[na, pi, qa]]</td>\n",
       "      <td>[1, 3]</td>\n",
       "      <td>[1, 3]</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ripuni</td>\n",
       "      <td>[ri, puni]</td>\n",
       "      <td>[[ri, pu, ni]]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[1, 3]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>kapunchik</td>\n",
       "      <td>[ka, pu, nchik]</td>\n",
       "      <td>[[ka, pu, nchik]]</td>\n",
       "      <td>[1, 3]</td>\n",
       "      <td>[1, 3]</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>raykum</td>\n",
       "      <td>[ra, ykum]</td>\n",
       "      <td>[[ima, rayku, m]]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[2, 7]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>nispam</td>\n",
       "      <td>[ni, spam]</td>\n",
       "      <td>[[ni, spa, m]]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[1, 4]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>rimanku</td>\n",
       "      <td>[rima, nku]</td>\n",
       "      <td>[[rima, n, ku]]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[3, 4]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>punchay</td>\n",
       "      <td>[puncha, y]</td>\n",
       "      <td>[[punchay]]</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>tumankupas</td>\n",
       "      <td>[tuma, nkupa, s]</td>\n",
       "      <td>[[tuma, nku, pas]]</td>\n",
       "      <td>[3, 8]</td>\n",
       "      <td>[3, 6]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>chikan</td>\n",
       "      <td>[chika, n]</td>\n",
       "      <td>[[chikan]]</td>\n",
       "      <td>[4]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>munay</td>\n",
       "      <td>[muna, y]</td>\n",
       "      <td>[[muna, y]]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>atikun</td>\n",
       "      <td>[ati, ku, n]</td>\n",
       "      <td>[[ati, ku, n]]</td>\n",
       "      <td>[2, 4]</td>\n",
       "      <td>[2, 4]</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>chunka</td>\n",
       "      <td>[chunka]</td>\n",
       "      <td>[[chunka]]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>saniyapuni</td>\n",
       "      <td>[sani, ya, puni]</td>\n",
       "      <td>[[sani, ya, puni]]</td>\n",
       "      <td>[3, 5]</td>\n",
       "      <td>[3, 5]</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>hampusaq</td>\n",
       "      <td>[hampu, saq]</td>\n",
       "      <td>[[hampu, saq]]</td>\n",
       "      <td>[4]</td>\n",
       "      <td>[4]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>ñuqaykupis</td>\n",
       "      <td>[ñuqa, yku, pis]</td>\n",
       "      <td>[[ñuqa, yku, pis]]</td>\n",
       "      <td>[3, 6]</td>\n",
       "      <td>[3, 6]</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>chhikan</td>\n",
       "      <td>[chhika, n]</td>\n",
       "      <td>[[chhikan]]</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>santusmi</td>\n",
       "      <td>[santu, s, mi]</td>\n",
       "      <td>[[santus, mi]]</td>\n",
       "      <td>[4, 5]</td>\n",
       "      <td>[5]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>sutin</td>\n",
       "      <td>[suti, n]</td>\n",
       "      <td>[[suti, n]]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>intirukunapi</td>\n",
       "      <td>[inti, ru, ku, na, pi]</td>\n",
       "      <td>[[intiru, kuna, pi]]</td>\n",
       "      <td>[3, 5, 7, 9]</td>\n",
       "      <td>[5, 9]</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>wasinta</td>\n",
       "      <td>[wasi, n, ta]</td>\n",
       "      <td>[[wasi, n, ta]]</td>\n",
       "      <td>[3, 4]</td>\n",
       "      <td>[3, 4]</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>tomansunchik</td>\n",
       "      <td>[toma, n, sunchik]</td>\n",
       "      <td>[[toman, su, nchik]]</td>\n",
       "      <td>[3, 4]</td>\n",
       "      <td>[4, 6]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>uyarispaqa</td>\n",
       "      <td>[uyari, spa, qa]</td>\n",
       "      <td>[[uyari, spa, qa]]</td>\n",
       "      <td>[4, 7]</td>\n",
       "      <td>[4, 7]</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>turaykuna</td>\n",
       "      <td>[tura, y, kuna]</td>\n",
       "      <td>[[tura, y, kuna]]</td>\n",
       "      <td>[3, 4]</td>\n",
       "      <td>[3, 4]</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>aparuni</td>\n",
       "      <td>[apa, ru, ni]</td>\n",
       "      <td>[[apa, ru, ni]]</td>\n",
       "      <td>[2, 4]</td>\n",
       "      <td>[2, 4]</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>tiyakuq</td>\n",
       "      <td>[tiya, ku, q]</td>\n",
       "      <td>[[tiya, ku, q]]</td>\n",
       "      <td>[3, 5]</td>\n",
       "      <td>[3, 5]</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>tumani</td>\n",
       "      <td>[tuma, ni]</td>\n",
       "      <td>[[tuma, ni]]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>qhisakuwan</td>\n",
       "      <td>[qhisa, ku, wan]</td>\n",
       "      <td>[[qhisaku, wan]]</td>\n",
       "      <td>[4, 6]</td>\n",
       "      <td>[6]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Word              Prediction                        Gold  \\\n",
       "0         unupas              [unupa, s]                [[unu, pas]]   \n",
       "1        umankus           [uma, nku, s]             [[uma, nku, s]]   \n",
       "2        hikurin           [hiku, ri, n]               [[hikuri, n]]   \n",
       "3         sutipi              [suti, pi]                [[suti, pi]]   \n",
       "4        pikunas           [pi, kuna, s]             [[pi, kuna, s]]   \n",
       "5         atipaq              [ati, paq]  [[ati, paq], [ati, pa, q]]   \n",
       "6         tomani              [toma, ni]                [[toma, ni]]   \n",
       "7         rantiq              [ranti, q]                [[ranti, q]]   \n",
       "8       imakunas          [ima, kuna, s]            [[ima, kuna, s]]   \n",
       "9         chiqaq              [chiqa, q]                  [[chiqaq]]   \n",
       "10      hampichu            [hampi, chu]              [[hampi, chu]]   \n",
       "11        misata              [misa, ta]                [[misa, ta]]   \n",
       "12         nakuy             [na, ku, y]               [[na, ku, y]]   \n",
       "13         hampi                 [hampi]                   [[hampi]]   \n",
       "14        hampiy              [hampi, y]                [[hampi, y]]   \n",
       "15         limun               [limu, n]                   [[limun]]   \n",
       "16         hinas               [hina, s]                 [[hina, s]]   \n",
       "17        atispa              [ati, spa]                [[ati, spa]]   \n",
       "18         riman               [rima, n]                 [[rima, n]]   \n",
       "19         matin               [mati, n]                 [[mati, n]]   \n",
       "20       yachani             [yacha, ni]               [[yacha, ni]]   \n",
       "21      chaypiqa          [chay, pi, qa]            [[chay, pi, qa]]   \n",
       "22        niqchu            [ni, q, chu]              [[ni, q, chu]]   \n",
       "23        napiqa            [na, pi, qa]              [[na, pi, qa]]   \n",
       "24        ripuni              [ri, puni]              [[ri, pu, ni]]   \n",
       "25     kapunchik         [ka, pu, nchik]           [[ka, pu, nchik]]   \n",
       "26        raykum              [ra, ykum]           [[ima, rayku, m]]   \n",
       "27        nispam              [ni, spam]              [[ni, spa, m]]   \n",
       "28       rimanku             [rima, nku]             [[rima, n, ku]]   \n",
       "29       punchay             [puncha, y]                 [[punchay]]   \n",
       "30    tumankupas        [tuma, nkupa, s]          [[tuma, nku, pas]]   \n",
       "31        chikan              [chika, n]                  [[chikan]]   \n",
       "32         munay               [muna, y]                 [[muna, y]]   \n",
       "33        atikun            [ati, ku, n]              [[ati, ku, n]]   \n",
       "34        chunka                [chunka]                  [[chunka]]   \n",
       "35    saniyapuni        [sani, ya, puni]          [[sani, ya, puni]]   \n",
       "36      hampusaq            [hampu, saq]              [[hampu, saq]]   \n",
       "37    ñuqaykupis        [ñuqa, yku, pis]          [[ñuqa, yku, pis]]   \n",
       "38       chhikan             [chhika, n]                 [[chhikan]]   \n",
       "39      santusmi          [santu, s, mi]              [[santus, mi]]   \n",
       "40         sutin               [suti, n]                 [[suti, n]]   \n",
       "41  intirukunapi  [inti, ru, ku, na, pi]        [[intiru, kuna, pi]]   \n",
       "42       wasinta           [wasi, n, ta]             [[wasi, n, ta]]   \n",
       "43  tomansunchik      [toma, n, sunchik]        [[toman, su, nchik]]   \n",
       "44    uyarispaqa        [uyari, spa, qa]          [[uyari, spa, qa]]   \n",
       "45     turaykuna         [tura, y, kuna]           [[tura, y, kuna]]   \n",
       "46       aparuni           [apa, ru, ni]             [[apa, ru, ni]]   \n",
       "47       tiyakuq           [tiya, ku, q]             [[tiya, ku, q]]   \n",
       "48        tumani              [tuma, ni]                [[tuma, ni]]   \n",
       "49    qhisakuwan        [qhisa, ku, wan]            [[qhisaku, wan]]   \n",
       "\n",
       "   PredBoundaries GoldBoundaries(Chosen)  TP  FP  FN  P_word  R_word  \\\n",
       "0             [4]                    [2]   0   1   1     0.0     0.0   \n",
       "1          [2, 5]                 [2, 5]   2   0   0     1.0     1.0   \n",
       "2          [3, 5]                    [5]   1   1   0     0.5     1.0   \n",
       "3             [3]                    [3]   1   0   0     1.0     1.0   \n",
       "4          [1, 5]                 [1, 5]   2   0   0     1.0     1.0   \n",
       "5             [2]                    [2]   1   0   0     1.0     1.0   \n",
       "6             [3]                    [3]   1   0   0     1.0     1.0   \n",
       "7             [4]                    [4]   1   0   0     1.0     1.0   \n",
       "8          [2, 6]                 [2, 6]   2   0   0     1.0     1.0   \n",
       "9             [4]                     []   0   1   0     0.0     0.0   \n",
       "10            [4]                    [4]   1   0   0     1.0     1.0   \n",
       "11            [3]                    [3]   1   0   0     1.0     1.0   \n",
       "12         [1, 3]                 [1, 3]   2   0   0     1.0     1.0   \n",
       "13             []                     []   0   0   0     1.0     1.0   \n",
       "14            [4]                    [4]   1   0   0     1.0     1.0   \n",
       "15            [3]                     []   0   1   0     0.0     0.0   \n",
       "16            [3]                    [3]   1   0   0     1.0     1.0   \n",
       "17            [2]                    [2]   1   0   0     1.0     1.0   \n",
       "18            [3]                    [3]   1   0   0     1.0     1.0   \n",
       "19            [3]                    [3]   1   0   0     1.0     1.0   \n",
       "20            [4]                    [4]   1   0   0     1.0     1.0   \n",
       "21         [3, 5]                 [3, 5]   2   0   0     1.0     1.0   \n",
       "22         [1, 2]                 [1, 2]   2   0   0     1.0     1.0   \n",
       "23         [1, 3]                 [1, 3]   2   0   0     1.0     1.0   \n",
       "24            [1]                 [1, 3]   1   0   1     1.0     0.5   \n",
       "25         [1, 3]                 [1, 3]   2   0   0     1.0     1.0   \n",
       "26            [1]                 [2, 7]   0   1   2     0.0     0.0   \n",
       "27            [1]                 [1, 4]   1   0   1     1.0     0.5   \n",
       "28            [3]                 [3, 4]   1   0   1     1.0     0.5   \n",
       "29            [5]                     []   0   1   0     0.0     0.0   \n",
       "30         [3, 8]                 [3, 6]   1   1   1     0.5     0.5   \n",
       "31            [4]                     []   0   1   0     0.0     0.0   \n",
       "32            [3]                    [3]   1   0   0     1.0     1.0   \n",
       "33         [2, 4]                 [2, 4]   2   0   0     1.0     1.0   \n",
       "34             []                     []   0   0   0     1.0     1.0   \n",
       "35         [3, 5]                 [3, 5]   2   0   0     1.0     1.0   \n",
       "36            [4]                    [4]   1   0   0     1.0     1.0   \n",
       "37         [3, 6]                 [3, 6]   2   0   0     1.0     1.0   \n",
       "38            [5]                     []   0   1   0     0.0     0.0   \n",
       "39         [4, 5]                    [5]   1   1   0     0.5     1.0   \n",
       "40            [3]                    [3]   1   0   0     1.0     1.0   \n",
       "41   [3, 5, 7, 9]                 [5, 9]   2   2   0     0.5     1.0   \n",
       "42         [3, 4]                 [3, 4]   2   0   0     1.0     1.0   \n",
       "43         [3, 4]                 [4, 6]   1   1   1     0.5     0.5   \n",
       "44         [4, 7]                 [4, 7]   2   0   0     1.0     1.0   \n",
       "45         [3, 4]                 [3, 4]   2   0   0     1.0     1.0   \n",
       "46         [2, 4]                 [2, 4]   2   0   0     1.0     1.0   \n",
       "47         [3, 5]                 [3, 5]   2   0   0     1.0     1.0   \n",
       "48            [3]                    [3]   1   0   0     1.0     1.0   \n",
       "49         [4, 6]                    [6]   1   1   0     0.5     1.0   \n",
       "\n",
       "     F1_word  CorrectExactSeg  CorrectSplitCount  SplitCount+1  SplitCount-1  \\\n",
       "0   0.000000            False               True         False         False   \n",
       "1   1.000000             True               True         False         False   \n",
       "2   0.666667            False              False          True         False   \n",
       "3   1.000000             True               True         False         False   \n",
       "4   1.000000             True               True         False         False   \n",
       "5   1.000000             True               True         False          True   \n",
       "6   1.000000             True               True         False         False   \n",
       "7   1.000000             True               True         False         False   \n",
       "8   1.000000             True               True         False         False   \n",
       "9   0.000000            False              False          True         False   \n",
       "10  1.000000             True               True         False         False   \n",
       "11  1.000000             True               True         False         False   \n",
       "12  1.000000             True               True         False         False   \n",
       "13  1.000000             True               True         False         False   \n",
       "14  1.000000             True               True         False         False   \n",
       "15  0.000000            False              False          True         False   \n",
       "16  1.000000             True               True         False         False   \n",
       "17  1.000000             True               True         False         False   \n",
       "18  1.000000             True               True         False         False   \n",
       "19  1.000000             True               True         False         False   \n",
       "20  1.000000             True               True         False         False   \n",
       "21  1.000000             True               True         False         False   \n",
       "22  1.000000             True               True         False         False   \n",
       "23  1.000000             True               True         False         False   \n",
       "24  0.666667            False              False         False          True   \n",
       "25  1.000000             True               True         False         False   \n",
       "26  0.000000            False              False         False          True   \n",
       "27  0.666667            False              False         False          True   \n",
       "28  0.666667            False              False         False          True   \n",
       "29  0.000000            False              False          True         False   \n",
       "30  0.500000            False               True         False         False   \n",
       "31  0.000000            False              False          True         False   \n",
       "32  1.000000             True               True         False         False   \n",
       "33  1.000000             True               True         False         False   \n",
       "34  1.000000             True               True         False         False   \n",
       "35  1.000000             True               True         False         False   \n",
       "36  1.000000             True               True         False         False   \n",
       "37  1.000000             True               True         False         False   \n",
       "38  0.000000            False              False          True         False   \n",
       "39  0.666667            False              False          True         False   \n",
       "40  1.000000             True               True         False         False   \n",
       "41  0.666667            False              False         False         False   \n",
       "42  1.000000             True               True         False         False   \n",
       "43  0.500000            False               True         False         False   \n",
       "44  1.000000             True               True         False         False   \n",
       "45  1.000000             True               True         False         False   \n",
       "46  1.000000             True               True         False         False   \n",
       "47  1.000000             True               True         False         False   \n",
       "48  1.000000             True               True         False         False   \n",
       "49  0.666667            False              False          True         False   \n",
       "\n",
       "    SplitCount±1  OverlapExactAndSplit  \n",
       "0           True                 False  \n",
       "1           True                  True  \n",
       "2           True                 False  \n",
       "3           True                  True  \n",
       "4           True                  True  \n",
       "5           True                  True  \n",
       "6           True                  True  \n",
       "7           True                  True  \n",
       "8           True                  True  \n",
       "9           True                 False  \n",
       "10          True                  True  \n",
       "11          True                  True  \n",
       "12          True                  True  \n",
       "13          True                  True  \n",
       "14          True                  True  \n",
       "15          True                 False  \n",
       "16          True                  True  \n",
       "17          True                  True  \n",
       "18          True                  True  \n",
       "19          True                  True  \n",
       "20          True                  True  \n",
       "21          True                  True  \n",
       "22          True                  True  \n",
       "23          True                  True  \n",
       "24          True                 False  \n",
       "25          True                  True  \n",
       "26          True                 False  \n",
       "27          True                 False  \n",
       "28          True                 False  \n",
       "29          True                 False  \n",
       "30          True                 False  \n",
       "31          True                 False  \n",
       "32          True                  True  \n",
       "33          True                  True  \n",
       "34          True                  True  \n",
       "35          True                  True  \n",
       "36          True                  True  \n",
       "37          True                  True  \n",
       "38          True                 False  \n",
       "39          True                 False  \n",
       "40          True                  True  \n",
       "41         False                 False  \n",
       "42          True                  True  \n",
       "43          True                 False  \n",
       "44          True                  True  \n",
       "45          True                  True  \n",
       "46          True                  True  \n",
       "47          True                  True  \n",
       "48          True                  True  \n",
       "49          True                 False  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.head(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
